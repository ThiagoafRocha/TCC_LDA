{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b542615-f077-40d7-ab84-bbc4e70ba255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('error', 0.07867998263135041),\n",
      "   ('run', 0.06191923577941815),\n",
      "   ('file', 0.056534954407294835),\n",
      "   ('window', 0.04793747286148502),\n",
      "   ('raspberry_pi', 0.028050369083803733),\n",
      "   ('core', 0.026747720364741642),\n",
      "   ('build', 0.024402952670429873),\n",
      "   ('follow', 0.021623968736430743),\n",
      "   ('version', 0.020495006513243597),\n",
      "   ('script', 0.02040816326530612)]),\n",
      " (1,\n",
      "  [('create', 0.055798288731112325),\n",
      "   ('request', 0.051064991807755325),\n",
      "   ('aw', 0.04687784452940105),\n",
      "   ('thing', 0.044693245949390135),\n",
      "   ('update', 0.04432914618605498),\n",
      "   ('follow', 0.025122883670125613),\n",
      "   ('http', 0.021481886036774075),\n",
      "   ('post', 0.02075368651010377),\n",
      "   ('button', 0.018478062989259057),\n",
      "   ('status', 0.016111414527580557)]),\n",
      " (2,\n",
      "  [('send', 0.14107731769879076),\n",
      "   ('message', 0.059728838402345184),\n",
      "   ('receive', 0.05954562110663247),\n",
      "   ('topic', 0.05542323195309637),\n",
      "   ('publish', 0.04534628068889703),\n",
      "   ('event', 0.043697325027482595),\n",
      "   ('broker', 0.03865884939538292),\n",
      "   ('command', 0.026108464639061926),\n",
      "   ('node', 0.0209783803591059),\n",
      "   ('node_re', 0.01676438255771345)]),\n",
      " (3,\n",
      "  [('datum', 0.18037366858739304),\n",
      "   ('data', 0.035446132355508995),\n",
      "   ('database', 0.033700017461148944),\n",
      "   ('store', 0.03230312554566091),\n",
      "   ('query', 0.020429544264012573),\n",
      "   ('process', 0.018508817880216518),\n",
      "   ('table', 0.014580059367906408),\n",
      "   ('single', 0.013532390431290378),\n",
      "   ('stream', 0.012048192771084338),\n",
      "   ('storage', 0.010214772132006287)]),\n",
      " (4,\n",
      "  [('app', 0.09066818095778914),\n",
      "   ('network', 0.05186447094453909),\n",
      "   ('android', 0.046091236040128714),\n",
      "   ('connect', 0.04126443308726103),\n",
      "   ('internet', 0.03274654552337687),\n",
      "   ('access', 0.025837592277115275),\n",
      "   ('local', 0.025269733106189664),\n",
      "   ('mobile', 0.01788756388415673),\n",
      "   ('phone', 0.016846488737459777),\n",
      "   ('connection', 0.015805413590762825)]),\n",
      " (5,\n",
      "  [('work', 0.0518490637056554),\n",
      "   ('project', 0.04855556601110379),\n",
      "   ('show', 0.028418180107273924),\n",
      "   ('simple', 0.021925284652300744),\n",
      "   ('find', 0.019666886233179637),\n",
      "   ('issue', 0.017126188011668392),\n",
      "   ('point', 0.016749788275148208),\n",
      "   ('idea', 0.016467488472758068),\n",
      "   ('image', 0.016373388538628023),\n",
      "   ('give', 0.016279288604497975)]),\n",
      " (6,\n",
      "  [('device', 0.3524163568773234),\n",
      "   ('azure', 0.06756505576208179),\n",
      "   ('hub', 0.05817843866171004),\n",
      "   ('cloud', 0.032063197026022304),\n",
      "   ('create', 0.026301115241635687),\n",
      "   ('gateway', 0.025650557620817842),\n",
      "   ('register', 0.01895910780669145),\n",
      "   ('edge', 0.01654275092936803),\n",
      "   ('property', 0.0137546468401487),\n",
      "   ('group', 0.010873605947955391)]),\n",
      " (7,\n",
      "  [('client', 0.0821547057796403),\n",
      "   ('connection', 0.048367382573773356),\n",
      "   ('set', 0.047319713637157326),\n",
      "   ('log', 0.03719224724986904),\n",
      "   ('certificate', 0.028810895756940808),\n",
      "   ('type', 0.022786799371398637),\n",
      "   ('option', 0.017984983411908503),\n",
      "   ('key', 0.01746114894360049),\n",
      "   ('error', 0.015103893836214423),\n",
      "   ('connect', 0.014580059367906408)]),\n",
      " (8,\n",
      "  [('esp', 0.038703788224554996),\n",
      "   ('write', 0.035965312642628935),\n",
      "   ('read', 0.03368324965769055),\n",
      "   ('module', 0.02647193062528526),\n",
      "   ('command', 0.0252852578731173),\n",
      "   ('return', 0.022820629849383843),\n",
      "   ('string', 0.020994979461433135),\n",
      "   ('problem', 0.01834778639890461),\n",
      "   ('function', 0.018165221360109538),\n",
      "   ('output', 0.017252396166134186)]),\n",
      " (9,\n",
      "  [('server', 0.09302325581395349),\n",
      "   ('system', 0.033561331509347925),\n",
      "   ('question', 0.03137254901960784),\n",
      "   ('understand', 0.023985408116735066),\n",
      "   ('implement', 0.023711810305517556),\n",
      "   ('control', 0.022799817601459188),\n",
      "   ('open', 0.01933424532603739),\n",
      "   ('base', 0.018878248974008208),\n",
      "   ('communication', 0.018057455540355677),\n",
      "   ('web', 0.017783857729138167)]),\n",
      " (10,\n",
      "  [('application', 0.08627069133398248),\n",
      "   ('service', 0.06007789678675755),\n",
      "   ('find', 0.04975657254138267),\n",
      "   ('user', 0.040603700097370986),\n",
      "   ('run', 0.03807205452775073),\n",
      "   ('start', 0.03680623174294061),\n",
      "   ('call', 0.027361246348588122),\n",
      "   ('java', 0.02629016553067186),\n",
      "   ('api', 0.02502434274586173),\n",
      "   ('platform', 0.024829600778967866)]),\n",
      " (11,\n",
      "  [('sensor', 0.09029512443334259),\n",
      "   ('time', 0.06577851790174855),\n",
      "   ('temperature', 0.030530113794060506),\n",
      "   ('problem', 0.025164215006013506),\n",
      "   ('state', 0.01989083171431215),\n",
      "   ('read', 0.017670459802016838),\n",
      "   ('object', 0.015172541400684615),\n",
      "   ('input', 0.015080025904338977),\n",
      "   ('result', 0.014894994911647702),\n",
      "   ('number', 0.014432417429919511)])]\n",
      "\n",
      "\n",
      "\n",
      "Grau de Coerência:  0.4234247938624786 \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] O sistema não pode encontrar o arquivo especificado: 'files/topico12.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2208/1598327789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mldamallet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;31m# Format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2208/1598327789.py\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[1;34m(ldamodel, corpus, texts, alldata)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0msent_topics_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mcreate_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# Get main topic in each document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2208/1598327789.py\u001b[0m in \u001b[0;36mcreate_file\u001b[1;34m()\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\".json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'files/topico12.json'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', '&quot', 'datum', 'work', 'connect', 'code', 'iot', 'make', 'add', 'message', 'follow', 'solution', 'day'])\n",
    "\n",
    "datafile = json.load(codecs.open('data.json', 'r', 'utf-8-sig'))\n",
    "\n",
    "data = []\n",
    "for item in datafile[\"items\"]:\n",
    "    data.append(item[\"body\"])\n",
    "    \n",
    "#Remove Emoji\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "    \n",
    "data_notags = []\n",
    "#Remove tags and break lines\n",
    "for item in data:\n",
    "    data_bl = item.replace('\\n', ' ').replace('\\r', '')\n",
    "    data_nc = re.sub(r'<pre>.+?</pre>', '', data_bl)\n",
    "    data_ne = deEmojify(data_nc)\n",
    "    data_nt = BeautifulSoup(data_ne, \"lxml\").text\n",
    "\n",
    "    data_notags.append(data_nt)\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data_notags))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_trigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "id2word.filter_extremes(no_below=2, no_above=1.0)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "mallet_path = 'C:/Users/ThIaG/PycharmProjects/lda3/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=12, id2word=id2word)\n",
    "\n",
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(num_topics = 12, formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\n\\n\\nGrau de Coerência: ', coherence_ldamallet, '\\n\\n')\n",
    "\n",
    "def create_file():\n",
    "    path = \"files/topico\"\n",
    "    ext = \".json\"\n",
    "    for i in range(0,12):\n",
    "        os.remove(path + str(i) + ext)\n",
    "        open(path + str(i) + ext, \"x\")\n",
    "        \n",
    "\n",
    "def save_data(index, topic_num, data):\n",
    "    json_object = json.dumps(data[\"items\"][index])\n",
    "    with open(\"files/topico\"+ str(topic_num) + \".json\", \"a\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n",
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data, alldata=datafile):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    create_file()\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                save_data(i, topic_num, alldata)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data, alldata=datafile)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_contribution = topic_contribution.rename_axis('Dominant_Topic').reset_index(name='percentage')\n",
    "\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']].drop_duplicates()\n",
    "topic_num_keywords.index = range(len(topic_num_keywords))\n",
    "\n",
    "df_dominant_topics = pd.merge(topic_contribution, topic_num_keywords, how='inner', on='Dominant_Topic')\n",
    "df_dominant_topics.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5eafd7-9f23-4c36-a1ce-023f6da2bada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5736706-011c-418d-be30-59cb6478971a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
