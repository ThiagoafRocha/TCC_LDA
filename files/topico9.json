{
    "items": [{
        "tags": ["pyspark", "iot"],
        "is_answered": false,
        "view_count": 10,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1631961672,
        "creation_date": 1631961672,
        "question_id": 69233893,
        "link": "https://stackoverflow.com/questions/69233893/methods-or-platform-large-log-iot-data-for-analysis",
        "title": "methods or platform Large log IoT data for analysis",
        "body": "<p>I have 2 TB json log files from IoT device.\nI need to query this for analyzing.\nNow I'm cleansing this with my python code. and I don't know what kind of db or data structure is helpful for querying these kind of IoT events.\nit's file not the streaming data...\nWhat kind of platform or methods are available for query with time efficient?</p>\n"
    }, {
        "tags": ["json", "google-cloud-platform", "google-bigquery", "iot", "google-dataflow"],
        "is_answered": true,
        "view_count": 35,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1631655689,
        "creation_date": 1631303151,
        "question_id": 69137338,
        "link": "https://stackoverflow.com/questions/69137338/how-to-put-nested-json-data-into-bigquery-table-with-google-cloud-platforms-dat",
        "title": "How to put nested JSON data into BigQuery table with Google Cloud Platform&#39;s dataflow&#39;s Pub/Sub Topic -&gt; BigQuery Template",
        "body": "<p>I am trying to store messages sent from an IoT device in a BigQuery table.</p>\n<p>The cloud architecture is as follows:</p>\n<p>Local Device -&gt; json_message -&gt; mqtt_client -&gt; GC IoT device -&gt; Device Registry -&gt; Pub/Sub Topic -&gt; Dataflow with Pub/Sub Topic to BigQuery Template -&gt; BigQuery Table</p>\n<p>I have gotten this system working with a non-nested JSON message that is constructed like this</p>\n<pre class=\"lang-py prettyprint-override\"><code>json_dict = {&quot;instrument&quot;: instrument,\n &quot;spectrum&quot;: str(self.spectrum),\n &quot;spectrum_creation_time&quot;: self.creation_dt.timestamp(),\n &quot;messenger_creation_time&quot;: time.time()}\n\nreturn json.dumps(json_dict)\n\n</code></pre>\n<p>The table in BigQuery, which successfully stores this data has the following schema:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>   Last modified                  Schema                 Total Rows   Total Bytes   Expiration   Time Partitioning   Clustered Fields   Labels\n ----------------- ------------------------------------ ------------ ------------- ------------ ------------------- ------------------ --------\n  04 Sep 00:24:22   |- instrument: string                1277         81897474\n                    |- spectrum: string\n                    |- spectrum_creation_time: string\n                    |- messenger_creation_time: string\n</code></pre>\n<p>Now I am trying to get this same system working with a nested JSON message, which is constructed like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>json_dict = {'timestamp': 'AUTO',\n             'values': [\n                        {'id': instrument + '.Time',\n                         'v': time.time(),\n                         't': time.time()},\n                        {'id': instrument + 'Intensity',\n                         'v': str(self.spectrum),\n                         't': self.creation_dt.timestamp()}\n                        ]}\n\nreturn json.dumps(json_dict)\n</code></pre>\n<p>I am trying to store it in a BigQuery table with the following schema:</p>\n<pre><code>   Last modified               Schema              Total Rows   Total Bytes   Expiration   Time Partitioning   Clustered Fields   Labels\n ----------------- ------------------------------ ------------ ------------- ------------ ------------------- ------------------ --------\n  09 Sep 23:56:20   |- timestamp: timestamp        0            0\n                    +- values: record (repeated)\n                    |  +- time: record\n                    |  |  |- id: string\n                    |  |  |- v: string\n                    |  |  |- t: timestamp\n                    |  +- spectrum: record\n                    |  |  |- id: string\n                    |  |  |- v: string\n                    |  |  |- t: timestamp\n</code></pre>\n<p>Unfortunately, when I try this approach I get the following error, which is output to an error table in BigQuery by DataFlow.</p>\n<pre><code>{&quot;errors&quot;:[{&quot;debugInfo&quot;:&quot;&quot;,&quot;location&quot;:&quot;values[0].v&quot;,&quot;message&quot;:&quot;no such field: v.&quot;,&quot;reason&quot;:&quot;invalid&quot;}],&quot;index&quot;:0}\nnull\n</code></pre>\n<p>What is the best way to solve this issue? I cannot change the nested JSON structure, because I am building a test suite and this is the required format.</p>\n"
    }, {
        "tags": ["algorithm", "cloud", "iot", "genetic-algorithm", "edge-detection"],
        "is_answered": false,
        "view_count": 14,
        "favorite_count": 0,
        "closed_date": 1631471548,
        "score": -1,
        "last_activity_date": 1631021923,
        "creation_date": 1631021923,
        "question_id": 69089270,
        "link": "https://stackoverflow.com/questions/69089270/reduce-latency-and-offloading-between-the-edge-computing-and-iot-application",
        "title": "reduce latency and offloading between the edge computing and IoT application",
        "body": "<p>i have to develop an algorithme the  goal is to reduce latency and offloading between the edge computing and IoT applications and i don't know how to start and where start the code please i need help\nThe error correction technique it use the frequently to solve the problem between the edge and IoT application\nthe ability and flexibility performance between the multi edge and multi devices under the IoT application\nThe challenge of limited data speed exacerbated by the proliferation of billions of data-intensive applications under the IoT.\nThe error correction ability and flexibility can improved get a good latency and offloading between the edge and devise under IoT application.\nDevelop, enhancement to solve and process the offloading and latency between the edge and device under the environments IoT field by using these technique.\nThe main goal it is Design and develops the bald eagle search optimization algorithm to solve the latency and offloading between the edge and IoT application by using the error correction techniques</p>\n"
    }, {
        "tags": ["database", "database-design", "iot"],
        "is_answered": false,
        "view_count": 14,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1630413033,
        "creation_date": 1630395587,
        "last_edit_date": 1630413033,
        "question_id": 68994701,
        "link": "https://stackoverflow.com/questions/68994701/template-of-an-iot-database-schema",
        "title": "Template of an IoT Database Schema",
        "body": "<p>I am currently implementing an IoT structure for a project. The inspiration comes from this link and paper <a href=\"https://www.sciencedirect.com/science/article/pii/S1877050914009144\" rel=\"nofollow noreferrer\">https://www.sciencedirect.com/science/article/pii/S1877050914009144</a>.  The overall structure is:</p>\n<ul>\n<li>Sensors &lt;--&gt; Xbee &lt;--&gt; Zigbee &lt;--&gt; Xbee &lt;--&gt; Gateway/Base Station &lt;--&gt; Router &lt;--&gt; Devices</li>\n</ul>\n<p>However, the paper did not show the database schema. For someone that doesn't know a lot about database schema. Does anyone how to structure a database schema for this project, or if a template is already available that I can use?</p>\n"
    }, {
        "tags": ["caching", "simulation", "iot", "sdn"],
        "is_answered": false,
        "view_count": 34,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1630272738,
        "creation_date": 1630049445,
        "last_edit_date": 1630272738,
        "question_id": 68949533,
        "link": "https://stackoverflow.com/questions/68949533/implement-in-network-caching-on-edge-computing-sdn-iot-networks",
        "title": "Implement in-network caching on edge computing SDN-IoT networks",
        "body": "<p>I use IoTSim-Osmosis as a simulation framework to use SDN and edge computing in IoT networks. I want to implement in-network caching on it to evaluate the effect of that on energy consumption and latency in IoT.</p>\n<p>What shall I do now?</p>\n"
    }, {
        "tags": ["python", "filtering", "iot"],
        "is_answered": false,
        "view_count": 22,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1630001853,
        "creation_date": 1629960056,
        "last_edit_date": 1630001853,
        "question_id": 68933674,
        "link": "https://stackoverflow.com/questions/68933674/smart-iot-log-data-filtering",
        "title": "Smart IOT log data Filtering",
        "body": "<p>I want to develop a program to capture IOT log data coming from specific port. After that I want to filter those captured data before storing them in storages. I have started developing an script using python but it is not working properly. I want to develop this by adding a filter to the storing end. Here I get the output to the text file.</p>\n<pre><code>from scapy.all import *\nfrom scapy.layers.http import HTTPRequest # import HTTP packet\n\n\n\ndef sniff_packets(iface=None):\n    if iface:\n        # port 80 for http (generally)\n        # `process_packet` is the callback\n        sniff(filter=&quot;port 9200&quot;, prn=process_packet, iface=iface, store=False)\n    else:\n        # sniff with default interface\n        sniff(filter=&quot;port 9200&quot;, prn=process_packet, store=False)\n\n\ndef process_packet(packet):\n    &quot;&quot;&quot;\n    This function is executed whenever a packet is sniffed\n    &quot;&quot;&quot;\n    if packet.haslayer(HTTPRequest): #tcp packet trace \n        print(packet.show()) #show packet data\n        f = open(&quot;/file.txt&quot;, &quot;w&quot;)\n        f.write(packet.show())\n        f.close()\n\n\nif __name__ == &quot;__main__&quot;:\n    import argparse\n    parser = argparse.ArgumentParser(description=&quot;HTTP Packet Sniffer, this is useful when you're a man in the middle.&quot; \\\n                                                 + &quot;It is suggested that you run arp spoof before you use this script, otherwise it'll sniff your personal packets&quot;)\n    parser.add_argument(&quot;-i&quot;, &quot;--iface&quot;, help=&quot;Interface to use, default is scapy's default interface&quot;)\n    parser.add_argument(&quot;--show-raw&quot;, dest=&quot;show_raw&quot;, action=&quot;store_true&quot;, help=&quot;Whether to print POST raw data, such as passwords, search queries, etc.&quot;)\n\n    # parse arguments\n    args = parser.parse_args()\n    iface = args.iface\n    show_raw = args.show_raw\n\n    sniff_packets(iface)\n</code></pre>\n"
    }, {
        "tags": ["iot", "gsm", "sd-card", "modbus", "buffering"],
        "is_answered": false,
        "view_count": 13,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1629999319,
        "creation_date": 1629999319,
        "question_id": 68943070,
        "link": "https://stackoverflow.com/questions/68943070/how-to-do-data-buffering-in-a-gsm-iot-gateway-made-using-stm32-and-sdcard",
        "title": "How to do data buffering in a GSM IoT gateway made using STM32 and SDcard",
        "body": "<p>I am designing an IoT gateway to transmit data from a MODBUS device to MQTT broker through GSM/LTE network. MODBUS polling is done by the gateway as the address is fixed. My sampling interval is 1 min. My sample size is 2K bits. As the cellular connectivity here seems erratic, I need to buffer my samples and a csv file in SD card with FATfs seems to be a better choice.The connectivity may take up-to several hours or even days (worst case) to resume. I seek advice and suggestions to implement this process in the most reliable way.</p>\n<p>Please guide me to any similar works published.</p>\n<p>My hardware is comprised of STM32F411 (or higher) controller and a quectel LTE CAT1 radio.\nFreeRTOS and CubeIDE are used. Entire system except buffering works well as intended.</p>\n"
    }, {
        "tags": ["python-3.x", "raspberry-pi3", "iot", "gsm", "raspberry-pi4"],
        "is_answered": false,
        "view_count": 14,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1629448016,
        "creation_date": 1629447357,
        "last_edit_date": 1629448016,
        "question_id": 68859009,
        "link": "https://stackoverflow.com/questions/68859009/what-can-be-the-possibilites-for-multiple-access-on-port-ttyama0",
        "title": "What can be the possibilites for multiple access on port ttyAMA0?",
        "body": "<p>While uploading data from various sensors and GPS coordinates using SIM808 to AWS, we are getting this error after it uploads few rows. We are not running any other code but still, it gives multiple access on port error.</p>\n<p>We are running it on RPi 4 8GB/4GB and RPi 3B+.</p>\n<pre><code>Data inserted successfully!\nTraceback (most recent call last):\n  File &quot;HARDWARE_CODE_UPDATED.py&quot;, line 140, in &lt;module&gt;\n    lat, long, speed = gsm()\n  File &quot;HARDWARE_CODE_UPDATED.py&quot;, line 53, in gsm\n    msg2 = port.read(100)\n  File &quot;/usr/local/lib/python3.7/dist-packages/serial/serialposix.py&quot;, line 596, in read\n    'device reports readiness to read but returned no data '\nserial.serialutil.SerialException: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\n</code></pre>\n"
    }, {
        "tags": ["sql", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 20,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1629441963,
        "creation_date": 1629440821,
        "last_edit_date": 1629441963,
        "question_id": 68857802,
        "link": "https://stackoverflow.com/questions/68857802/csv-file-exported-from-tdengine-database-cannot-be-reimported",
        "title": "csv file exported from TDengine database cannot be reimported",
        "body": "<p>I have two tables (t4, t5) with the same schema, the csv file exported from t4 cannot be imported into t5</p>\n<pre class=\"lang-sql prettyprint-override\"><code>taos&gt; describe t4;\n             Field              |         Type         |   Length    |   Note   |\n=================================================================================\n ts                             | TIMESTAMP            |           8 |          |\n c1                             | INT                  |           4 |          |\n c2                             | FLOAT                |           4 |          |\n c3                             | INT                  |           4 |          |\n c4                             | INT                  |           4 |          |\nQuery OK, 5 row(s) in set (0.000233s)\n\ntaos&gt; describe t5;\n             Field              |         Type         |   Length    |   Note   |\n=================================================================================\n ts                             | TIMESTAMP            |           8 |          |\n c1                             | INT                  |           4 |          |\n c2                             | FLOAT                |           4 |          |\n c3                             | INT                  |           4 |          |\n c4                             | INT                  |           4 |          |\nQuery OK, 5 row(s) in set (0.000540s)\n\ntaos&gt; select * from t4 &gt;&gt; test.csv;\nQuery OK, 2000000 row(s) in set (4.302214s)\n\ntaos&gt; insert into t5 file 'test.csv';\n\nDB error: Syntax error in SQL (0.000301s)\n</code></pre>\n<p>I have checked the documentation, there is no problem with my SQL syntax, why the csv file cannot be imported?</p>\n"
    }, {
        "tags": ["sql", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 11,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1629379268,
        "creation_date": 1629368922,
        "last_edit_date": 1629379268,
        "question_id": 68846169,
        "link": "https://stackoverflow.com/questions/68846169/how-to-show-vnodes-in-one-dnode-in-tdengine-cluster",
        "title": "how to show vnodes in one dnode in TDengine cluster?",
        "body": "<p>I know there are a lot of commands like: show mnodes, show dnodes, show vgroups in taos shell.</p>\n<p>From show vgroups command we can see vnodes status like below:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>taos&gt; show dnodes;\n   id   |           end_point            | vnodes | cores  |   status   | role  |       create_time       |      offline reason      |\n======================================================================================================================================\n      1 | tdnode1:6030                   |      0 |      4 | ready      | mnode | 2021-08-19 18:14:56.647 |                          |\n      2 | tdnode2:6030                   |      7 |      4 | ready      | vnode | 2021-08-19 18:14:56.837 |                          |\n      3 | tdnode3:6030                   |      8 |      4 | ready      | vnode | 2021-08-19 18:14:56.841 |                          |\n      4 | tdnode4:6030                   |      7 |      4 | ready      | vnode | 2021-08-19 18:14:56.842 |                          |\n      5 | tdnode5:6030                   |      8 |      4 | ready      | vnode | 2021-08-19 18:14:56.844 |                          |\nQuery OK, 5 row(s) in set (0.001483s)\n\ntaos&gt; show vgroups;\n    vgId     |   tables    |  status  |   onlines   | v1_dnode | v1_status | v2_dnode | v2_status | v3_dnode | v3_status | compacting  |\n========================================================================================================================================\n           2 |        1000 | ready    |           3 |        5 | master    |        3 | slave     |        2 | slave     |           0 |\n           3 |        1000 | ready    |           3 |        4 | master    |        5 | slave     |        3 | slave     |           0 |\n           4 |        1000 | ready    |           3 |        3 | master    |        4 | slave     |        2 | slave     |           0 |\n           5 |        1000 | ready    |           3 |        2 | master    |        5 | slave     |        4 | slave     |           0 |\n           6 |        1000 | ready    |           3 |        5 | master    |        3 | slave     |        2 | slave     |           0 |\n           7 |        1000 | ready    |           3 |        4 | master    |        5 | slave     |        3 | slave     |           0 |\n           8 |        1000 | ready    |           3 |        3 | master    |        4 | slave     |        2 | slave     |           0 |\n           9 |        1000 | ready    |           3 |        2 | master    |        5 | slave     |        4 | slave     |           0 |\n          10 |        1000 | ready    |           3 |        5 | master    |        3 | slave     |        2 | slave     |           0 |\n          11 |        1000 | ready    |           3 |        4 | master    |        5 | slave     |        3 | slave     |           0 |\nQuery OK, 10 row(s) in set (0.001678s)\n</code></pre>\n<p>But it is really hard to tell which vnodes in dnode with id 1, is there any simple command to show vnodes only in dnode with 1?</p>\n"
    }, {
        "tags": ["database", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 6,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1629126654,
        "creation_date": 1629126654,
        "question_id": 68805233,
        "link": "https://stackoverflow.com/questions/68805233/maxtablespervnode-doesnt-work-in-tdengine-database",
        "title": "maxTablesPerVnode doesn&#39;t work in TDengine database",
        "body": "<p>I created a database and I would like to make the database has maximum 100 tables in each vnode, so that I will have 10 vnodes if I create 1000 tables.</p>\n<p>I adjusted the value of maxTablesPerVnode to 100, but it didn't work no matter how many times I restarted taosd. The number of tables in each vnode is still 1000 by default. what is the problem?</p>\n"
    }, {
        "tags": ["iot", "td-engine"],
        "is_answered": true,
        "view_count": 23,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1629094163,
        "creation_date": 1628607015,
        "question_id": 68728982,
        "link": "https://stackoverflow.com/questions/68728982/free-up-memory-in-tdengine-database",
        "title": "Free up memory in TDengine database?",
        "body": "<p>the taosd memory increased from 8% to 25% after I executed several queries in taos client, why the memory did't go down after the I get query results? Is there any way to free up  memory in TDengine database?</p>\n"
    }, {
        "tags": ["storage", "iot"],
        "is_answered": false,
        "view_count": 8,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1629090075,
        "creation_date": 1629090075,
        "question_id": 68797583,
        "link": "https://stackoverflow.com/questions/68797583/what-is-tdengine-storage-logic-how-does-keep-and-days-configuration-work-togeth",
        "title": "what is TDengine storage logic\uff1f how does keep and days configuration work together\uff1f",
        "body": "<p>I'm trying to use TDengine in my IoT scenario,I have to know how TDengine deal with data</p>\n<p>it's difficult for me to understand this sentence in the official documentation.</p>\n<p>&quot;Given \u201cdays\u201d and \u201ckeep\u201d parameters, the total number of data files in a vnode is: keep/days+1.&quot;</p>\n<p>and I found the timestamp of future data couldn't be more than &quot;now+days&quot;, the old data couldn't be less than &quot;now-keep&quot;\uff1f I've understood the meaning of &quot;keep&quot;, &quot;days&quot;</p>\n<p>here is my question how exactly do they work together\uff1f</p>\n"
    }, {
        "tags": ["iot", "td-engine"],
        "is_answered": false,
        "view_count": 7,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1628583774,
        "creation_date": 1627983970,
        "last_edit_date": 1628583774,
        "question_id": 68633813,
        "link": "https://stackoverflow.com/questions/68633813/tdengine-why-does-query-through-restful-consume-more-cpu-compare-with-query-thro",
        "title": "TDengine why does query through RESTFUL consume more CPU compare with query through terminal",
        "body": "<p>I was examing the usage of RESTFUL connector. When I tried to run query through RESTFUL, I noticed that the cpu consumption for taosd is much higher compared with query through terminal.</p>\n<p>What is the difference causing this to happen? I check the document, but it did not discuss the cause of such difference.</p>\n"
    }, {
        "tags": ["sql", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 12,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1628583613,
        "creation_date": 1628090182,
        "last_edit_date": 1628583613,
        "question_id": 68653825,
        "link": "https://stackoverflow.com/questions/68653825/why-the-first-node-has-less-data-than-other-nodes-in-tdengine-cluster",
        "title": "why the first node has less data than other nodes in TDengine cluster",
        "body": "<p>I built a TDengine cluster with 4 nodes, created a database with 3 replicas and inserted some data into this database. I found that the first node always has less data than other nodes.\neg:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>taos&gt; use stress_test;\nDatabase changed.\n\ntaos&gt; show vgroups;\n    vgId     |   tables    |  status  |   onlines   | v1_dnode | v1_status | v2_dnode | v2_status | v3_dnode | v3_status | compacting  |\n========================================================================================================================================\n           3 |        1000 | ready    |           3 |        4 | master    |        3 | slave     |        2 | slave     |           0 |\n           4 |        1000 | ready    |           3 |        3 | master    |        4 | slave     |        2 | slave     |           0 |\n           5 |        1000 | ready    |           3 |        4 | master    |        3 | slave     |        2 | slave     |           0 |\n           6 |        1000 | ready    |           3 |        3 | master    |        2 | slave     |        4 | slave     |           0 |\n           7 |        1000 | ready    |           3 |        1 | master    |        3 | slave     |        4 | slave     |           0 |\nQuery OK, 5 row(s) in set (0.011133s)\n</code></pre>\n<p>From above example, we can see that node 1 has only 1 vnode, node 2, 3, 4, each has 5 vnodes.\nwhy node 1 has less data than other 3 nodes?</p>\n"
    }, {
        "tags": ["database", "iot", "timescaledb", "td-engine"],
        "is_answered": false,
        "view_count": 39,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1628425399,
        "creation_date": 1627571589,
        "question_id": 68578529,
        "link": "https://stackoverflow.com/questions/68578529/why-does-tdengine-consume-so-much-less-storage-space-compared-with-timescaledb",
        "title": "Why does TDengine consume so much less storage space compared with TimescaleDB?",
        "body": "<p>For the past few days, I have inserted around 80 million rows of data into TDengine, and it turns out the storage only consumes around 1GB. Compared with TimescaleDB's storage usage, TDengine only uses around 1/15 of TimescaleDB's storage space. For TimescaleDB, the storage consumption is usually around 15-18GB depending on the schema.</p>\n<p>The TDengine's version was 2.0.20.8 shown on taos client, and the version of TimescaleDB was 2.2.1</p>\n<p>Can anyone explain why TDengine consume so much less space compared with TimescaleDB? is this level of compression going to cause the data to lose its precision for TDengine?</p>\n"
    }, {
        "tags": ["database", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 9,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1628150328,
        "creation_date": 1628150328,
        "question_id": 68662701,
        "link": "https://stackoverflow.com/questions/68662701/tdengine-query-when-using-cluster",
        "title": "TDengine query when using cluster",
        "body": "<p>I have built a TDengine cluster with three dnodes, created 10M tables and inserted some data into the datbase. The replica I setted was 1.</p>\n<p>Due to the limited memory, when I run select sum(col1) from db.stb interval(5s), one dnode is killed by the system due to OOM. However, the query continues and gived me the result.</p>\n<p>Is query continued after one dnode down is acceptable? I am not sure if the returned result is correct, and I am thinking to allocate more memory to test the result.</p>\n"
    }, {
        "tags": ["sql", "database", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 23,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1627747882,
        "creation_date": 1627654618,
        "question_id": 68592463,
        "link": "https://stackoverflow.com/questions/68592463/how-to-adjust-parameters-in-tdengine-database",
        "title": "How to adjust parameters in TDengine database?",
        "body": "<p>When I use &quot;show databases&quot; command in taos shell, I see there are a lot of database parameters, like keep, days, cache, blocks</p>\n<pre class=\"lang-sql prettyprint-override\"><code>taos&gt; show databases;\n              name              |      created_time       |   ntables   |   vgroups   | replica | quorum |  days  |   keep0,keep1,keep(D)    |  cache(MB)  |   blocks    |   minrows   |   maxrows   | wallevel |    fsync    | comp | cachelast | precision | update |   status   |\n====================================================================================================================================================================================================================================================================================\n test                           | 2021-05-26 17:33:17.338 |           1 |           1 |       1 |      1 |     10 | 3650,3650,3650           |          16 |           6 |         100 |        4096 |        1 |        3000 |    2 |         0 | ms        |      0 | ready      |\n\nQuery OK, 1 row(s) in set (0.001774s)\n</code></pre>\n<p>To make best practice of TDengine database, how should I adjust these databases parameters?</p>\n"
    }, {
        "tags": ["sql", "database", "time-series", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 25,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1627490978,
        "creation_date": 1627490978,
        "question_id": 68564404,
        "link": "https://stackoverflow.com/questions/68564404/tdengine-import-from-csv-file",
        "title": "TDengine import from csv file",
        "body": "<p>Just found the speed for importing sorted csv file is faster than the speed for importing unsorted csv file in TDengine database, each csv file has 1000000 rows, the only difference is one file has timestamp sorted, the other has timestamp unsorted.</p>\n<p>Anyone can explain why importing sorted csv file is faster?</p>\n<pre class=\"lang-sql prettyprint-override\"><code>taos&gt; create table if not exists t1(ts timestamp, c1 int, c2 float, c3 int, c4 int);\nQuery OK, 0 of 0 row(s) in database (0.001659s)\n\ntaos&gt; insert into t1 file 'unsorted.csv';\nQuery OK, 1000000 of 1000000 row(s) in database (2.025508s)\n\ntaos&gt; create table if not exists t2(ts timestamp, c1 int, c2 float, c3 int, c4 int);\nQuery OK, 0 of 0 row(s) in database (0.001335s)\n\ntaos&gt; insert into t2 file 'sorted.csv';\nQuery OK, 1000000 of 1000000 row(s) in database (0.994504s)\n</code></pre>\n"
    }, {
        "tags": ["database", "iot", "td-engine"],
        "is_answered": false,
        "view_count": 10,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1627361876,
        "creation_date": 1627361876,
        "question_id": 68539187,
        "link": "https://stackoverflow.com/questions/68539187/tdengine-how-to-check-if-database-parameter-modification-is-effective",
        "title": "TDengine How to Check If Database Parameter Modification is Effective",
        "body": "<p>In TDengine's Documentation,it states that the database parameters can be modified through the following command</p>\n<pre><code>alter database &lt;db_name&gt; &lt;db_parameter&gt; &lt;value&gt;\n</code></pre>\n<p>For som parameters, like days, it seems the modification happens immediately. Right after I modify days from 10 to 1, I can no longer insert data 2 days after today.\nHowever, alteration of some other parameters like blocks and cache does not seem to be effective right away. Taosd's memory consumption did not decrease right away.\nCan someone tell me how to check if modification of these parameters is effective?</p>\n"
    }, {
        "tags": ["database", "iot"],
        "is_answered": true,
        "view_count": 32,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1627262345,
        "creation_date": 1627021629,
        "question_id": 68494903,
        "link": "https://stackoverflow.com/questions/68494903/tdengine-want-to-use-multiple-path-for-storing-data",
        "title": "TDengine Want to Use Multiple Path for Storing Data",
        "body": "<p>I am thinking about using different path when storing databases, and noticed the following config in TDengine's config file.</p>\n<pre><code># data file's directory\n# dataDir                   /var/lib/taos\n</code></pre>\n<p>Is it possible to set multiple dataDir and letting TDengine store files in several paths?</p>\n"
    }, {
        "tags": ["python", "tensorflow", "keras", "iot", "tensorflow-lite"],
        "is_answered": true,
        "view_count": 1744,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1626534561,
        "creation_date": 1573265824,
        "question_id": 58775848,
        "link": "https://stackoverflow.com/questions/58775848/tflite-cannot-set-tensor-dimension-mismatch-on-model-conversion",
        "title": "TFLIte Cannot set tensor: Dimension mismatch on model conversion",
        "body": "<p>I've a keras model constructed as follows</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>module_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\nbackbone = hub.KerasLayer(module_url)\nbackbone.build([None, 224, 224, 3])\nmodel = tf.keras.Sequential([backbone, tf.keras.layers.Dense(len(classes), activation='softmax')])\nmodel.build([None, 224, 224, 3])\nmodel.compile('adam', loss='sparse_categorical_crossentropy')\n</code></pre>\n\n<p>Then I load Caltech101 dataset from TF hub as follows</p>\n\n<pre><code>samples, info = tfds.load(\"caltech101\", with_info=True)\ntrain_samples, test_samples = samples['train'], samples['test']\ndef normalize(row):\n    image, label = row['image'], row['label']\n    image = tf.dtypes.cast(image, tf.float32)\n    image = tf.image.resize(image, (224, 224))\n    image = image / 255.0\n    return image, label\ntrain_data = train_samples.repeat().shuffle(1024).map(normalize).batch(32).prefetch(1)\ntest_data = test_samples.map(normalize).batch(1)\n</code></pre>\n\n<p>Now i'm ready to train and save my model as follows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model.fit_generator(train_data, epochs=1, steps_per_epoch=100)\nsaved_model_dir = './output'\ntf.saved_model.save(model, saved_model_dir)\n</code></pre>\n\n<p>At this point the model is usuable, I can evaluate an input of shape (224, 224, 3). I try to convert this model as follows:</p>\n\n<pre><code>def generator2():\n  data = train_samples\n  for _ in range(num_calibration_steps):\n    images = []\n    for image, _ in data.map(normalize).take(1):\n      images.append(image)\n    yield images\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\nconverter.representative_dataset = tf.lite.RepresentativeDataset(generator2)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\ntflite_default_quant_model = converter.convert()\n</code></pre>\n\n<p>The conversion triggers the following error</p>\n\n<pre><code>/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in FeedTensor(self, input_value)\n    110 \n    111     def FeedTensor(self, input_value):\n--&gt; 112         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\n    113 \n    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):\n\nValueError: Cannot set tensor: Dimension mismatch\n</code></pre>\n\n<p>Now there is a similar <a href=\"https://stackoverflow.com/questions/52012010/tensorflow-lite-valueerror-cannot-set-tensor-dimension-mismatch\">question</a> but in there case they are loading an already converted model unlike my case where the issue happens when I try to convert a model.</p>\n\n<p>The converter object is an auto generated class from C++ code using <a href=\"http://www.swig.org\" rel=\"nofollow noreferrer\">SWIG</a> which makes it difficult to inspect. How can I found the exact Dimension expected by the converter object?</p>\n"
    }, {
        "tags": ["azure", "powerbi", "iot", "azure-iot-central"],
        "is_answered": false,
        "view_count": 27,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1626502583,
        "creation_date": 1626467189,
        "question_id": 68414904,
        "link": "https://stackoverflow.com/questions/68414904/how-to-add-systemproperties-to-iot-central-message",
        "title": "How to add `systemProperties` to IOT Central Message",
        "body": "<p>I've been trying to follow the '<a href=\"https://docs.microsoft.com/en-us/azure/iot-central/core/howto-connect-powerbi\" rel=\"nofollow noreferrer\">Visualize and analyze your Azure IoT Central data in a Power BI dashboard</a>' tutorial to display data from Azure IoT Central in Power BI. However, when I open the file generated by the '<a href=\"https://appsource.microsoft.com/en/product/web-apps/iot-central.power-bi-solution-iot-central\" rel=\"nofollow noreferrer\">Power BI Solution for Azure IoT Central V3</a>', the tables and graphs are present, but nothing is populated.</p>\n<p>This is some information and steps I have taken to try and solve this:</p>\n<ul>\n<li>My first idea is that the message does not have <code>contentType:application/JSON</code> or <code>contentEncoding:utf-8</code> in the <code>systemProperties</code> header in the message (like <a href=\"https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-devguide-routing-query-syntax#message-routing-query-based-on-message-properties\" rel=\"nofollow noreferrer\">here</a>). I think this may be a problem because when I go to download raw data from the storage blob, I don't see the <code>systemProperties</code> or <code>messages</code> headers (I've attached a sample of what I do see below). I've looked around but I don't see how I would add these headers to message through IoT Central.</li>\n<li>I looked at the SQL database generated by the solution, and it appears there is no information being passed to it. I checked this by selecting 'Power BI (preview)' in the side menu of the SQL database.</li>\n<li>I have waited a few hours and the table and graphs are still not populated so I don't think it's a time lag issue.</li>\n</ul>\n<p>Any help would be great!</p>\n<p>Sample storage blob data:</p>\n<pre><code>{&quot;messageSource&quot;:&quot;telemetry&quot;,&quot;telemetry&quot;:{&quot;humidity&quot;:41.44,&quot;temperature&quot;:30.34,&quot;pressure&quot;:892.51},&quot;schema&quot;:&quot;default@v1&quot;,&quot;messageProperties&quot;:{},&quot;enrichments&quot;:{},&quot;applicationId&quot;:&quot;72236ce8-5edf-493f-8b9d-a7cef16c20bc&quot;,&quot;deviceId&quot;:&quot;jasper&quot;,&quot;enqueuedTime&quot;:&quot;2021-07-16T20:12:54.729Z&quot;,&quot;templateId&quot;:&quot;urn:modelDefinition:p08bqguhitr:xryhwfwxqgp&quot;}...\n</code></pre>\n"
    }, {
        "tags": ["azure", "iot", "azure-iot-hub", "azure-iot-hub-device-management"],
        "is_answered": true,
        "view_count": 40,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1625035121,
        "creation_date": 1623142709,
        "last_edit_date": 1624996368,
        "question_id": 67884519,
        "link": "https://stackoverflow.com/questions/67884519/azure-iot-device-to-cloud-metrics-graph-drops-to-zero-at-a-particular-time-stam",
        "title": "Azure IoT Device to Cloud, Metrics graph drops to zero at a particular time stamp",
        "body": "<p>I have an Azure IoT device connected to an Azure IoT Hub. The device sends 6 - 7 messages per minute. By looking at the D2C message metrics, I found an outlier, that states that at a specific time, the count of the D2C message was zero (see picture). As the messages are routed to a storage, I can check the storage to see if there are messages missing at that specific time, but the data saved in the storage shows that every message was received correctly at that time. Does any one know how that comes or if the metrics are not that reliable generally? If that's the case, what is the best practice to monitor the IoT Hub message transfer?</p>\n<p><a href=\"https://i.stack.imgur.com/ZI2wz.png\" rel=\"nofollow noreferrer\">IoT Hub D2C Message Metrics</a></p>\n<p><a href=\"https://i.stack.imgur.com/a8T5A.png\" rel=\"nofollow noreferrer\">EnqueuedTimeUtc in the storage</a></p>\n"
    }, {
        "tags": ["database", "database-design", "architecture", "iot"],
        "is_answered": true,
        "view_count": 42,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1624830665,
        "creation_date": 1623868558,
        "last_edit_date": 1624060790,
        "question_id": 68008302,
        "link": "https://stackoverflow.com/questions/68008302/sensor-network-calculating-features-design-database",
        "title": "Sensor network calculating features - design database",
        "body": "<p>How would you model a system involving sensors</p>\n<ul>\n<li>which have some spatial data like lat,lon,altitude,...</li>\n<li>that produce raw measurements let's say distance, temperature,...</li>\n<li>for the purpose of calculating &quot;features&quot; of the location they are at like the volume, temperature</li>\n</ul>\n<p>Here's my idea for tables</p>\n<pre><code>Sensor\n- id\n- model\n- type\n\nSensorMetadata\n- sensorId\n- timestamp // for time series to account for changing\n- lat\n- long\n- alt\n- metadata: json // some dynamic changeable data based on the domain lets say relative distance to something...\n- unit\n\nMeasurements\n- id\n- timestamp\n- sensorId\n- value\n\nFeature\n- id\n- type // complex features like volume\n- value\n- timestamp\n- // relation to location (maybe sensorMetadata)\n</code></pre>\n<p>1 would you model it to be specific for the domain if let's say tanks of fluids are in question or would you model it generic using &quot;features&quot; language?</p>\n<p>2 how and when would you calculate &quot;features&quot; based on measurements? Clearly here I'm missing\nwhat sensors would I take into account to calculate the feature of the location (let's say they use multiple sensors in some cases)</p>\n"
    }, {
        "tags": ["google-cloud-platform", "google-cloud-dataflow", "iot", "google-cloud-pubsub"],
        "is_answered": true,
        "view_count": 273,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1623210976,
        "creation_date": 1623059064,
        "last_edit_date": 1623210976,
        "question_id": 67869367,
        "link": "https://stackoverflow.com/questions/67869367/google-dataflow-pricing-streaming-mode",
        "title": "Google Dataflow Pricing Streaming Mode",
        "body": "<p>I'm new to Dataflow.\nI'd like to use the Dataflow streaming template &quot;Pub/Sub Subscription to BigQuery&quot; to transfer some messages, say 10000 per day.\nMy question is about pricing since I don't understand how they're computed for the streaming mode, with Streaming Engine enabled or not.\nI've used the Google Calculator which asks for the following:<br />\nMachine Type, Number of worker nodes used by the job, If streaming or Batch job, Number of GB of Persistent Disks (PD), Hours the job runs per month.</p>\n<p>Consider the easiest case, since I don't need many resources, i.e.</p>\n<ul>\n<li>Machine type: n1-standard1</li>\n<li>Max Workers: 1</li>\n<li>Job Type: Streaming</li>\n<li>Price: in us-central1</li>\n</ul>\n<p><strong>Case 1: Streaming Engine DISABLED</strong></p>\n<ul>\n<li>Hours using the vCPU = 730 hours (1 month always active). Is this always true for the streaming mode? Or there can be a case in a streaming mode in which the usage is lower?</li>\n<li>Persistent Disks: 430 GB HDD, which is the default value.</li>\n</ul>\n<p>So I will pay:</p>\n<ul>\n<li>(vCPU) 730 x $0.069(cost vCPU/hour) = $50.37</li>\n<li>(PD) 730 x $0.000054 x 430 GB = $16.95</li>\n<li>(RAM) 730 x $0.003557 x 3.75 GB = $9.74\nTOTAL: $77.06, as confirmed by the calculator.</li>\n</ul>\n<p><strong>Case 2 Streaming Engine ENABLED.</strong></p>\n<ul>\n<li>Hours using the v CPU = 730 hours</li>\n<li>Persistent Disks: 30 GB HDD, which is the default value</li>\n</ul>\n<p>So I will pay:</p>\n<ul>\n<li>(vCPU) 30 x $0.069(cost vCPU/hour) = $50.37</li>\n<li>(PD) 30 x $0.000054 x 430 GB = $1.18</li>\n<li>(RAM) 30 x $0.003557 x 3.75 GB = $9.74\nTOTAL: $61.29 PLUS the amount of Data Processed (which is extra with Streaming Engine)</li>\n</ul>\n<p>Considering messages of 1024 Byte, we have a traffic of 1024 x 10000 x 30 Bytes = 0.307 GB, and an extra cost of 0.307 GB x $0.018 = $0.005 (almost zero).</p>\n<p>Actually, with this kind of traffic, I will save about $15 in using Streaming Engine.\nAm I correct? Is there something else to consider or something wrong with my assumptions and my calculations?\nAdditionally, considering the low amount of data, is Dataflow really fitted for this kind of use? Or should I approach this problem in a different way?</p>\n<p>Thank you in advance!</p>\n"
    }, {
        "tags": ["video-streaming", "iot"],
        "is_answered": false,
        "view_count": 9,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1622864675,
        "creation_date": 1622864675,
        "question_id": 67846065,
        "link": "https://stackoverflow.com/questions/67846065/aws-ephemeral-video-pipelines-for-handling-consumer-facing-data-from-aws-greengr",
        "title": "AWS ephemeral video pipelines for handling consumer-facing data from AWS Greengrass",
        "body": "<p>I am trying to create video streaming pipelines for end-user IoT devices that function as a remote dvr (essentially), but I'm having trouble figuring out a cost-effective method of doing this. After rooting around the docs for AWS Kinesis, I found that though it is intended for this type of application (especially Kinesis Video Streams), it is cost-prohibitive. Additionally, due to the limitations of the onboard video processing capabilities, I would ideally be able to implement a solution that features something like signed urls (returned from authenticated https requests) that would act as network endpoints for server-side WebRTC sockets. The primary objective is the acquisition and storage of video footage in user-specific buckets (I'm using AWS Cognitio for end-user identity management); any real analysis or processing would occur afterwards, with the exception of encoding/decoding.</p>\n<p>Since I am using AWS Greengrass on the IoT devices, I figure that there is a way to design policies such that uploads matching metadata criteria (i don't remember the specifics but either video or streaming content-type) are automatically uploaded to user-specific, access controlled buckets. If you have any advice on this, it would be greatly appreciated.</p>\n<p>Additionally, I am unclear on the pros and cons of doing the upload through proper streaming vs. uploading small batches of data at regular intervals (i.e., uploading 15 sec clips) along with a metadata file that will be used to stitch the clips together back into one video later on.</p>\n<p>IoT devices:\nHardware:</p>\n<ul>\n<li>Nvidia Jetson Nano</li>\n<li>4x video streams</li>\n<li>Automotive application -&gt; cellular connectivity (pipeline needs to be resilient to interruptions</li>\n<li>Telemetry data is also being uploaded (this is less of a problem, since existing solutions work fine)</li>\n<li>Using GStreamer for Embedded Video Processing</li>\n</ul>\n"
    }, {
        "tags": ["azure", "azure-sql-database", "azure-cosmosdb", "iot"],
        "is_answered": false,
        "view_count": 66,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1622526459,
        "creation_date": 1622206127,
        "last_edit_date": 1622210137,
        "question_id": 67739161,
        "link": "https://stackoverflow.com/questions/67739161/storing-iot-data-in-azure-sql-vs-cosmos-vs-other-methods",
        "title": "Storing IOT Data in Azure: SQL vs Cosmos vs Other Methods",
        "body": "<p>The project I am working on as an architect has got an IOT setup where lots of sensors are sending data like water pressure, temperature etc. to an FTP(cant change it as no control over it due to security). From here few windows service on Azure pull the data and store it into an Azure SQL Database.</p>\n<p>Here is my observation with respect to this architecture:</p>\n<p>Problems: 1 TB limit in Azure SQL. With higher tier it can go to 4 TB but that's the max. So it does not appear to be infinitely scalable plus with size, the query issues could be a problem. Columnstore index and partitioning seem to be options but size limitation and DTUs is a deal breaker.</p>\n<p>Problem-2- IOT data and SQL Database(downstream storage) seem to be tightly coupled. If some customer wants to extract few months of data or even more with millions of rows, DB will get busy and possibly throttle other customers due to DTU exhaustion.</p>\n<p>I would like to have some ideas on possibly scaling this further. SQL DB is great and with JSON support it is awesome but a it is not horizontally scalable solution.</p>\n<p>Here is what I am thinking:</p>\n<ol>\n<li><p>All the messages should be consumed from FTP by Azure IOT hub by some means.</p>\n</li>\n<li><p>From the central hub, I want to push all messages to Azure Blob Storage in 128 MB files for later analysis at cheap cost.</p>\n</li>\n<li><p>At the same time,\u00a0 I would like all messages to go to IOT hub and from there to Azure CosmosDB(for long term storage)\\Azure SQL DB(Long term but not sure due to size restriction).</p>\n</li>\n</ol>\n<p>I am keeping data in blob storage because if client wants or hires a Machine learning team to create some models, I would prefer them to pull data from Blob storage rather than hitting my DB.</p>\n<p>Kindly suggest few ideas on this. Thanks in advance!!</p>\n<p>Chandan Jha</p>\n"
    }, {
        "tags": ["amazon-web-services", "iot", "serverless", "aws-iot", "aws-serverless"],
        "is_answered": false,
        "view_count": 32,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1621303889,
        "creation_date": 1621303889,
        "question_id": 67579093,
        "link": "https://stackoverflow.com/questions/67579093/using-aws-iot-core-for-real-time-notifications-on-the-front-end-app-from-the-bac",
        "title": "Using AWS IoT Core for real-time notifications on the front-end app from the backend",
        "body": "<p>It's a multi-tenant application. Our users are assigned permissions based on the groups they are in. The addition/deletion of users to groups does not happen very often but we would ideally like to get real-time notifications.</p>\n<p>If I were to consider IoT real-time messaging so that the client app will subscribe to IoT topics and the backend will push to IoT topic whenever a user gets added/deleted from a group,</p>\n<ol>\n<li><p>would it be a <strong>topic per tenant</strong> or a <strong>topic per group per tenant</strong> or <strong>a topic per user</strong>?</p>\n</li>\n<li><p>Also, how would the cost add up in this case given the addition/deletions to the group won't be very often but considering this approach just so that the permissions reflect in the real-time?</p>\n</li>\n</ol>\n<p>What are the pros/cons of using this approach over maintaining a table in DynamoDB for storing data of recently update user permissions? The only problem with the DynamoDB approach for us is that we would like to cache the JWT auth token at the authorizer and we were considering returning the updates from DDB on pre token generation trigger which means a delay of 1-5 minutes due to caching</p>\n"
    }, {
        "tags": ["protocols", "communication", "mqtt", "iot", "opc-ua"],
        "is_answered": true,
        "view_count": 19552,
        "favorite_count": 9,
        "score": 12,
        "last_activity_date": 1620900630,
        "creation_date": 1430142554,
        "last_edit_date": 1441554103,
        "question_id": 29897654,
        "link": "https://stackoverflow.com/questions/29897654/opc-ua-protocol-vs-mqtt-protocol",
        "title": "OPC UA protocol vs MQTT protocol",
        "body": "<p>I would like to compare OPC UA vs MQTT on basis of the general characteristics of both the protocols such as Overhead (Packets), Security, Information modeling and Reliability. Where can I find some sample data for Overhead, and other characteristics,  for each protocol so that I compare them. I need your suggestions. You can suggest any tool to compare these protocols. </p>\n"
    }, {
        "tags": ["http", "arduino", "iot", "nodemcu"],
        "is_answered": false,
        "view_count": 46,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1620848022,
        "creation_date": 1620848022,
        "question_id": 67509979,
        "link": "https://stackoverflow.com/questions/67509979/how-to-send-data-from-2-sensors-from-nodemcu-to-mysql-with-a-http-request",
        "title": "How to send data from 2 sensors, from nodemcu to mysql with a http request",
        "body": "<p>I want to send the rfid and weight together to the database.\nI can send either successfully through http and store in my database(local) using php on the other end, but how do I send the latest weight data as soon as there is a new rfid card present, I want both the data to be stored in the same table.\nCheck the code below and help me please.\nThanks in advance.</p>\n<pre><code>#include &lt;SPI.h&gt;\n#include &lt;MFRC522.h&gt;\n#include &lt;ESP8266HTTPClient.h&gt;\n#include &lt;ESP8266WiFi.h&gt;\n#include &lt;HX711_ADC.h&gt;\n#include &lt;EEPROM.h&gt;\n\n#define SS_PIN D4\n#define RST_PIN D2\n\n#define ERROR_PIN D0\n#define SUCCESS_PIN D1\n#define CONN_PIN D8\n\n//pins:\nconst int HX711_dout = D1; //mcu &gt; HX711 dout pin\nconst int HX711_sck = D8; //mcu &gt; HX711 sck pin\n\n//display text:\n\n\n//HX711 constructor:\nHX711_ADC LoadCell(HX711_dout, HX711_sck);\n\nconst int calVal_eepromAdress = 0;\nlong t;\n\nconst char *ssid = &quot;ssid&quot;; \nconst char *password = &quot;password&quot;;\n\nMFRC522 mfrc522(SS_PIN, RST_PIN);\nvoid setup() {\n   delay(1000);\n   Serial.begin(9600);\n   Serial.println();\n   Serial.println(&quot;Starting...&quot;);\n\n   LoadCell.begin();\n    float calibrationValue; // calibration value (see example file &quot;Calibration.ino&quot;)\n    calibrationValue = -20.35; // uncomment this if you want to set the calibration value in the sketch\n    #if defined(ESP8266)|| defined(ESP32)\n    //EEPROM.begin(512); // uncomment this if you use ESP8266/ESP32 and want to fetch the calibration value from eeprom\n    #endif\n    //EEPROM.get(calVal_eepromAdress, calibrationValue); // uncomment this if you want to fetch the calibration value from eeprom\n  \n    long stabilizingtime = 2000; // preciscion right after power-up can be improved by adding a few seconds of stabilizing time\n    boolean _tare = true; //set this to false if you don't want tare to be performed in the next step\n    LoadCell.start(stabilizingtime, _tare);\n    if (LoadCell.getTareTimeoutFlag()) {\n      Serial.println(&quot;Timeout, check MCU&gt;HX711 wiring and pin designations&quot;);\n      while (1);\n    }\n    else {\n      LoadCell.setCalFactor(calibrationValue); // set calibration value (float)\n      Serial.println(&quot;Startup is complete&quot;);\n    }\n\n   WiFi.mode(WIFI_OFF);    \n   delay(1000);\n   WiFi.mode(WIFI_STA);\n   WiFi.begin(ssid, password);\n   Serial.println(&quot;&quot;);\n\n   pinMode(CONN_PIN, OUTPUT);\n   pinMode(SUCCESS_PIN, OUTPUT);\n   pinMode(ERROR_PIN, OUTPUT);\n   \n   Serial.print(&quot;Connecting&quot;);\n   while (WiFi.status() != WL_CONNECTED) {\n     delay(500);\n     Serial.print(&quot;.&quot;);\n   }\n\n   Serial.println(&quot;&quot;);\n   Serial.print(&quot;Connected to &quot;);\n   Serial.println(ssid);\n   Serial.print(&quot;IP address: &quot;);\n   Serial.println(WiFi.localIP()); \n   \n   SPI.begin();\n   mfrc522.PCD_Init();\n}\n\nvoid sendRfidLog(long cardId) {\n  \n  if(WiFi.status() == WL_CONNECTED) {\n    HTTPClient http;\n    String postData = &quot;cardid=&quot; + String(cardId) + /*&quot;&amp;weight=&quot; + String(weight) +*/&quot;&amp;action=insertRfIdLog&quot;;\n    http.begin(&quot;http://192.168.1.9/gardensuite/php/rfid.php&quot;);            \n    http.addHeader(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;);  \n    \n    int httpCode = http.POST(postData); \n    String payload = http.getString();\n    Serial.println(httpCode);\n    Serial.println(payload);\n    \n    if(payload.equals(&quot;success&quot;)) {\n     digitalWrite(SUCCESS_PIN, HIGH);\n    } else {\n     digitalWrite(ERROR_PIN, HIGH);\n    }\n    \n    http.end();\n  }\n}\n\nvoid toggleConnStat() {\n  if(WiFi.status() == WL_CONNECTED) {\n    digitalWrite(CONN_PIN, HIGH);\n  } else {\n    digitalWrite(CONN_PIN, LOW);\n  }\n}\n\nvoid loop() {\n  \n  if ( mfrc522.PICC_IsNewCardPresent()){\n    if ( mfrc522.PICC_ReadCardSerial()){\n      long code=0;\n      for (byte i = 0; i &lt; mfrc522.uid.size; i++){\n        code=((code+mfrc522.uid.uidByte[i])*10);\n      }\n      Serial.print(code);\n      sendRfidLog(code);\n    }\n  }\n  \n  toggleConnStat();\n  delay (500);\n  \n  digitalWrite(SUCCESS_PIN, LOW);\n  digitalWrite(ERROR_PIN, LOW);\n\n  static boolean newDataReady = 0;\n    const int serialPrintInterval = 0; //increase value to slow down serial print activity\n  \n    // check for new data/start next conversion:\n    if (LoadCell.update()) newDataReady = true;\n  \n    // get smoothed value from the dataset:\n    if (newDataReady) {\n      if (millis() &gt; t + serialPrintInterval) {\n        float weight = LoadCell.getData();\n        Serial.print(&quot;Load_cell output val: &quot;);\n        Serial.println(weight);\n        newDataReady = 0;\n        t = millis();\n      }\n    }\n}\n</code></pre>\n"
    }, {
        "tags": ["statistics", "iot", "physics"],
        "is_answered": false,
        "view_count": 33,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1620803913,
        "creation_date": 1620736302,
        "last_edit_date": 1620803913,
        "question_id": 67486831,
        "link": "https://stackoverflow.com/questions/67486831/how-to-calculate-kwh-based-on-per-one-hour-data",
        "title": "How to calculate kwh based on per one hour data?",
        "body": "<p>I have streaming data  at every 3 seconds  saving in mongo, from there I am trying to calculate kwh per hour .\nvalue storing in  w<br />\nI want to calculate kwh of that hour\nI tried this calculation</p>\n<pre><code>sum  = w1+w2+w3---wn  of one hour  \ncount  = number of records per hour \nkwh   of one hour  = (sum/count)/1000\n</code></pre>\n<p>Is this calculation is correct . Please help me to figure out this problem</p>\n"
    }, {
        "tags": ["postgresql", "database-design", "bigdata", "iot", "multi-tenant"],
        "is_answered": false,
        "view_count": 30,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1620147694,
        "creation_date": 1620147694,
        "question_id": 67389081,
        "link": "https://stackoverflow.com/questions/67389081/iot-postgres-database-single-table-database-per-client-vs-shared-for-all-c",
        "title": "IoT - Postgres database - Single table &amp; database per client vs Shared for all clients",
        "body": "<p>I am creating a software platform displaying dashboards based on sensors data. Some key metrics / context :</p>\n<ul>\n<li>Sensors are placed within rooms and generating ~150 million records per client per year</li>\n<li>Platform should work for ~20-30 clients</li>\n<li>Only using Postgres (timeseries db might be too complex to learn so far), and if possible without using table partitionning of Postgres (might be too complex at this stage)</li>\n<li>Platform &amp; architecture should work 1 year</li>\n<li>Platform would display dashboards and aggregated data (at hour, day, month level) but I also need granular / raw data from sensors (e.g. at 5min interval) to trigger alerts</li>\n</ul>\n<p>What is your point of view / advice regarding the 2 options below :</p>\n<ul>\n<li><p>Option 1 : 1 database per client with 1 table for raw sensors data (~150 million records table after 1 year)</p>\n<ul>\n<li>Easier to handle data (150 million per table is ok --&gt; already tried to run queries on it and latency is ok with right index), more agile regarding client request (e.g. if one client asking for a specific dashboard / report in the front end), less risky as if 1 database bug -&gt; other clients would not rely on the same one</li>\n<li>But might be hard to maintain and can increase manual tasks notably regarding front end output (different queries / data sources per client)</li>\n</ul>\n</li>\n<li><p>Option 2 : 1 database for all clients with 1 big table gathering all raw sensors data (can be 4,5 billion rows after 1 year with no delete or 1,1 billion rows if only keeping 3 months data - implying regular delete) and 1 aggregation table aggregated raw data at hour level (&lt; 10 million rows) to display output data in front end</p>\n<ul>\n<li>Maybe more difficult to handle all data (time for aggregation queries on &gt;1bn rows table?, ingestion in a table of this size?, delete would take lot of time?, etc.)</li>\n<li>Complexity to be agile and potential risk increase / cross data contamination (bug for one client =&gt; bug for all clients)</li>\n<li>But faster queries based on aggregated data, easy maintenance and more automatized queries and front end output (same data source / database)</li>\n</ul>\n</li>\n</ul>\n<p>Below a picture with more detailed info / pros &amp; cons I gathered - would be great to have your advice on this question ?\nAlso I know Postgres will not be a viable option in the future (as timeseries db or Nosql can perform better) but I would like to stick with it for a first version so far for various reasons and learning curve.</p>\n<p>Many thanks for your help !</p>\n<p><a href=\"https://i.stack.imgur.com/ONLIv.png\" rel=\"nofollow noreferrer\">Picture with more detailed information on database architecture options</a></p>\n"
    }, {
        "tags": ["python", "django", "single-page-application", "iot", "dashboard"],
        "is_answered": false,
        "view_count": 91,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1619710413,
        "creation_date": 1619710413,
        "question_id": 67320562,
        "link": "https://stackoverflow.com/questions/67320562/is-django-a-good-choice-for-web-dashboard-that-works-with-data-from-iot-devices",
        "title": "Is Django a good choice for web-dashboard that works with data from IoT devices and sensor devices?",
        "body": "<p>I am a business owner and I would like to develop a web based service that uses the data from a number of sensors and IoT devices and analyze the data with scientific and mathematical methods. These data and their interpretations would be then be represented as reports, analytical summaries etc. My clients can login to the web dashboard and can see the reports and analytics.</p>\n<p>I would like to use to the flexibility and power of Python for data analytics, machine learning etc. My choice is Django for this purpose. I would like to know if I am making the right choice based on the following outcomes</p>\n<ol>\n<li>I need to serve more than 10k customers on a monthly basis</li>\n<li>I may have to scale up the resources and programming scopes significantly gradually</li>\n<li>I may have to opt for SPA(single page applications), rest APIs for additional functionalities and performance</li>\n<li>I may have to control the IoT devices according through the website</li>\n</ol>\n<p>Looking forward for your valuable insights</p>\n<p>Regards</p>\n"
    }, {
        "tags": ["sql", "amazon-web-services", "iot", "business-intelligence", "amazon-quicksight"],
        "is_answered": false,
        "view_count": 36,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1619546620,
        "creation_date": 1619468321,
        "last_edit_date": 1619546620,
        "question_id": 67273287,
        "link": "https://stackoverflow.com/questions/67273287/sum-data-in-time-interval-quicksight",
        "title": "sum data in time interval Quicksight",
        "body": "<p>I have a dataset that represents the mm of rain every 12 minutes. I would like to know how I can sum the total amount of rain in an event. This is <img src=\"https://i.stack.imgur.com/csXht.png\" alt=\"1\" /></p>\n<p>&quot;lluvia&quot; is rain in spanishI was trying using a calculated field <strong>previous mm rain</strong> like <code>lag(rain, [{Mexico Time} DESC], 1)</code> which could bring me the previous mm rain value and another calculated field <strong>total mm event rain</strong> like <code>ifelse({previous mm rain}&lt;&gt;0, sum(rain), 0)</code> but I\u00b4m still mising field wells to graph. The whole idea is to get the sum of rain in that only rain event or time interval when <code>lluvia != 0</code>, not all my rain register. <br> Example:\n<a href=\"https://i.stack.imgur.com/apVs4.png\" rel=\"nofollow noreferrer\">enter image description here</a>\n<br> The visual should display &quot;12 mm&quot; for that single rain event.\n<br> Thank you.</p>\n<p>(&quot;lluvia&quot; is rain in spanish)</p>\n"
    }, {
        "tags": ["python", "node.js", "mongodb", "mongoose", "iot"],
        "is_answered": false,
        "view_count": 28,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1618753393,
        "creation_date": 1618753393,
        "question_id": 67149255,
        "link": "https://stackoverflow.com/questions/67149255/extracting-data-through-mongoose",
        "title": "Extracting data through mongoose",
        "body": "<p>I am new to nodejs and react,I was trying to build IOT based weather station for which I am storing data in Mongodb.\nAs I can send data from my sensors to mongodb now I need to extract and send it to the frontend react but I wasn't able to get data from it I have tried learning on many blogs and documentation but I am missing something.</p>\n<p><a href=\"https://i.stack.imgur.com/97kCJ.png\" rel=\"nofollow noreferrer\">Mongodb data</a></p>\n<p>This is the form of data present in my mongodb.\nCan anyone help me develop the backend part for getting data or have any solution for this.</p>\n"
    }, {
        "tags": ["python", "mysql", "arraylist", "mariadb", "iot"],
        "is_answered": true,
        "view_count": 145,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1618489742,
        "creation_date": 1618483183,
        "question_id": 67106934,
        "link": "https://stackoverflow.com/questions/67106934/how-do-i-write-elements-of-a-list-into-mysql",
        "title": "How do I write elements of a list into mysql?",
        "body": "<p>I have a python list of about 200 values being generated every 5 seconds that need to be stored in a database with the timestamp. Because of the time factor, I think it is better stored in a column but I have not been able to figure out how. I can't pass the list variable directly into the database because MySQL does not support it. Does anyone have suggestions?</p>\n"
    }, {
        "tags": ["sql", "amazon-web-services", "iot", "amazon-athena"],
        "is_answered": true,
        "view_count": 57,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1618097490,
        "creation_date": 1618091914,
        "question_id": 67039634,
        "link": "https://stackoverflow.com/questions/67039634/union-of-4-tables-with-same-column-types-but-different-data-order-by-max-time",
        "title": "Union of 4 tables with same column types (but different data) order by max time in AWS Athena?",
        "body": "<p>I'm having an issue I have 4 tables that look like this:\n<br></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>device</th>\n<th>volume1</th>\n<th>volume2</th>\n<th>time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>device_id</td>\n<td>x</td>\n<td>y</td>\n<td>time_devicemessage</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Each table is for one single <strong>device</strong> so I have 4 devices, which send messages in different timestamps.\n<br><br>\nI would like to know a query for how to unite these 4 tables into 1 but showing only the last <strong>volume</strong> data from each table based on its timestamp.\n<br><br>\nSo it may look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>device</th>\n<th>volume1</th>\n<th>volume2</th>\n<th>time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>deviceA</td>\n<td>lastvalue (x)</td>\n<td>lastvalue (y)</td>\n<td>time_devicemessageA</td>\n</tr>\n<tr>\n<td>deviceB</td>\n<td>lastvalue (x)</td>\n<td>lastvalue (y)</td>\n<td>time_devicemessageB</td>\n</tr>\n<tr>\n<td>deviceC</td>\n<td>lastvalue (x)</td>\n<td>lastvalue (y)</td>\n<td>time_devicemessageC</td>\n</tr>\n<tr>\n<td>deviceD</td>\n<td>lastvalue (x)</td>\n<td>lastvalue (y)</td>\n<td>time_devicemessageD</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Thank you very much for your support, I would appreciate lots the help!</p>\n<p>Regards, Ruben.</p>\n"
    }, {
        "tags": ["simulation", "iot", "contiki", "lte", "m2m"],
        "is_answered": false,
        "view_count": 23,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1617704007,
        "creation_date": 1617704007,
        "question_id": 66966713,
        "link": "https://stackoverflow.com/questions/66966713/how-to-calculate-the-arrival-rate-of-node-in-conitki",
        "title": "How to calculate the arrival rate of node in conitki?",
        "body": "<p><strong>Arrival Rate = packet generation rate of each source = no of packets/sec</strong></p>\n<p>In contiki-&gt;examples-&gt;ipv6-&gt;rpl-collect = files are udp-sender.c, udp-sink.c to perform a packet aggregation.</p>\n<p>What I understand is that in contiki &quot;<strong>collect-common.c</strong>&quot; file will be the one who is performing this task.</p>\n<p>In this file there is a comment &quot;/* Send a packet every 60-62 seconds. */&quot; and have calculation.</p>\n<p>It is for 60-62 what if I want to set the arrival rate 10,20sec?</p>\n<p>I am unable to understand the logic. <strong>Kindly help me</strong>.</p>\n<pre><code>/*\n * Copyright (c) 2010, Swedish Institute of Computer Science.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. Neither the name of the Institute nor the names of its contributors\n *    may be used to endorse or promote products derived from this software\n *    without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE INSTITUTE AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED.  IN NO EVENT SHALL THE INSTITUTE OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n *\n */\n\n/**\n * \\file\n *         Example of how the collect primitive works.\n * \\author\n *         Adam Dunkels &lt;adam@sics.se&gt;\n */\n\n#include &quot;contiki.h&quot;\n#include &quot;lib/random.h&quot;\n#include &quot;net/netstack.h&quot;\n#include &quot;dev/serial-line.h&quot;\n#include &quot;dev/leds.h&quot;\n#include &quot;collect-common.h&quot;\n#include &quot;net/packetbuf.h&quot;\n#include &quot;net/ip/uip-udp-packet.h&quot;\n\n\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;ctype.h&gt;\n\nstatic unsigned long time_offset;\nstatic int send_active = 1;\n\n#ifndef PERIOD\n#define PERIOD 60\n#endif\n#define RANDWAIT (PERIOD)\n\n/*---------------------------------------------------------------------------*/\nPROCESS(collect_common_process, &quot;collect common process&quot;);\nAUTOSTART_PROCESSES(&amp;collect_common_process);\n/*---------------------------------------------------------------------------*/\nstatic unsigned long\nget_time(void)\n{\n  return clock_seconds() + time_offset;\n}\n/*---------------------------------------------------------------------------*/\nstatic unsigned long\nstrtolong(const char *data) {\n  unsigned long value = 0;\n  int i;\n  for(i = 0; i &lt; 10 &amp;&amp; isdigit(data[i]); i++) {\n    value = value * 10 + data[i] - '0';\n  }\n  return value;\n}\n/*---------------------------------------------------------------------------*/\nvoid\ncollect_common_set_send_active(int active)\n{\n  send_active = active;\n}\n/*---------------------------------------------------------------------------*/\nvoid\ncollect_common_recv(const linkaddr_t *originator, uint8_t seqno, uint8_t hops,\n                    uint8_t *payload, uint16_t payload_len)\n{\n  unsigned long time;\n  uint16_t data;\n  int i;\n\n  printf(&quot;%u&quot;, 8 + payload_len / 2);\n  /* Timestamp. Ignore time synch for now. */\n  time = get_time();\n  printf(&quot; %lu %lu 0&quot;, ((time &gt;&gt; 16) &amp; 0xffff), time &amp; 0xffff);\n  /* Ignore latency for now */\n  printf(&quot; %u %u %u %u&quot;,\n         originator-&gt;u8[0] + (originator-&gt;u8[1] &lt;&lt; 8), seqno, hops, 0);\n  for(i = 0; i &lt; payload_len / 2; i++) {\n    memcpy(&amp;data, payload, sizeof(data));\n    payload += sizeof(data);\n   // printf(&quot; %u&quot;, data);\n  }\n  printf(&quot;\\n&quot;);\n  printf(&quot;Sink Recieved &quot;);\n  printf(&quot;%d&quot;,packetbuf_datalen());\n  printf(&quot; bytes. \\n&quot;);\n  leds_blink();\n}\n/*---------------------------------------------------------------------------*/\nPROCESS_THREAD(collect_common_process, ev, data)\n{\n  static struct etimer period_timer, wait_timer;\n  PROCESS_BEGIN();\n\n  collect_common_net_init();\n\n  /* Send a packet every 60-62 seconds. */\n  etimer_set(&amp;period_timer, CLOCK_SECOND * PERIOD);\n  while(1) {\n    PROCESS_WAIT_EVENT();\n    if(ev == serial_line_event_message) {\n      char *line;\n      line = (char *)data;\n      if(strncmp(line, &quot;collect&quot;, 7) == 0 ||\n         strncmp(line, &quot;gw&quot;, 2) == 0) {\n        collect_common_set_sink();\n      } else if(strncmp(line, &quot;net&quot;, 3) == 0) {\n        collect_common_net_print();\n      } else if(strncmp(line, &quot;time &quot;, 5) == 0) {\n        unsigned long tmp;\n        line += 6;\n        while(*line == ' ') {\n          line++;\n        }\n        tmp = strtolong(line);\n        time_offset = clock_seconds() - tmp;\n        printf(&quot;Time offset set to %lu\\n&quot;, time_offset);\n      } else if(strncmp(line, &quot;mac &quot;, 4) == 0) {\n        line +=4;\n        while(*line == ' ') {\n          line++;\n        }\n        if(*line == '0') {\n          NETSTACK_RDC.off(1);\n          printf(&quot;mac: turned MAC off (keeping radio on): %s\\n&quot;,\n                 NETSTACK_RDC.name);\n        } else {\n          NETSTACK_RDC.on();\n          printf(&quot;mac: turned MAC on: %s\\n&quot;, NETSTACK_RDC.name);\n        }\n\n      } else if(strncmp(line, &quot;~K&quot;, 2) == 0 ||\n                strncmp(line, &quot;killall&quot;, 7) == 0) {\n        /* Ignore stop commands */\n      } else {\n        printf(&quot;unhandled command: %s\\n&quot;, line);\n      }\n    }\n    if(ev == PROCESS_EVENT_TIMER) {\n      if(data == &amp;period_timer) {\n        etimer_reset(&amp;period_timer);\n        etimer_set(&amp;wait_timer, random_rand() % (CLOCK_SECOND * RANDWAIT));\n      } else if(data == &amp;wait_timer) {\n        if(send_active) {\n          /* Time to send the data */\n          collect_common_send();\n        }\n      }\n    }\n  }\n\n  PROCESS_END();\n}\n/*---------------------------------------------------------------------------*/\n</code></pre>\n"
    }, {
        "tags": ["docker", "iot", "thingsboard"],
        "is_answered": false,
        "view_count": 27,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1617088760,
        "creation_date": 1617013124,
        "last_edit_date": 1617088760,
        "question_id": 66852419,
        "link": "https://stackoverflow.com/questions/66852419/comparison-of-2-values-with-2-different-devices-on-thingsboard",
        "title": "Comparison of 2 values with 2 different devices on Thingsboard",
        "body": "<p>I have 2 Raspberry Pi which are associated with 2 zones, I have a sensor between the 2 which moves, currently from a graph I am looking at the distance between the sensor and the 2 RPs to imagine where it is, but I would like to automate this process with a variable :\nWhen the value of one of the sensors is greater than the other, I would like a variable associated with the zone to go to 1, and to 0 in the other case.</p>\n<p><a href=\"https://i.stack.imgur.com/3jXsC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/3jXsC.png\" alt=\"enter image description here\" /></a></p>\n<p>I looked to see if it was possible to do this with the rule chain, but I cannot compare the sensors with their type name and their own values.</p>\n<p>Or I don't know if it is possible to do it through the intermediary of a personalized widget.</p>\n"
    }, {
        "tags": ["node.js", "amazon-web-services", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 88,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1615785515,
        "creation_date": 1615202465,
        "last_edit_date": 1615785515,
        "question_id": 66528877,
        "link": "https://stackoverflow.com/questions/66528877/stream-100mb-file-from-aws-cloud-to-the-thing-using-aws-iot",
        "title": "Stream 100MB+ file from AWS Cloud to the Thing using AWS IoT",
        "body": "<p>So here is the scenario. We have a IoT device/thing which upon setting up need some drivers and firmware. Some files are huge in size like 100MB+ and in those cases instead of sending the data in one go, we would like stream the data from the AWS Cloud to the THING.</p>\n<p>I have been looking for ways to do it, and there is an option like IoT Streaming service, but there is no basic code documentation on how to do it in Nodejs. I'm a newbie with nodejs and IoT stuff so if someone can point me towards some help documentation or sites to see how to do it, that would be great!</p>\n<p>Using the below code, I cannot transfer more than 10MB.</p>\n<pre><code>iot.createStream(params, function (err, data) {\n   if (err) return err;\n   return data;\n})\n</code></pre>\n<p>Error that we are getting:</p>\n<p>File size 221413376 of stream file 0 in stream  exceeds the file size limit</p>\n<p>Params for createStream:</p>\n<pre><code>    files: [ /* required */\n      {\n        fileId: 0,\n        s3Location: {\n          bucket: bucket, /* required */\n          key: params.s3key\n        }\n      }\n    ],\n    roleArn: config.jobRoleARN, /* required */\n    streamId: uuidv1(), /* required */\n   description: 'Get file as Stream'\n  }```\n</code></pre>\n"
    }, {
        "tags": ["google-cloud-platform", "mqtt", "load-balancing", "iot", "google-cloud-iot"],
        "is_answered": true,
        "view_count": 62,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1615421829,
        "creation_date": 1614931141,
        "last_edit_date": 1615421829,
        "question_id": 66488796,
        "link": "https://stackoverflow.com/questions/66488796/how-google-iot-core-does-the-mqtt-load-balancing",
        "title": "How Google IoT Core does the MQTT Load Balancing",
        "body": "<p>It is not possible to find any references on how Google Actually Load balance the MQTT within their IoT Core.</p>\n<p>Is it  MQTT 3.1.1 spec called Shared Subscriptions which allow multiple clients to consume messages in a distributed fashion ?</p>\n"
    }, {
        "tags": ["database", "iot"],
        "is_answered": false,
        "view_count": 21,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1615137062,
        "creation_date": 1615043354,
        "last_edit_date": 1615137062,
        "question_id": 66507148,
        "link": "https://stackoverflow.com/questions/66507148/appropriate-database-structure-for-iot-platform",
        "title": "Appropriate database structure for IoT platform",
        "body": "<p>I'm designing a web platform for collecting and visualizing data from IoT devices. Each device has several sensors and send data to the platform via MQTT. Each device has ID. Some of the sensors attached to the device also have IDs.\nAn example MQTT topics look like:</p>\n<pre><code>controller/ControllerID/current/4\ncontroller/ControllerID/flow/4\ncontroller/ControllerID/temp/SensorID\n</code></pre>\n<p>The simple approach is to have a separate table for each measurement, but in this case, any more sensors to the device or adding new ones, requires adding more tables.</p>\n<p>The idea is to have a database structure that allows adding new and different devices without need to changes.\nAs shown in the diagram there is one table for all measurements. All types such as temperature, current, flow, pressure, etc. are specified in other table. That allows to add more types of sensors to the device without affecting the database structure, but I'm concern that putting all measurement values in one table might cause performance issues.\nAny suggestions are highly appreciated. Diagram here:</p>\n<p><a href=\"https://i.stack.imgur.com/MfZXk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MfZXk.png\" alt=\"Diagram\" /></a></p>\n"
    }, {
        "tags": ["database", "amazon-web-services", "amazon-dynamodb", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 33,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1614605319,
        "creation_date": 1614600551,
        "last_edit_date": 1614602201,
        "question_id": 66421891,
        "link": "https://stackoverflow.com/questions/66421891/dynamodb-maps-vs-lists-for-storing-iot-data",
        "title": "DynamoDB Maps vs Lists for storing IoT Data",
        "body": "<p>I'm trying to store IoT Data from data loggers that can have a variety of sensors attached, below is an example. Each logger sends an MQTT message every 20 seconds</p>\n<pre><code>&quot;state&quot;: {\n    &quot;reported&quot;: {\n      &quot;batv&quot;: 5105,\n      &quot;ts&quot;: 1614595073655,\n      &quot;temp&quot;: 20,\n      &quot;humidity&quot;: 50\n    }\n  }\n</code></pre>\n<p>My Question is in terms of storing these MQTT messages/readings efficiently in a DynamoDB table, should i store the readings in a Map containing Maps like this. (Note this is currently what I'm doing and when the number of readings gets large, it is very slow to load in AWS DynamoDB console.)</p>\n<pre><code>{\n  &quot;readings&quot;: {\n    &quot;ts1614592810955&quot;: {\n      &quot;battery_level&quot;: 5089,\n      &quot;temp&quot;: 20,\n      &quot;humidity&quot;: 50\n    },\n    &quot;ts1614593692395&quot;: {\n      &quot;battery_level&quot;: 5093,\n      &quot;temp&quot;: 20,\n      &quot;humidity&quot;: 50\n    }\n  },\n  &quot;serial_number&quot;: &quot;TDG_logger_thing&quot;\n}\n</code></pre>\n<p>The alternative which I'm leaning towards, is by storing readings in a list</p>\n<pre><code>{\n  &quot;readings&quot;: [\n    {\n      &quot;batv&quot;: 5105,\n      &quot;ts&quot;: 1614594313407,\n      &quot;temp&quot;: 20,\n      &quot;humidity&quot;: 50\n    },\n     {\n      &quot;batv&quot;: 5105,\n      &quot;ts&quot;: 1614594313555,\n      &quot;temp&quot;: 20,\n      &quot;humidity&quot;: 50\n    }\n  ],\n  &quot;serial_number&quot;: &quot;TDG_Logger_Thing&quot;\n}\n</code></pre>\n<p>Anyone with knowledge on DynamoDB or storing IoT data have any suggestions? greatly appreciated</p>\n<p>(BTW The flow of data is)</p>\n<p><strong>Data Logger -&gt; AWS IoT -&gt; AWS Lambda -&gt; DynamoDB</strong></p>\n"
    }, {
        "tags": ["firebase", "firebase-realtime-database", "google-cloud-firestore", "components", "iot"],
        "is_answered": true,
        "view_count": 95,
        "favorite_count": 0,
        "closed_date": 1613551622,
        "score": 2,
        "last_activity_date": 1613568951,
        "creation_date": 1613468186,
        "last_edit_date": 1613568951,
        "question_id": 66221884,
        "link": "https://stackoverflow.com/questions/66221884/sync-firestore-with-realtime-database",
        "title": "Sync Firestore with Realtime database",
        "body": "<p>I have my flutter application connected with firestore.</p>\n<p>But when it comes to the IOT component that works with my app i have to use the realtime database. How can i auto update the firestore data with the realtime database</p>\n<p>Example</p>\n<blockquote>\n<p><br>Realtime\n<br>Vehicle -&gt; ABC123 -&gt; latitude:3.256</p>\n</blockquote>\n<blockquote>\n<p><br>Firestore\n<br>Vehicle -&gt; ABC123 -&gt; latitude:3.256</p>\n</blockquote>\n<p>Auto sync of the databases within a specific period time</p>\n"
    }, {
        "tags": ["tensorflow", "iot", "object-detection", "inference", "mobilenet"],
        "is_answered": false,
        "view_count": 152,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1611739679,
        "creation_date": 1596286363,
        "question_id": 63205411,
        "link": "https://stackoverflow.com/questions/63205411/i-trained-ssd-mobilenet-v1-on-custom-dataset-problem-in-size-reduction-for-run",
        "title": "I trained ssd mobilenet v1 on custom dataset , Problem in size reduction for running in jetson",
        "body": "<p>I trained ssd mobilenet v1 on custom dataset now I want to run in jetson, I converted it to frozen graph pb file using tensorflow object detection api, i want to run this model on jetson nano, but I eats 2.5 GB of RAM, while model size is only 22.3MB. I tried with Tensorrt FP16 conversion and still same memory consumption.</p>\n<p>I need that model in the size of 5 to 6 MB or at least it must consume less memory on inference.</p>\n"
    }, {
        "tags": ["azure", "timestamp", "iot", "azure-stream-analytics", "stream-analytics"],
        "is_answered": false,
        "view_count": 75,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1611656780,
        "creation_date": 1611439279,
        "last_edit_date": 1611656780,
        "question_id": 65864894,
        "link": "https://stackoverflow.com/questions/65864894/azure-stream-analytics-lag-function-works-fine-on-test-but-not-on-query",
        "title": "Azure Stream Analytics - LAG function works fine on test, but not on query",
        "body": "<p>I have the following query:</p>\n<pre><code>WITH currentVector AS (\nSELECT \n    UnitId, [Read], ReadTime,\n    --If it is the first read in the last 6 days\n    CASE\n        WHEN LAG(UnitId) OVER (PARTITION BY UnitId LIMIT DURATION(day, 6)) IS NULL \n        THEN 1 ELSE 0 END [IsFirstRead], \nFROM IoTServerEventHub\nTIMESTAMP BY ReadTime\n) \n\nSELECT \n    UnitId, [Read], ReadTime,\n    CASE \n        WHEN [IsFirstRead] = 0 \n        THEN 1\n        ELSE 0\n    END IsValid\nINTO DestinationEventHub\nFROM currentVector\n</code></pre>\n<p>When I test this query in Azure portal - it works fine. If I have 2 reads and ReadTime is more than 6 days between them (even more than 1 year between one read and the next read) - both reads are marked as &quot;first in 6 days&quot;. But, when I run this query in production - the first read is marked as first, but the second read is marked as not first, although more than 6 days separate between them, and timestamp is by ReadTime and not by System.</p>\n<p>Any reason why?</p>\n<p>Thanks!</p>\n"
    }, {
        "tags": ["protocols", "mqtt", "iot", "hivemq"],
        "is_answered": true,
        "view_count": 82,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1611573136,
        "creation_date": 1611568069,
        "last_edit_date": 1611573136,
        "question_id": 65882320,
        "link": "https://stackoverflow.com/questions/65882320/does-mqtt-protocol-support-database",
        "title": "Does MQTT protocol support database?",
        "body": "<p>As we know MQTT have using subscribe/publish method. May i know what platform user can save the database using MQTT protocol. Its hivemq or mosquito support database so i can see previous data recorded from the sensor?\nIf MQTT can support database. What other method beside using apache webserver.</p>\n"
    }, {
        "tags": ["google-cloud-platform", "iot", "google-cloud-pubsub", "esp32"],
        "is_answered": true,
        "view_count": 34,
        "favorite_count": 0,
        "score": -2,
        "last_activity_date": 1611524462,
        "creation_date": 1608185091,
        "last_edit_date": 1608202552,
        "question_id": 65335438,
        "link": "https://stackoverflow.com/questions/65335438/esp32-to-gcp-iot-core-connection-failing",
        "title": "ESP32 to GCP IOT Core connection failing",
        "body": "<p>I want my data to move from iot device (esp32) to gcp iot core to cloud pub/sub. The issue is when I pull a message from pub/sub subscription the message body field displays &quot;device-esp32-connected&quot; rather than the actual json message that esp32 had sent.</p>\n<p>My json file's structure is like:</p>\n<pre><code>{\n    &quot;Key1&quot;:&quot;Value1&quot;,\n    &quot;Key2&quot;:&quot;Value1&quot;,\n    &quot;Key3&quot;:&quot;Value1&quot;,\n    &quot;List1&quot;: [\n        {\n            &quot;key1&quot;:&quot;value1&quot;,\n            &quot;key2&quot;:&quot;value2&quot;,\n            &quot;key3&quot;:&quot;value3&quot;\n        },\n        {\n            &quot;key1&quot;:&quot;value1&quot;,\n            &quot;key2&quot;:&quot;value2&quot;,\n            &quot;key3&quot;:&quot;value3&quot;\n        }\n    ]\n}\n</code></pre>\n<p>My actual json has a lot more fields in the &quot;list1&quot; list. It works if I decrease the number of fields but I cannot decrease the number of fields for my project.</p>\n"
    }, {
        "tags": ["python", "node.js", "database", "mongodb", "iot"],
        "is_answered": false,
        "view_count": 31,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1611006628,
        "creation_date": 1611006628,
        "question_id": 65782538,
        "link": "https://stackoverflow.com/questions/65782538/mongo-aggregation-bucket-duration-by-time-intervals",
        "title": "Mongo aggregation - bucket duration by time intervals",
        "body": "<p>I have scoured StackOverflow and other potential sources of help with regards to this but I have not been able to find anything similar...</p>\n<p>I am collecting status data in a Mongo collection that looks like this:</p>\n<pre><code>{\n  uniqueId: 1,\n  start_time: &quot;2020-01-01 00:00:00.000Z&quot;,\n  end_time: &quot;2020-01-01 05:00:00.000Z&quot;,\n  status: &quot;NO ALARM&quot;\n}\n</code></pre>\n<p>My goal is to generate reports from this data that calculate the duration of a type of status by an arbitrary time interval:</p>\n<pre><code>{\n  uniqueId: 1,\n  timestamp: &quot;2020-01-01 00:00:00.000Z&quot;,\n  duration: 1 // minute\n},\n{\n  uniqueId: 1,\n  timestamp: &quot;2020-01-01 00:01:00.000Z&quot;\n  duration: 1\n}\n...\n</code></pre>\n<p>The BIGGEST problem I've encountered is the fact that the duration can spread over multiple time periods and it doesn't look like there's a good way to deal with this in Mongo aggregation.</p>\n<p>Am I overthinking this problem or am I trying to use Mongo Aggregation for something it's not meant to do?</p>\n"
    }, {
        "tags": ["server", "arduino", "iot", "esp32", "aws-iot"],
        "is_answered": true,
        "view_count": 76,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1610704689,
        "creation_date": 1610679640,
        "question_id": 65729886,
        "link": "https://stackoverflow.com/questions/65729886/best-approach-to-connect-multiple-temperature-sensors-to-a-mobile-web-app",
        "title": "best approach to connect multiple temperature sensors to a mobile web app",
        "body": "<p>I am a web developer and I am starting to learn about the world of IoT.</p>\n<p>Because of the vaccines arrival to my country (Argentina) I got asked to build 80 temperature sensors to monitor them and I have some questions about it.</p>\n<p>What would be the best way to connect all of them to the cloud?</p>\n<ol>\n<li><p>If I use for example aws iot platform, do you know how much it would cost monthly for just sending\nand storing temperature logs for each sensor (remember, there are 80 of them)?</p>\n</li>\n<li><p>Is there any language/environment/protocol that works better for IoT? Because it's a constant flow\nof lightweight data...</p>\n</li>\n<li><p>Is there a better way to connect them to the internet besides using esp32 modules for\neach?(I saw a tutorial that said it's possible to connect some more to a single esp32 module)</p>\n</li>\n</ol>\n<p>If you have any advice I'd love to hear it. I know how to code but when it comes to backend and specially server stuff I have a lot to learn.</p>\n"
    }, {
        "tags": ["gps", "iot", "lora", "lorawan"],
        "is_answered": true,
        "view_count": 57,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1610021515,
        "creation_date": 1604912922,
        "question_id": 64748682,
        "link": "https://stackoverflow.com/questions/64748682/lorawan-is-it-possible-to-track-athletes",
        "title": "LoRaWan - is it possible to track athletes?",
        "body": "<p>For a long time I have been looking at LoRaWan technology for solving the problem of sports traking (in particular for orienteering), but the information is very fragmented and contradictory. I would like to know how difficult it is to implement the scheme described below. Links to examples of hardware, suitable articles on the topic, and in general any materials for thought are welcome.</p>\n<p><strong>Given:</strong>\nOn a relief forest terrain, 5 * 5 kilometers for example, control points are located, athletes run along it and visit these points. The terrain can be complicated by rocks, buildings, etc.</p>\n<p><strong>A task:</strong>\nRecord the athlete's visit to the checkpoint. A visit is successful, if the athlete is at a distance of 1-5 meters from the point. Give feedback to the athlete (light and sound signal that he has succeed, and he can proceed to the next point). Transmit athlete ID and point visited ID to base station. Ideally, transmit the position of the athlete (GPS tracking).</p>\n<p><strong>Requiremenets (in order of decreasing priority):</strong></p>\n<ol>\n<li>Lossless communication, signal loss is unacceptable.</li>\n<li>The price of a device wearable by an athlete should not exceed 40 eur, ideally not more than 10 eur. (up to 5 eur will generally be an absolute success). The price of the check point equipment should not exceed 50 eur. (check point must be weatherproof).</li>\n<li>If possible, not too expensive base station. &quot;Outdoor&quot; station from <em>The Things Network</em> worth ~ 450 eur. looks expensive. The station can be placed in a warm place (in a car, for example), and only an antenna outside.</li>\n<li>Ease of deployment and programming. Ideally, just turn everything on and collect data in CSV (for example) to process later. Slightly less - the ability to write a handler in any high-level language (python, C#, etc).</li>\n<li>The possibility of organizing control points in the mesh network, in case one of them is outside the range of reliable reception, or the ability to easily increase the range by installing additional relay nodes.</li>\n</ol>\n<p>Thanks!</p>\n"
    }, {
        "tags": ["iot", "grafana", "influxdb", "flux-influxdb"],
        "is_answered": false,
        "view_count": 40,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1608532419,
        "creation_date": 1608532419,
        "question_id": 65388424,
        "link": "https://stackoverflow.com/questions/65388424/how-to-handle-discrete-data-in-influxdb-and-flux-query",
        "title": "How to handle discrete data in Influxdb and Flux query",
        "body": "<p>I am very new (can say noob) in Influxdb and considering influx database for my next project. I\u2019m working on a factory project where I\u2019m getting data from PLC and data is in narrow format (get data only when data change and function previous fill will work fine for this task), now i getting data in format like</p>\n<pre><code>timestamp            | machine_state | state code | good count | reject count |watch dog\n2020-12-21T03:07:52Z | fault         | 3          | 500        | 3            | true\n2020-12-21T03:08:48Z | null          | null       | null       | null         | false\n2020-12-21T03:09:32Z | idle          | 1          | null       | null         | true\n2020-12-21T03:10:11Z | executing     | 1          | 510        | 4            | false\n2020-12-21T03:11:52Z | null          | null       | 520        | null         | true\n2020-12-21T03:15:22Z | fault         | 5          | 540        | null         | false\n2020-12-21T03:20:52Z | null          | null       | null       | null         | true\n</code></pre>\n<p>i\u2019m expecting following out put from this data (assuming current time is 2020-12-21T03:25:52Z )</p>\n<pre><code>event history = [\n{\nmachine_state:\u201cfault\u201d,\nfrom:2020-12-21T03:07:52Z,\nto:2020-12-21T03:09:48Z,\nduration:3 min.\nstate_code:3\n},\n{\nmachine_state:\u201cidle\u201d,\nfrom:2020-12-21T03:09:48Z,\nto:2020-12-21T03:10:32Z,\nduration:1 min.\nstate_code:1\n},\n{\nmachine_state:\u201cexecuting\u201d,\nfrom:2020-12-21T03:10:32Z,\nto:2020-12-21T03:15:22Z,\nduration:5 min.\nstate_code:1\n},\n{\nmachine_state:\u201cfault\u201d,\nfrom:2020-12-21T03:15:22Z,\nto:2020-12-21T03:25:52Z (current time),\nduration:10 min.\nstate_code:5\n}\n],\n</code></pre>\n<p>Thanks in advance</p>\n"
    }, {
        "tags": ["node.js", "database", "mongodb", "iot"],
        "is_answered": false,
        "view_count": 29,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1608443679,
        "creation_date": 1608370687,
        "question_id": 65368426,
        "link": "https://stackoverflow.com/questions/65368426/put-some-data-in-mongodb-collection-every-5-minutes",
        "title": "Put some data in MongoDB collection every 5 minutes",
        "body": "<p>I have IoT application that monitors humidity and temperature in my room and displays it in html page that I created. Now I need to put data in MongoDB database. Temperature and humidity data is updated every 2 seconds and I think that is too much information for database. I want to store that info in database every 5 minutes. What function can I use to do that?</p>\n"
    }, {
        "tags": ["mongodb", "time-series", "iot", "bucket"],
        "is_answered": false,
        "view_count": 40,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1607172272,
        "creation_date": 1607172272,
        "question_id": 65157062,
        "link": "https://stackoverflow.com/questions/65157062/mongodb-how-to-model-a-collection-schema-to-store-and-compare-timeframe-series",
        "title": "MongoDb: How to model a collection schema to store and compare timeframe series",
        "body": "<p>I am trying to store and compare series of data that represent the timeframe of a particular wireless broadcast of random numbers overtime. The data are sent by multiple IOT devices. (i.e.: contact tracing)</p>\n<p>Since it is a wireless broadcasted number, I want to query what numbers have been broadcasted from all devices in a specific timeframe and which of the other devices have become in contact with the broadcaster, by comparing the numbers by equality and then the timeframes by overlap.</p>\n<p>I'm now trying to model a Mongo db collection in order to be able to quickly query and group results by devices that have met each other within a given time range.</p>\n<p>The data sent to the server are structured in this fashion:</p>\n<p>This, for example, is device id &quot;1&quot; telling the server that it has wirelessly broadcasted the randomId &quot;1e51a3e7&quot; From midnight to 1 am on the 11th november 2020.</p>\n<pre><code>    {\n      type       : &quot;broadcasted&quot;,\n      device     : &quot;1&quot;,\n      randomId   : &quot;1e51a3e7&quot;,\n      dateFrom   : &quot;2020-11-11T00:00:00.000+00:00&quot;,\n      dateTo     : &quot;2020-11-11T01:00:00.000+00:00&quot;,\n    }\n</code></pre>\n<p>This is another device, id &quot;2&quot;, that is telling the server that it has received the randomId &quot;1e51a3e7&quot; between 00:15 and 00:20 on the same day.</p>\n<pre><code>    {\n      type       : &quot;received&quot;,\n      device     : &quot;2&quot;,\n      randomId   : &quot;1e51a3e7&quot;,\n      dateFrom   : &quot;2020-11-11T00:15:00.000+00:00&quot;,\n      dateTo     : &quot;2020-11-11T00:20:00.000+00:00&quot;,\n    }\n</code></pre>\n<p>I'd like to search my collection(s), having 100k documents stored per day, to have a result of every device combination, with multiple randomId and timeframes, grouped by device:</p>\n<pre><code>hostDevice:&quot;1&quot;\nguests:[\n  {\n   guestDevice: &quot;2&quot;,\n   meetings:[\n       { timeframes: \n          {\n           dateFrom:2020-11-11T00:15:00.000+00:00\n           dateTo:2020-11-11T00:20:00.000+00:00 \n         }\n       }, \n       {...}\n     ]\n    ]\n</code></pre>\n<p>The model I've adopted is most likely wrong, but so far I've tried this working version:</p>\n<p>Collection 'numbers':</p>\n<pre><code>_id: 5fc7db930323947e20b81c52\ntype: &quot;received&quot;\nrandomId: &quot;1e51a3e7&quot; \ndevice: &quot;2&quot;\ntimeframes: Array\n             0:Object\n                 _id:5fc7db930323947e20b81c53\n                 dateFrom:2020-11-11T00:15:00.000+00:00\n                 dateTo:2020-11-11T00:20:00.000+00:00\n             1:Object\n             2:Object\n             3:Object\n\n_id: 5fc7db930323947e20b81c55\ntype: &quot;broadcasted&quot;\nrandomId: &quot;1e51a3e7&quot; \ndevice: &quot;1&quot;\ntimeframes: Array\n             0:Object\n                 _id:5fc7db930323947e20b81c56\n                 dateFrom:2020-11-11T00:00:00.000+00:00\n                 dateTo:2020-11-11T01:00:00.000+00:00\n             1:Object\n             2:Object\n             3:Object\n</code></pre>\n<p>In this way I'm able to query and compare the timeframes of 500 devices having 50000 timeframes per day in less than 1 second using a $graphLookup on the randomId key over the same collection, associating the broadcasted with the received. (if I use $lookup instead, it takes much more time)\nBut, when I try to group them by device after the timeframe comparison (that checks if they are compatible), I get &quot;operation exceeded time limit&quot;, it doesn't matter how long I set the timeout. This does not happens if I filter the device list beforehand restricting the number of documents.\nI also noticed that if I perform an unwind of the &quot;guest&quot; device timeframes after the graphLookup it exceeds running time aswell, unless I specify the option &quot;preserveNullAndEmptyArrays: true&quot;: in that case it takes less than a second.</p>\n<p>It is important to notice that I need a 1 minute resolution at best. I don't even need to store the full timestamp, I just need to efficiently compare the timeframes within 1 minute margin.</p>\n<p>I'm also trying to understand if a day-based <a href=\"https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices\" rel=\"nofollow noreferrer\">bucketing</a> solution could fit my needs improving query performances.</p>\n<p><strong>Question 1</strong>: Is this the best way of storing timeseries data and compare them?</p>\n<p><strong>Question 2</strong>: Why does unwind without option (or even &quot;timeframes&quot;: {$match: {$ne: []}} crashes if the array comes from the graphLookup?</p>\n<p><strong>Question 3</strong>: are indexes used after a $graphLookup operation?</p>\n<p><strong>Question 4</strong>: is there a way to organize data in a way that I can efficiently apply indexes to?</p>\n"
    }, {
        "tags": ["sqlite", "cloud", "iot", "data-stream"],
        "is_answered": false,
        "view_count": 19,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1607074140,
        "creation_date": 1607074140,
        "question_id": 65141029,
        "link": "https://stackoverflow.com/questions/65141029/streaming-iot-data-from-sql-lite-db-to-cloud-service-provider",
        "title": "Streaming IOT data from SQL Lite DB to Cloud Service Provider",
        "body": "<p>If we are storing the IOT sensors data to SQL Lite database running within Raspberry Pi, what are some of the approaches to stream this data to any cloud service provider?\nSo what we want to achieve is something like this:</p>\n<pre><code>Sensor --&gt; SQL Lite (Persistent Storage) --&gt; Auto Sync with Cloud DB\n</code></pre>\n"
    }, {
        "tags": ["sql-server", "database-design", "iot", "database-performance"],
        "is_answered": true,
        "view_count": 277,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1606710056,
        "creation_date": 1606609215,
        "last_edit_date": 1606710056,
        "question_id": 65055774,
        "link": "https://stackoverflow.com/questions/65055774/database-design-for-iot-application",
        "title": "Database design for IoT application",
        "body": "<p>Our application shows near-real-time IoT data (up to 5 minute intervals) for our customers' remote equipment.</p>\n<p>The original pilot project stores every device reading for all time, in a simple &quot;Measurements&quot; table on a SQL Server 2008 database.\nThe table looks something like this:</p>\n<p><code>Measurements: (DeviceId, Property, Value, DateTime)</code>.</p>\n<p>Within a year or two, there will be maybe 100,000 records in the table per device, with the queries typically falling into two categories:</p>\n<ul>\n<li>&quot;Device latest value&quot; (95% of queries): looking at the latest value only</li>\n<li>&quot;Device daily snapshot&quot; (5% of queries): looking at a single representative value for each day</li>\n</ul>\n<p>We are now expanding to 5000 devices.  The <code>Measurements</code> table is small now, but will quickly get to half a billion records or so, for just those 5000 devices.</p>\n<p>The application is very read-intensive, with frequently-run queries looking at the &quot;Device latest values&quot; in particular.</p>\n<p><strong>[EDIT #1: To make it less opinion-based]</strong></p>\n<h2>What database design techniques can we use to optimise for fast reads of the &quot;latest&quot; IoT values, given a big table with years worth of &quot;historic&quot; IoT values?</h2>\n<p>One suggestion from our team was to store <code>MeasurementLatest</code> and <code>MeasurementHistory</code> as two separate tables.</p>\n<p><strong>[EDIT #2: In response to feedback]</strong></p>\n<p>In our test database, seeded with 50 million records, and with the following index applied:</p>\n<p><code>CREATE NONCLUSTERED INDEX [IX_Measurement_DeviceId_DateTime] ON Measurement (DeviceId ASC, DateTime DESC) </code></p>\n<p>a typical &quot;get device latest values&quot; query (e.g. below) still takes more than 4,000 ms to execute, which is way too slow for our needs:</p>\n<pre><code>SELECT DeviceId, Property, Value, DateTime\nFROM Measurements m\nWHERE m.DateTime = (\n  SELECT MAX(DateTime) \n  FROM Measurements m2\n  WHERE m2.DeviceId = m.DeviceId)\n</code></pre>\n"
    }, {
        "tags": ["iot", "contiki", "cooja"],
        "is_answered": false,
        "view_count": 136,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1605356001,
        "creation_date": 1605356001,
        "question_id": 64833759,
        "link": "https://stackoverflow.com/questions/64833759/how-to-get-pdrpacket-delivery-ratio-and-overhead-of-a-network-in-cooja",
        "title": "How to get PDR(Packet Delivery Ratio) and overhead of a network in Cooja?",
        "body": "<br>\nI am new to cooja and I want to calculate PDR and overhead for a network consisting of 10 udp-senders and 1 udp-sink.<br>\nSo how should I calculate PDR and overhead here? Like what steps to be followed?\n \n"
    }, {
        "tags": ["machine-learning", "dataset", "data-science", "iot"],
        "is_answered": false,
        "view_count": 84,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1604774781,
        "creation_date": 1604676902,
        "question_id": 64717494,
        "link": "https://stackoverflow.com/questions/64717494/which-machine-learning-algorithm-should-i-use-to-predict-if-particular-parking-s",
        "title": "Which machine learning algorithm should i use to predict if particular parking space will be occupied?",
        "body": "<p>I'm working on my idea for Master thesis topic.\nI get a dataset with milions of records which describe on-street parking sensors.</p>\n<p>Data i have :\n-vehicle present on particular sensor ( true  or false)\nIt's normal that there are few parking event where there are False values with different duration time in a row.</p>\n<p>-arrival time and departure time(month,day,hour,minute and even second)\n-duration in minutes</p>\n<p>And few more columns, but i don't have any idea how to show in my analysis that &quot;continuity of time&quot; and\nreflect this in the calculations for a certain future time based on the time when the parking space was usually free or occupied.</p>\n<p>Any ideas?</p>\n"
    }, {
        "tags": ["iot", "contiki", "cooja"],
        "is_answered": false,
        "view_count": 83,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1604384743,
        "creation_date": 1604315954,
        "last_edit_date": 1604384743,
        "question_id": 64644546,
        "link": "https://stackoverflow.com/questions/64644546/how-to-use-collect-view-on-a-network-consisting-of-sky-motes-of-rpl-border-route",
        "title": "How to use collect view on a network consisting of sky motes of rpl border router and websense (sky mote) in cooja simulator",
        "body": "<br/>\nI am new in contiki cooja.<br/>\nI created a network in cooja which has one rpl border router and others are websense(Tmote sky)<br/>\nIs there a way to use collect view here to get performance metrics like power consumption, hops etc (like shown in collect view).<br/>\nIf yes can anyone tell how can it be done or is there any other way to get power consumption, latency etc.\n"
    }, {
        "tags": ["mqtt", "iot", "influxdb", "telegraf"],
        "is_answered": false,
        "view_count": 73,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1603726724,
        "creation_date": 1603700431,
        "question_id": 64533567,
        "link": "https://stackoverflow.com/questions/64533567/store-data-from-an-mqtt-queue-into-two-influxdb-measurements",
        "title": "Store data from an mqtt queue into two influxdb measurements",
        "body": "<p>I set up an influxdata architecture composed of telegraf, influxdb and chronograf to collect data from ambient sensors (temperature, pressure and hygrometry). For now, I am perfectly collecting data and metadata from mqtt to a measurement with infinite data retention.</p>\n<p>Now, the users of the project would like that, from a record passing through mqtt, the data are stored infinitely in one measurement and the metadata are stored in a second measurement with a retention of 6 months.</p>\n<p>Currently, I have no problem parsing json, decoding base64, converting values into tags and fields, renaming them or storing them in ONE measurement with telegraf. But I'm not very good at duplicating them to two measurements and processing them in parallel.</p>\n<p>For example, here is the json frame that I capture from mqtt :</p>\n<pre><code>{\n    &quot;applicationID&quot;:&quot;1&quot;,\n    &quot;applicationName&quot;:&quot;chirp-app&quot;,\n    &quot;deviceName&quot;:&quot;0200000001&quot;,\n    &quot;devEUI&quot;:&quot;60c5a8fffe76ea89&quot;,\n    &quot;rxInfo&quot;:[\n        {\n            &quot;gatewayID&quot;:&quot;60c5a8fffe76154b&quot;,\n            &quot;uplinkID&quot;:&quot;70254019-7f5b-42cd-8cdf-c6f1eaf421c8&quot;,\n            &quot;name&quot;:&quot;rak7249&quot;,\n            &quot;time&quot;:&quot;2020-10-23T14:29:14.260435Z&quot;,\n            &quot;rssi&quot;:-76,\n            &quot;loRaSNR&quot;:9,\n            &quot;location&quot;:{\n                &quot;latitude&quot;:43.50438,\n                &quot;longitude&quot;:1.52947,\n                &quot;altitude&quot;:293\n            }\n        }\n    ],\n    &quot;txInfo&quot;:{\n        &quot;frequency&quot;:868100000,\n        &quot;dr&quot;:0\n    },\n    &quot;adr&quot;:true,\n    &quot;fCnt&quot;:1290,\n    &quot;fPort&quot;:8,\n    &quot;data&quot;:&quot;CAIBZwdosAZzJyYCZwCtBAIBtA==&quot;,\n    &quot;object&quot;:{\n        &quot;analogInput&quot;:{\n            &quot;4&quot;:4.36,\n            &quot;8&quot;:3.59\n        },\n        &quot;barometer&quot;:{\n            &quot;6&quot;:1002.2\n        },\n        &quot;humiditySensor&quot;:{\n            &quot;7&quot;:88\n        },\n        &quot;temperatureSensor&quot;:{\n            &quot;2&quot;:17.3\n        }\n    }\n}\n</code></pre>\n<p>Here is how the records created from the mqtt queue are stored in influxdb :</p>\n<pre><code>Connected to http://localhost:8086 version 1.8.3\nInfluxDB shell version: 1.8.3\n&gt; use iot\nUsing database iot\n&gt; select time, device_id, longitude, latitude, altitude, temperature, pressure, humidity from device_0200000001 order by time desc limit 5\nname: device_0200000001\ntime                device_id  longitude latitude altitude temperature pressure humidity\n----                ---------  --------- -------- -------- ----------- -------- --------\n1603698989487116640 0200000001 1.52989   43.50454 275      11.4        995.5    89\n1603698383735575991 0200000001 1.52981   43.50444 267      11.4        995.5    89\n1603697777987368971 0200000001 1.52983   43.5044  232      11.4        995.3    88.5\n1603697172240129341 0200000001 1.52988   43.50445 235      11.5        995.2    88\n1603696566494739058 0200000001 1.52999   43.50457 243      11.6        995      87\n</code></pre>\n<p>What I would like to get:</p>\n<ul>\n<li>a series containing the data: <code>device_02000001 (time, device_id, temperature, pressure, humidity)</code></li>\n<li>a series containing data and metadata: <code>device_meta_0200000001 (time, device_id, temperature, pressure, humidity, gatewayID, uplinkID, name, gw_time, rssi, loRaSNR, latitude, longitude, altitude)</code></li>\n</ul>\n<p>Can someone tell me how I can process the data in parallel and then store them in two different measurements?</p>\n<p>I've already tried to create two configuration files that would use the same mqtt file, but every time I restart telegraf, it crashes with the error systemd[1]: <code>telegraf.service: Start request repeated too quickly.</code></p>\n<p>Thanks for your help</p>\n<p>Thierry</p>\n"
    }, {
        "tags": ["python", "raspberry-pi", "iot", "influxdb"],
        "is_answered": true,
        "view_count": 76,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1603554399,
        "creation_date": 1603483487,
        "last_edit_date": 1603554399,
        "question_id": 64506636,
        "link": "https://stackoverflow.com/questions/64506636/is-there-any-way-to-make-my-mcp3008-sampling-uniform",
        "title": "Is there any way to make my Mcp3008 sampling uniform?",
        "body": "<p>I'm doing a job to use ADC mcp3008 sample sensor data and collect them until 10k.Then send it to influxdb.</p>\n<p>All these work should be done in 1 second. That's the point.</p>\n<p>Now the problem is, the timestamp of each data are very  uneven.</p>\n<p>As you can see:</p>\n<p><a href=\"https://i.stack.imgur.com/QQxzY.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/QQxzY.png\" alt=\"data in influxdb\" /></a></p>\n<p>I want the timestamp be uniform at 0.1ms. I mean the time precision in influsdb should be 0.1ms.</p>\n<p>But unfortunately, the write parameter time_precision only have  's', 'ms', 'u' or 'n'.</p>\n<p>So all I can do is make the sampling process more uniform, right?</p>\n<p>I use multiprocessing module to do this work. And here is my original Code:</p>\n<pre><code>import Adafruit_GPIO.SPI as SPI # Import Adafruit GPIO_SPI Module\nimport Adafruit_MCP3008         # Import Adafruit_MCP3008\nimport serial\nimport time\nimport datetime\nfrom influxdb import InfluxDBClient\nfrom multiprocessing import Process, Queue\ndef producer(name):\n    i=0\n    while True:\n        begin=time.time()\n        body = []\n        while i&lt;10000:\n            val = round(mcp.read_adc(0),4) #here read the data from SPI port\n            current_time = datetime.datetime.utcnow()\n            js = {\n                &quot;measurement&quot;: &quot;Double&quot;,\n                &quot;time&quot;: current_time,\n                &quot;tags&quot;: {\n                },\n                &quot;fields&quot;: {\n                    &quot;sensor2&quot;: val\n                }\n            }\n            body.append(js)\n            i+=1\n        i=0\n        res = client.write_points(body) #Send influxdb 10k data at once\n        body.clear()\n        end=time.time()-begin\n        print(end,name)\n           \nif __name__ == &quot;__main__&quot;:\n    HW_SPI_PORT = 0 # Set the SPI Port. Raspi has two.\n    HW_SPI_DEV  = 0 # Set the SPI Device\n    mcp = Adafruit_MCP3008.MCP3008(spi=SPI.SpiDev(HW_SPI_PORT, HW_SPI_DEV))\n    client = InfluxDBClient(host='XXXXX', port=8086, username='admin', password='admin', database= 'db',ssl=False, verify_ssl=False)\n    p1 = Process(target=producer,args=(0,))\n    p2 = Process(target=producer,args=(1,))\n    p3 = Process(target=producer,args=(2,))\n    p4 = Process(target=producer,args=(3,))\n    p5 = Process(target=producer,args=(4,))\n    p6 = Process(target=producer,args=(5,))\n    p1.start()\n    p2.start()\n    p3.start()\n    p4.start()\n    p5.start()\n    p6.start()\n</code></pre>\n<p>Yeah...I had to go through six processes to finish in an average of one second..</p>\n<p>So is there any way to make the sampling uniform?\nTo make the timestamp like this:</p>\n<pre><code>1603469938916'5'26000   -0.175\n1603469938916'6'26000   -0.172\n1603469938916'7'26000   -0.178\n1603469938916'8'26000   -0.175\n1603469938916'9'26000   -0.182\n</code></pre>\n<p>I mean with the time precision 0.1ms.</p>\n<p>Thanks! This must be a strange problem.</p>\n<p>PS:\nI got an idea, is there any way to make my timestamp\u2019s precision be 0.1ms? Something like :</p>\n<pre><code>timestamp=datetime.datetime.utcnow()\n...Some operation...\nprint(timestamp)\n</code></pre>\n<p>Then get:  1603469938916900000</p>\n<p>It may work.</p>\n<p>Yeah, I find the solution:</p>\n<pre><code>from datetime import datetime\nimport math\ndef format():\n    dt = datetime.utcnow()\n    dt_round_microsec = math.floor(dt.microsecond/100)*100 \n    dt = dt.replace(microsecond=dt_round_microsec)\n    return dt\n</code></pre>\n<p>Better proposals are welcome</p>\n"
    }, {
        "tags": ["python", "logging", "raspberry-pi", "iot"],
        "is_answered": false,
        "view_count": 30,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1602487619,
        "creation_date": 1602487619,
        "question_id": 64313360,
        "link": "https://stackoverflow.com/questions/64313360/the-way-to-get-fixed-sampling-rate-from-mcp3008",
        "title": "The way to get fixed sampling rate from MCP3008?",
        "body": "<p>I\u2019m using a raspberry pi 4 to collect sensor data by a python script and write it to a log file by using a Mcp3008. The code is like:</p>\n<pre><code>While i&lt;1000:\n  val=mcp.read_adc(0)\n  logging.info('Signal={0:0.3f}'.format(item))\n  i+=1\n</code></pre>\n<p>The loop takes about 0.2s, but not stable.</p>\n<p>Unlike some sensor have the fixed sampling rate. The Mcp3008 will keep reading data.</p>\n<p>So is there a way to make the sampling rate fixed and stable? Like 1k,2k.</p>\n<p>I have four node like this and wanna to let them sample each data synchronously.\nBecause the data should have the timestamp with ns-level precision.</p>\n<p>Thankyou much appreciated!</p>\n"
    }, {
        "tags": ["api", "http", "iot", "grafana", "grafana-datasource"],
        "is_answered": false,
        "view_count": 292,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1601316207,
        "creation_date": 1601301204,
        "last_edit_date": 1601316176,
        "question_id": 64103592,
        "link": "https://stackoverflow.com/questions/64103592/how-to-read-data-from-iot-platform-by-grafana",
        "title": "How to read data from IoT platform by Grafana",
        "body": "<p>my issue is that I'm using Grafana to make a dashboard, and I'm trying to make a connection between an IoT platform and Grafana, I have data stored there, and I want to make Grafana read that data so I can make a separate dashboard.</p>\n<p>Here is a command that I used in the Ubuntu Terminal to read historical data from that IoT platform:</p>\n<pre><code>curl -X GET \\\n  -H &quot;Authorization: Bearer ${JWT}&quot; \\\n  &quot;http://localhost:8000/history/device/25c6b5/history?lastN=3&amp;attr=temperature&quot;\n</code></pre>\n<p>I tried to find how Grafana can read that data, but so far, I only found how to use cURL to read data from Grafana, does Grafana has support to read data from other sources using cURL?</p>\n"
    }, {
        "tags": ["android", "database", "flask", "sqlalchemy", "iot"],
        "is_answered": true,
        "view_count": 96,
        "favorite_count": 0,
        "score": -3,
        "last_activity_date": 1599942217,
        "creation_date": 1599940387,
        "question_id": 63864358,
        "link": "https://stackoverflow.com/questions/63864358/push-data-from-database-to-flask-web-app-and-android-device",
        "title": "Push data from database to flask web app and Android device",
        "body": "<p>How can I show real time database changes in a flask website?\nLike on_update and on_insert, the data will be pushed to the website for the user to see.</p>\n<p>I want to get alert from an IOT device and insert it to the database and the users who are subbed to that device should get the real-time alerts.\nSo I thought</p>\n<pre><code>&gt;IOT detects\n&gt;HTTP POST to database\n&gt;Flask App detects the database change\n&gt;push to clients on web app and android\n</code></pre>\n<p>I made a web app that queries the DB with flask-sqlalchemy but thats it, these are supposed to be real-time alerts! I'm so frustrated it's been a week. I am going nowhere and I feel so lost now.</p>\n<pre><code>&gt;polling\n&gt;web sockets\n&gt;SSE\n&gt;flask sse\n&gt;use AJAX\n&gt;use JQUERY\n</code></pre>\n<p>HHHOOOOOWWWWW?????? Most of the examples are for chat apps, and I see NO method where you listen to database changes and send it to clients ;(</p>\n"
    }, {
        "tags": ["spring-boot", "redis", "microservices", "iot"],
        "is_answered": false,
        "view_count": 187,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1599025988,
        "creation_date": 1599024008,
        "last_edit_date": 1599025988,
        "question_id": 63699141,
        "link": "https://stackoverflow.com/questions/63699141/best-way-to-track-trace-a-json-object-a-time-series-data-as-it-flows-through-a",
        "title": "Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform",
        "body": "<blockquote>\n<p>We are working on an IOT platform, which ingests many device parameter\nvalues (time series) every second from may devices. Once ingested the\neach JSON (batch of multiple parameter values captured at a particular\ninstance) What is the best way to track the JSON as it flows through\nmany microservices down stream in an event driven way?</p>\n<p>We use spring boot technology predominantly and all the services are\ncontainerised.</p>\n<p>Eg: Option 1 - Is associating UUID to each object and then updating\nthe states idempotently in Redis as each microservice processes it\nideal? Problem is each microservice will be tied to Redis now and we\nhave seen performance of Redis going down as number api calls to Redis\nincrease as it is single threaded (We can scale this out though).\nOption 2 - Zipkin?</p>\n<p>Note: We use Kafka/RabbitMQ to process the messages in a distributed\nway as you mentioned here. My question is about a strategy to track\neach of this message and its status (to enable replay if needed to\nattain only once delivery). Let's say a message1 is being by processed\nby Service A, Service B, Service C. Now we are having issues to track\nif the message failed getting processed at Service B or Service C as\nwe get a lot of messages</p>\n</blockquote>\n"
    }, {
        "tags": ["javascript", "python", "raspberry-pi", "iot", "dashboard"],
        "is_answered": false,
        "view_count": 226,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1597851661,
        "creation_date": 1595544141,
        "question_id": 63064160,
        "link": "https://stackoverflow.com/questions/63064160/how-to-graph-data-from-a-raspberry-pi-in-real-time-on-a-webpage",
        "title": "How to graph data from a Raspberry Pi in real-time on a webpage?",
        "body": "<p>I have a script on a raspberry pi that collects ambient light sensor data in lux. It currently sends that data to <a href=\"https://ubidots.com/docs/hw/#send-data\" rel=\"nofollow noreferrer\">Ubidots</a> via an http post request. It graphs this data with lux on the y axis and seconds on the x axis. I'm trying to build a dashboard myself because Ubidots can't be used for commercial purposes. I've currently converted the data in the python file to make a csv and used the Fetch api to graph it. This is good for the time being however I want to be able to see the graph occur in real time. How would I go about sending and graphing this data in <strong>real time</strong>.</p>\n"
    }, {
        "tags": ["django", "postgresql", "amazon-web-services", "raspberry-pi", "iot"],
        "is_answered": true,
        "view_count": 358,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1597168077,
        "creation_date": 1596927596,
        "last_edit_date": 1596929891,
        "question_id": 63320971,
        "link": "https://stackoverflow.com/questions/63320971/django-aws-for-iot-logging-and-rendering-of-data",
        "title": "django + aws for iot logging and rendering of data",
        "body": "<p>I am planning on making a django iot project hosted on aws ec2 instance. The architecture is on the picture. The sensor's data will be read by raspberry pi 3 and will be sent to django web app (django/raspi-data/) on json format hosted on an ec2 instance by creating an api to communicate the raspberry with django. The sent data will be logged on a postgresql hosted on aws rds. For the user to be able to see the data summary, he/she will access another web app (django/client/) hosted on the same ec2 instance as the previous one. The web app will get the data from postgresql db and will be sent to the user's browser in json format. javascript then will handle how to summarize the data. The user can choose if s/he want to render the data in line graph on daily average reading of the data or in monthly basis. Also, the sampling rate of sensor will be 1 every 3 seconds. The Database design will be just date and data only.</p>\n<p><a href=\"https://i.stack.imgur.com/Jvi9o.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Jvi9o.png\" alt=\"Architecture of the IoT system for reading temperature, displaying average in daily or monthly basis\" /></a></p>\n<p>My question is, is this architecture good? or there are any problems here? if there are problems, what could be the possible solution for these?</p>\n"
    }, {
        "tags": ["geolocation", "blockchain", "iot", "ethereum"],
        "is_answered": true,
        "view_count": 110,
        "favorite_count": 1,
        "score": 1,
        "last_activity_date": 1595281584,
        "creation_date": 1595251009,
        "question_id": 62996258,
        "link": "https://stackoverflow.com/questions/62996258/track-and-trace-geolocation-temperature-with-blockchains",
        "title": "Track and trace geolocation &amp; temperature with blockchains",
        "body": "<p>For my internship, I need to implement a blockchain based solution to manage a drug supply chain. The management of this supply chain implies to track-and-trace (geolocate) a drug on the chain, but also to monitor the storage temperature to see if the cold chain is respected. For that I also intend to use IOT, where a device will feed information on the blockchain solution. However, I have a few questions that I can't find easier.</p>\n<p>The first one is that I don't know if I should use ethereum or not, since each time that a new block is added (the block representing the update on the information about the product on &quot;real-time&quot;) I will to use money. Is there any solution for that? Or do I need to create a blockchain with javascript?</p>\n<p>The second question is that I absolutely don't know from where to begin in order to implement IOT on the block chain. I searched on research site, but they only talk about it, without presenting any example...</p>\n<p>The third one is more a confirmation than a question since I want to know if my idea to use an IOT to track and manage products on a supply chain can be done on a wide scale, since the bigger a blockchain the slower is the time to add a block because of the consensus mechanism. So it means that my &quot;real time&quot; tracking on truly be &quot;on time&quot; since there would be a waiting time before the block is added to the blockchain. If the time is just a few seconds to minutes, then there is no problem, but because the number of block will rapidly increase because of the real time tracking (1 block each minute for each storage or transport vehicles I was planning for) that this problem of scalability makes it impractible.</p>\n<p>I'm thanking anybody in advance who will help me solve these questions.</p>\n"
    }, {
        "tags": ["javascript", "node.js", "iot", "autodesk-forge"],
        "is_answered": false,
        "view_count": 76,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1593352791,
        "creation_date": 1593349499,
        "question_id": 62622929,
        "link": "https://stackoverflow.com/questions/62622929/how-i-can-add-a-heat-map-or-fader-extension-to-my-forge-viewer",
        "title": "How I can add a heat map or Fader extension to my forge viewer?",
        "body": "<p>Actually, I'm devlopping an application of BIM-Sensors integration.\nI get the values of sensors at each timestamp as markups in viewer but I want to represent these values in some surfaces of model as gradient at each timestamp.\nAs shown in the picture below, I want to get something like that:\nI would like to ask you for help</p>\n<p><a href=\"https://i.stack.imgur.com/Yl3oi.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Yl3oi.png\" alt=\"enter image description here\" /></a></p>\n"
    }, {
        "tags": ["geolocation", "geospatial", "iot"],
        "is_answered": false,
        "view_count": 166,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1593298655,
        "creation_date": 1593298247,
        "last_edit_date": 1593298655,
        "question_id": 62616363,
        "link": "https://stackoverflow.com/questions/62616363/point-in-polygon-based-search-vs-geo-hash-based-search",
        "title": "Point in polygon based search vs geo hash based search",
        "body": "<p>I'm looking for some advice.</p>\n<p>I'm developing a system with geographic triggers, these enable my device to perform certain actions depending on where it is. The triggers are contained within polygons that are stored in my database I've explored multiple options to get this working, however, I'm not very familiar with geo-spacial systems.</p>\n<p>An option would be to use the current location of the device and query the DB directly to give me all the polygons that contain that point, thus, all the triggers since they are linked together. A potential problem with this approach, I think, would be the possible amount of polygons stored, and the frequency of the queries, since this system serves multiple devices simultaneously and each one of them polls every few seconds.</p>\n<p>An other option I'm exploring is to encode the polygons to an array of geo-hashes and then attach the trigger to each one of them.</p>\n<p>Green is the geohashes that the trigger will be attached to, yellow are areas that need to be recalculated with a higher precision. The idea is to encode the polygon in the most efficient way down to X precision.</p>\n<p><a href=\"https://i.stack.imgur.com/KXbzk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/KXbzk.png\" alt=\"1\" /></a></p>\n<p>An other optimization I came up with is to only store the intersection of polygons with roads since these devices are only use in motor vehicles.</p>\n<p><a href=\"https://i.stack.imgur.com/SWS3V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/SWS3V.png\" alt=\"2\" /></a></p>\n<p>Doing this enable the device to work offline performing it's own encoding and lookup, with a potential disadvantage being that the device will have to implement logic to stay up-to-date with triggers added or removed ( potentially every 24 hours )</p>\n<p><a href=\"https://i.stack.imgur.com/Qc6Wv.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Qc6Wv.png\" alt=\"3\" /></a></p>\n<p>I'm looking for the most efficient way to implement this given some constrains such as:</p>\n<ul>\n<li>Potentially unreliable networks ( the device has LTE connectivity )</li>\n<li>Limited processing power, the devices for now are based on a raspberry pi 3 Compute module, however, they perform other tasks such as image processing.</li>\n<li>Limited storage, since they store videos and images.</li>\n<li>Potential large amount of triggers/polygons</li>\n<li>Potential large amount of devices.</li>\n</ul>\n<p>Any thoughts are greatly appreciated.</p>\n"
    }, {
        "tags": ["database", "nosql", "iot", "geocoding"],
        "is_answered": false,
        "view_count": 79,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1592309185,
        "creation_date": 1592234184,
        "last_edit_date": 1592234640,
        "question_id": 62391409,
        "link": "https://stackoverflow.com/questions/62391409/database-for-iot-device-data",
        "title": "database for IOT device data",
        "body": "<p>I am new to IOT world.There is a requirement to store geo co-ordinates(Longitude and Latitude) received from iot devices.</p>\n\n<ol>\n<li>each device will emit messages in every 10 seconds.</li>\n<li>Device count will be around 1000 in the beginning.</li>\n<li>Data will be used to show history and produce analytics.</li>\n</ol>\n\n<p>I am new to IOT as well as no sql world. </p>\n\n<p>Could any one please suggest  what database should be store data? </p>\n\n<p>need to retrieve data based on time and device and several other cases</p>\n\n<p>Thanks in advance</p>\n"
    }, {
        "tags": ["performance", "logging", "redis", "iot"],
        "is_answered": true,
        "view_count": 142,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1591458018,
        "creation_date": 1591432851,
        "last_edit_date": 1591456496,
        "question_id": 62229326,
        "link": "https://stackoverflow.com/questions/62229326/validate-and-processing-data-in-redis-sorted-set-efficiently",
        "title": "validate and processing data in Redis sorted set efficiently",
        "body": "<p>we have micro service (written in <code>Go lang</code>) which it's primary purpose is to get the logs from multiple IoT devices and do some processing on them and put the result into a PostgreSQL table. The way the system works is that each device has its own sorted set which the logs will be saved there and for each log the score would be a timestamp ( of course I know time series would be a better decision but we currently want to worked with sorted sets). know this logs come every 1 second from each device.\n<br> I want to process the data inside these sets every 5 second, but for each set, the logs inside should pass some tests:</p>\n\n<ol>\n<li>there should be more than one log inside the set</li>\n<li>two logs can be removed from the set, if the time difference between timestamps is 1 second</li>\n</ol>\n\n<p>when the logs are validated then they can be passed to other methods or functions to the the rest of the processing. If logs are invalid ( there exists a log that has time difference of more than 1 second with other logs) then it go's back to the set and wait for the next iteration to be checked again.</p>\n\n<p><strong>Problem:</strong>\n<br> My problem is basically that I don't know how to get the data out of the list, validate them and put them back again! to be more clear for each set,all or none of  the logs inside can be removed, and this occurs while new data is coming in contently, and since I cant validate the data with redis it self I don't know what to do. My current solution is as follows:\n<br> Every 5 seconds, all data from each set should be removed from Redis and saved in some data structure inside the code ( like a list...) the after validating, some logs that are not yet validated should be putted back to Redis. as you can see these solution needs two database access from the code,  and when putting the invalid logs, they should be sorted by Redis ...\nwhen the logs are so much and there are many devices, I think this solution is not the best way to go. I'm not very experienced with Redis so would be thankful to give your comments on the problem. Thanks</p>\n"
    }, {
        "tags": ["redis", "iot"],
        "is_answered": false,
        "view_count": 35,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1591215123,
        "creation_date": 1591215123,
        "question_id": 62181923,
        "link": "https://stackoverflow.com/questions/62181923/how-to-mange-and-save-iot-device-logs-in-redis-to-efficiently-use-in-future",
        "title": "How to mange and save IoT Device logs in Redis to efficiently use in future",
        "body": "<p>I work for a company that specializes in car IoT devices. These devices are installed in cars and when the car start to work, each second they send a log containing information about the car. We use MQTT as a broker for our logs... \n<br> I'm working on a micro-service which picks the logs from the MQTT server and then should do the following: For every device it should create a row in a table in our database ( if not yet created) which contains how much this car(device) has worked. And by how much I mean How much time has it been on and how mush distance it has traveled. </p>\n\n<p><br> Now up to here everything seems simple but the problem is that there is the possibility that a device losses its connection to the internet and doesn't send the logs immediately. How ever the device saves does logs in its memory and whenever it comes back online, it sends the older data in between the newer once, so if for instance it goes of on 11 am and comes back online on 1 pm then the patterns is like this:</p>\n\n<ul>\n<li>11 am log (old log)</li>\n<li>1 am log ( current log)</li>\n<li>11 am log (old log)</li>\n<li>1 am log ( current log)</li>\n<li>11 am log (old log)</li>\n<li>1 am log ( current log)\n...</li>\n</ul>\n\n<p>it is possible that the logs of one day be sent on another day and so on...</p>\n\n<p><br> since there are so many devices and so many logs each second, we couldn't afford to access the database each time a log is received, so we have a <code>redis</code> to store are data faster and every few seconds we update the db from the <code>redis</code>. It is obvious that each received log represents of <code>1 second</code> of work, but My real problem is to calculate the distance each car traveled! For that we need two consecutive logs from the same car so that we can find the <code>distance delta</code>. Considering that it is very possible that the logs of a car are mixed, I'm facing some real challenges to handle the data.\n<br> currently I'm saving a key-value pair in Redis which the key looks like this : <code>deviceid:-:date</code> and the value contains the <code>total distance/time</code> plus some other info. Actually I have a key-value pair stored for every online device that we are receiving form and the <code>date</code> is exactly the <code>date of sending</code> of that log. As you can see this method doesn't help\nto find the distance traveled and since I cant store all the logs I really don't know what to do...</p>\n\n<p><br> one way I thought is to keep a <code>sorted-set</code> for every online car on any date, and in that set, I use the timestamp of the logs as the <code>score</code> so that logs are sorted in a logical sequential form. How every I don't know where to go from here and how to efficiently calculate the data I need. \n<br> If it helps, we are using <code>golang</code> as our primary language. sorry about my bad English. Any idea or help would be very appreciated. Thanks   </p>\n"
    }, {
        "tags": ["python", "tensorflow", "server", "client-server", "iot"],
        "is_answered": false,
        "view_count": 53,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1591186774,
        "creation_date": 1591186225,
        "last_edit_date": 1591186774,
        "question_id": 62172513,
        "link": "https://stackoverflow.com/questions/62172513/how-to-implement-client-server-interactions-in-python",
        "title": "How to implement client-server interactions in python",
        "body": "<p>I have data on multiple files. I want to simulate federated learning, which is basically :\nhaving the datasets on edge devices</p>\n\n<pre><code>initialisation of the server\ninitialisation of all client models with the same random weights \n\nfor each round in range(R):\n\n   for each client in client_list:\n\n      client does :\n      uploading weights from a server\n      training neural networks locally with those weights\n      returning the new weights to the server.\n\n   server update its weights by doing a mean of the weights returned by the clients.\n</code></pre>\n\n<p>I have a file client.py with a class with all the methods the client needs : doing rounds of training and returning the new weights, evaluating the model over a train set, updating the learning rate.</p>\n\n<p>I have a file server.py with all the methods needed for the server : updating the weights</p>\n\n<p>I want to simulate this with first of all two computers, but I don't know anything about client-server architecture </p>\n\n<p>Thank you by advance for all the insights you will give me, and have a nice day :)</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-iot-edge"],
        "is_answered": false,
        "view_count": 157,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1591036975,
        "creation_date": 1590126729,
        "question_id": 61948670,
        "link": "https://stackoverflow.com/questions/61948670/azure-iot-edge-batching-behaviour",
        "title": "Azure IoT Edge Batching behaviour",
        "body": "<p>I am noticing the following behavior using IoT Edge batching: I know that my individual message size is less than 200 bytes. I can batch these comfortably 1000 at a time and send them to IoT Hub when I am not using IoT Edge. When using IoT Edge, the maximum batch that is being sent to IoT Hub is 100 messages. I can set the environment variable \"MaxUpstreamBatchSize\" on EdgeHub to a value below 100 and the batch size would change accordingly, but anything above 100 is ignored and the batch size defaults to 100. When I don't enter a value for the \"MaxUpstreamBatchSize\" I get batch size of 10. So is 100 the maximum batch size that we can get from IoT Edge?\nThanks</p>\n"
    }, {
        "tags": ["machine-learning", "iot", "anomaly-detection", "kapacitor"],
        "is_answered": false,
        "view_count": 37,
        "favorite_count": 0,
        "score": -2,
        "last_activity_date": 1590912267,
        "creation_date": 1590851329,
        "last_edit_date": 1590852853,
        "question_id": 62104382,
        "link": "https://stackoverflow.com/questions/62104382/how-to-resolve-conflict-between-users-decision-and-iot-system",
        "title": "How to resolve conflict between users decision and IoT system?",
        "body": "<p>In an Internet project, we have to use machine learning to resolve the conflict between the user's decisions and the system. In this project we use TICKstack (Telegraf, Influxdb, Chronograf, Kapacitor). To better understand the subject, consider the following two scenarios. \nIs there a plugin in Kapacitor to do this?</p>\n\n<p><a href=\"https://i.stack.imgur.com/YFeTG.png\" rel=\"nofollow noreferrer\">Conflict scenarios</a></p>\n"
    }, {
        "tags": ["apache-kafka", "iot", "ksqldb"],
        "is_answered": true,
        "view_count": 433,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1589386409,
        "creation_date": 1522419043,
        "last_edit_date": 1589386409,
        "question_id": 49576001,
        "link": "https://stackoverflow.com/questions/49576001/iot-ksql-tumbling-issues",
        "title": "IoT - KSQL Tumbling issues",
        "body": "<p>Our JavaScript program writing 20 messages asynchronously using kafka-rest every seconds.\n We try to do aggregation on incoming message but it return some inconsistent result.</p>\n\n<p>Please find topic, stream and aggregated result table definition below.</p>\n\n<p>Topic:</p>\n\n<pre><code>./bin/kafka-avro-console-producer \u2013broker-list localhost:9092 \u2013topic order_flow \u2013property value.schema='{\u201ctype\u201d:\u201drecord\u201d,\u201dname\u201d:\u201dmyrecord\u201d,\u201dfields\u201d:[{\u201cname\u201d:\u201dOrderID\u201d,\u201dtype\u201d:\u201dint\u201d},{\u201cname\u201d:\u201dOrderDate\u201d,\u201dtype\u201d:\u201dlong\u201d},{\u201cname\u201d:\u201dStatus\u201d,\u201dtype\u201d:\u201dstring\u201d},{\u201cname\u201d:\u201dProductID\u201d,\u201dtype\u201d:\u201dint\u201d}]}\u2019\n</code></pre>\n\n<p>Stream:</p>\n\n<pre><code>CREATE STREAM ORDERS_SRC WITH (KAFKA_TOPIC=\u2019order_flow\u2019, VALUE_FORMAT=\u2019AVRO\u2019);\n</code></pre>\n\n<p>NEW STREAM \u2013 this stream use the actual event date rather than time when message wrote in kafka.</p>\n\n<pre><code>CREATE STREAM ORDERS WITH (TIMESTAMP =\u2019ORDERDATE\u2019) AS SELECT ORDERDATE,ORDERID, STATUS, PRODUCTID FROM ORDERS_SRC;\n</code></pre>\n\n<p>Now we are aggregating data based on it status using:</p>\n\n<pre><code>CREATE TABLE ORDERS_AGG_SEC as select Status,Count(*) from ORDERS_D WINDOW TUMBLING(SIZE 1 SECONDS) GROUP BY STATUS;\n</code></pre>\n\n<p>Now when we run query SELECT * FROM ORDERS_AGG_SEC; it returning below result</p>\n\n<pre><code>1522328177000 | Processing : Window{start=1522328177000 end=-} | Processing | 20\n1522328178000 | Processing : Window{start=1522328178000 end=-} | Processing | 20\n1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 5\n1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 20\n1522328180000 | Processing : Window{start=1522328180000 end=-} | Processing | 20\n1522328181000 | Processing : Window{start=1522328181000 end=-} | Processing | 15\n1522328181000 | Processing : Window{start=1522328181000 end=-} | Processing | 20\n1522328182000 | Processing : Window{start=1522328182000 end=-} | Processing | 20\n1522328183000 | Processing : Window{start=1522328183000 end=-} | Processing | 15\n1522328183000 | Processing : Window{start=1522328183000 end=-} | Processing | 20\n1522328184000 | Processing : Window{start=1522328184000 end=-} | Processing | 20\n1522328185000 | Processing : Window{start=1522328185000 end=-} | Processing | 15\n1522328185000 | Processing : Window{start=1522328185000 end=-} | Processing | 20\n1522328186000 | Processing : Window{start=1522328186000 end=-} | Processing | 20\n1522328187000 | Processing : Window{start=1522328187000 end=-} | Processing | 15\n1522328187000 | Processing : Window{start=1522328187000 end=-} | Processing | 20\n1522328188000 | Processing : Window{start=1522328188000 end=-} | Processing | 20\n1522328189000 | Processing : Window{start=1522328189000 end=-} | Processing | 15\n1522328189000 | Processing : Window{start=1522328189000 end=-} | Processing | 20\n1522328190000 | Processing : Window{start=1522328190000 end=-} | Processing | 20\n1522328191000 | Processing : Window{start=1522328191000 end=-} | Processing | 15\n</code></pre>\n\n<p>Expected Result:\nI should get 20 count at every 1 second for Processing status</p>\n\n<p>Actual Result:\nI am getting more than one records for every 1 second interval for same status like below:</p>\n\n<pre><code>1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 5\n1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 20\n</code></pre>\n\n<p>Please find my javascript code below:</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>function getRandomInt(min, max) {\n    min = Math.ceil(min);\n    max = Math.floor(max);\n    return Math.floor(Math.random() * (max \u2013 min)) + min; //The maximum is exclusive and the minimum is inclusive\n}\nvar orderdate = Date.now();\nfor (var i = 0; i &lt; 20; i++) {\n    var data = {\n        \"OrderID\": getRandomInt(1, 20000),\n        \"OrderDate\": orderdate,\n        \"Status\": \"Processing\",\n        \"ProductID\": getRandomInt(1, 10)\n    }\n    node.send({payload:data}); // this function asynchronously call kafka-rest api producer.\n}\n</code></pre>\n\n<p>Note: kafka rest api running with default properties</p>\n"
    }, {
        "tags": ["iot", "webhooks", "ifttt"],
        "is_answered": false,
        "view_count": 1237,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1588817494,
        "creation_date": 1544627474,
        "last_edit_date": 1544627875,
        "question_id": 53745999,
        "link": "https://stackoverflow.com/questions/53745999/ifttt-webhooks-limit",
        "title": "IFTTT Webhooks limit",
        "body": "<p><strong>Info</strong> <br/>I am using IFTTT to send temperature data from my Nordic Thingy 52 to google spreadsheets using a recipe I wrote.<br/>\nIt works great and 13 rows have been written to my sheet.<br/></p>\n\n<p><strong>Error</strong> <br>\nThe temperature measurement interval was every 2 seconds which was too high for IFTTT as I got the following error:</p>\n\n<blockquote>\n  <p>Error<br/> Usage limit exceeded Dec 12 - 4:41 PM  Webhooks<br/> If\n  Maker Event \"temperature_update\", then Add row to Gal Margalit\u2019s\n  Google Drive spreadsheet<br/> Tap to view our Troubleshooting Services\n  page to learn about usage limits Hide details</p>\n</blockquote>\n\n<p>I've gone through every documentation, troubleshooting and faqs they have on the website but I find no numbers on data limit or intervals.<br/>\nI've increased the measurement interval to be taken every 60 seconds.<br/>\n<br/>\n<strong>Does anyone know what is the data limit on IFTTT?</strong></p>\n"
    }, {
        "tags": ["amazon-ec2", "hyperledger-fabric", "hyperledger", "iot", "scalability"],
        "is_answered": false,
        "view_count": 26,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1588439216,
        "creation_date": 1588192513,
        "question_id": 61511191,
        "link": "https://stackoverflow.com/questions/61511191/application-architecture-for-scalable-hyperledger-v1-4-with-iot-data",
        "title": "Application Architecture for scalable hyperledger v1.4 with IOT data",
        "body": "<p>I am working on the Hyperledger Application that can store sensor data from <em>IoT</em>.\nUsing HLF v1.4 with <strong>Raft</strong>. Each IoT device will provide JSON data at fixed intervals which gets stored in Hyperledger. I have worked with HLF v1.3 which doesn't scale very well.</p>\n\n<p>With v1.4, I am planning to start with 2 organization setup with 5 peers for each organization.</p>\n\n<blockquote>\n  <p>But the limiting factor seems to be, as the number of blocks increase by adding new transactions and querying the network takes a longer time.</p>\n</blockquote>\n\n<ul>\n<li><strong>What are the steps that can be taken to scale the HLF with v1.4 or onwards.</strong></li>\n<li><strong>What type of Server specs should be used for good performance, like RAM, CPUs when selecting a server e.g EC2</strong></li>\n</ul>\n"
    }, {
        "tags": ["node.js", "database", "iot", "autodesk-forge", "timescaledb"],
        "is_answered": false,
        "view_count": 123,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1588365453,
        "creation_date": 1588190070,
        "last_edit_date": 1588365453,
        "question_id": 61510526,
        "link": "https://stackoverflow.com/questions/61510526/link-timescale-database-to-forge-in-real-time",
        "title": "link Timescale Database to Forge in real time",
        "body": "<p>I'm working with timescale database which is queried by Postgresql.</p>\n\n<p>I'm looking for how I can stream in real time the data from my database to forge and establish the link between the database and forge using nodeJS. I need really your help.</p>\n\n<p>Sincerely</p>\n"
    }, {
        "tags": ["mqtt", "iot", "mosquitto"],
        "is_answered": true,
        "view_count": 172,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1587566517,
        "creation_date": 1587565631,
        "last_edit_date": 1587566517,
        "question_id": 61367621,
        "link": "https://stackoverflow.com/questions/61367621/how-to-implement-a-request-in-mqtt",
        "title": "How to implement a request in MQTT?",
        "body": "<p>There is an IoT server that only supports MQTT protocol and the administrator shared below code in order to get data from the server</p>\n\n<pre><code>mosquitto_sub -v -t \"app/26\" -h broker.example.com -p 1883  -u \"dir1-mqtt\" -P \"5sp-1001-tech\n</code></pre>\n\n<p>Actually i don't know how to deal with it.</p>\n\n<p>How can i send that request to the target and get the response in a fastest way?</p>\n"
    }, {
        "tags": ["redis", "iot", "stackexchange.redis", "buffering", "disk-io"],
        "is_answered": true,
        "view_count": 146,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1586508874,
        "creation_date": 1586463053,
        "last_edit_date": 1586508874,
        "question_id": 61129442,
        "link": "https://stackoverflow.com/questions/61129442/what-is-the-mechanism-of-snapshotting-in-redis",
        "title": "What is the mechanism of snapshotting in Redis?",
        "body": "<p>We can define in Redis configuration file about snapshotting to happen after certain interval of time. I want to clear out that whether that snapshotting process in differential or it creates complete new dump of the Redis db that resides in RAM and deletes the older one.</p>\n\n<p>Also, if there is no differential snapshotting mechanism in Redis, then it means that if I am taking snapshot of Redis db at a interval of 5 minutes, then my disk I/O will not reduce and will be constant depending upon the size of db even if I had only changed one key. Correct?</p>\n"
    }, {
        "tags": ["http", "arduino", "iot", "gsm"],
        "is_answered": false,
        "view_count": 274,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1586367769,
        "creation_date": 1586342690,
        "question_id": 61098843,
        "link": "https://stackoverflow.com/questions/61098843/how-do-i-store-data-from-httpread-into-a-variable",
        "title": "How do i store data from HTTPREAD into a variable?",
        "body": "<p>I need a way to store HTTPREAD data into a variable because I will be comparing its value to another variable. Is there any way? </p>\n\n<pre><code>  {\n  myGsm.print(\"AT+HTTPPARA=\\\"URL\\\",\\\"http://7ae0eae2.ngrok.io/get-ignition/ccb37bd2-a59e-4e56-a7e1-68fd0d7cf845\"); // Send PARA command\n  myGsm.print(\"\\\"\\r\\n\"); \n  delay(1000);\n  printSerialData();\n\n  myGsm.println();\n  myGsm.println(\"AT+HTTPACTION=0\");//submit the GET request \n  delay(8000);//the delay is important if the return datas are very large, the time required longer.\n  printSerialData();\n  myGsm.println(\"AT+HTTPREAD=0,17\");// read the data from the website you access\n  delay(3000);\n  printSerialData();\n  delay(1000);\n}\n\nvoid printSerialData()\n{\n while(myGsm.available()!=0)\n Serial.write(myGsm.read());\n}\n</code></pre>\n"
    }, {
        "tags": ["python", "django", "database", "iot"],
        "is_answered": false,
        "view_count": 113,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1586115226,
        "creation_date": 1586115226,
        "question_id": 61048339,
        "link": "https://stackoverflow.com/questions/61048339/how-to-deal-with-large-amounts-of-data-from-django-based-iot-projects",
        "title": "How to deal with large amounts of data from django based IOT projects",
        "body": "<p>I'm working on a django IOT related dashboard style project. I have a question related to the scale of the project. Let's say that there are 30 or more raspberry pi's or ardunios and you want to get the location and some other data off of them and store it in a database. How would one go about designing a database for a system at such a large scale.</p>\n"
    }, {
        "tags": ["iot", "azure-iot-sdk", "iotconnect"],
        "is_answered": true,
        "view_count": 263,
        "favorite_count": 3,
        "score": 3,
        "last_activity_date": 1585641189,
        "creation_date": 1542006281,
        "last_edit_date": 1585641189,
        "question_id": 53257299,
        "link": "https://stackoverflow.com/questions/53257299/mqtt-communication-for-devices-which-one-is-the-better-option-for-pubsub-data-e",
        "title": "MQTT communication for devices, which one is the better option for PubSub data either RabbitMQ or Azure ServiceBus for IotConnect?",
        "body": "<p>MQTT communication for devices, which one is the better option for PubSub data either RabbitMQ or Azure ServiceBus for IotConnect?</p>\n\n<p>We have use cases for the smart device that can continue be sending data on the cloud, which option is best and cost-effective that generate large amount data with many devices without any interruption.</p>\n\n<p>We want to go with Azure ServiceBus, I want to know is the best option over the RabbitMQ for IotConnect? </p>\n"
    }, {
        "tags": ["sql", "database", "nosql", "iot"],
        "is_answered": true,
        "view_count": 29,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1584959209,
        "creation_date": 1584679093,
        "question_id": 60768640,
        "link": "https://stackoverflow.com/questions/60768640/best-technology-for-building-race-simulation-application",
        "title": "Best technology for building race simulation application",
        "body": "<p>I am trying to do something new, something I have never done before. I am looking for advice or point me into right direction how to choose technology. I am trying to build race simulation app that will have thousands of iot devices streaming data into central platform. While I understand that I can use some sort of IOT hub with cloud providers, but what technology do I choose for storing data?</p>\n\n<p>Example is online indoor biking app. There are apps where you can connect your indoor bike online and have simulated race. For my project I am trying to build something similar. Do I use NO SQL db in this scenario? What technology will allow better scale of application like this since it could be millions of devices around the world in \"simulated\" race. I am not worried about front-end and things like that, but backend, IOT hub, storing data, presenting-real time?</p>\n"
    }, {
        "tags": ["python", "opencv", "image-processing", "raspberry-pi", "iot"],
        "is_answered": false,
        "view_count": 59,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1583508816,
        "creation_date": 1583508816,
        "question_id": 60567214,
        "link": "https://stackoverflow.com/questions/60567214/parking-detection-on-raspberry-pi-and-decision-making",
        "title": "Parking Detection on Raspberry Pi and decision making?",
        "body": "<p><b>Dears,</b><p>\nIt's an honor to be on this programming-heaven of a platform! Thank you so much StackOverflowers for what you've done to us college students and programming noobies :D, anyway:</p>\n\n<p>I'm taking an IoT course, and currently working on the final capstone. My idea that I chose was a camera-based parking system that detects availability using image-recognition. Tools provided to us were: Raspberry Pi 3, and a raspberry pi camera module.<p> So the camera should take a picture every 5 seconds and save it in a specific folder, a python code should process the image and give a boolean result for each parking spot and send it to MySQL database. Finally, this boolean results will be retrieved by a web-page and upon this info, the results should be shown on a virtual parking map for each parking spot.</p>\n\n<p>What we reached right now is:<br>\n1- A python script that allows the camera to take a picture and save it in a specific folder.<br>\n2- A webpage that call on the MySQL database and get results for 20 parking lots (Parking lot number and status) and show them (in a very elegant way) on a parking scheme.</p>\n\n<p>Our issue with the image processing part (which is basically 90% of the project :D). I've looked up github codes that allow me \"using openCV\" to process a specific photo and show images on X server, but I don't need that! I need to grap the info shown on the X server (whether the parking is green or red) and send this info for each parking lot to the MySQL database. <p>\nSo my question is: are you guys familiar with any that somehow close enough to this idea, so I can start with it and edit the script to satisfy my need and specific photo that I captured using my RPi Camera? Please help me out, because i'm starting to panic tbh ':D.</p>\n"
    }, {
        "tags": ["javascript", "node.js", "mongodb", "mongoose", "iot"],
        "is_answered": false,
        "view_count": 140,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1583153280,
        "creation_date": 1583150840,
        "last_edit_date": 1583152445,
        "question_id": 60488515,
        "link": "https://stackoverflow.com/questions/60488515/time-interval-series-database-handling-in-node-js-and-mongodb",
        "title": "Time-interval series database handling in node-js and Mongodb",
        "body": "<p>Actually i'm creating a iot-temperature sensor database with has a event series database contain  start_timestamp and end_timestamp, for every temperature change event i inserting a new document in database,\nMy Schema is</p>\n\n<pre><code>var tempSchema = mongoose.Schema({\n    sensor_name:{\n        type:String\n    },\n    temp_value:{\n        type:Number\n    },\n    start_timestamp:{\n        type:Date,\n    },\n    end_timestamp:{\n        type:Date,\n        default:null\n    }\n});\n</code></pre>\n\n<p>for every new document insert i have to first update end_timestamp of last event and then insert new document with same timestamp which i updated last document end_timestamp.</p>\n\n<p>Problems i'm facing</p>\n\n<p>1) i have 30 sensors and every 2 sec i'm getting data from sensor and first i'm comparing current value with last value and if this value change i'm firing a event and update last value which has end_timestamp null,because of event loop is very busy and async nature of node sometime 2 entry of same event inserting on database the only difference start_timestamp is different (like 2020-02-22T15:52:21.639+00:00 and 2020-02-22T15:52:21.710+00:00) but ideally should be only one end_timestamp null in database</p>\n\n<p>2) 2 operations is happening on same time, right now i'm using 2 function for separate opration like  update_entry for end_timestamp update and post_entry for new document,is there is any method in mongodb i used to perform this operation together. </p>\n\n<p>Give me your kind help \nThanks in advance</p>\n"
    }, {
        "tags": ["mqtt", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 4046,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1582763257,
        "creation_date": 1565273838,
        "question_id": 57414725,
        "link": "https://stackoverflow.com/questions/57414725/what-is-the-maximum-message-payload-size-for-a-aws-iot-core-broker",
        "title": "What is the maximum message payload size for a AWS-IoT core broker?",
        "body": "<p>In the documentation <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html\" rel=\"nofollow noreferrer\">AWS Service Limits</a> the payload size limit is not clear for the IoT-Core service. Since we can <a href=\"https://aws.amazon.com/pt/blogs/iot/how-to-bridge-mosquitto-mqtt-broker-to-aws-iot/\" rel=\"nofollow noreferrer\">bridge local mosquitto to AWS IoT</a>, I suppose that this last one should accept the max size of payload from mosquitto <a href=\"http://www.steves-internet-guide.com/mqtt-protocol-messages-overview/\" rel=\"nofollow noreferrer\">witch is 256Mb</a>. So what is the max size (in terms of payload) that I can send to a broker in AWS-IotCore?</p>\n\n<p>The reason for that question is that I dont want to create to many documents in my mongoDb. For that I am going to send hourly data and the document or the message payload will be much higher than sends for seconds. </p>\n"
    }, {
        "tags": ["mongodb", "amazon-web-services", "iot", "amazon-kinesis"],
        "is_answered": true,
        "view_count": 476,
        "favorite_count": 0,
        "score": 4,
        "last_activity_date": 1580510847,
        "creation_date": 1579704224,
        "last_edit_date": 1579756171,
        "question_id": 59862218,
        "link": "https://stackoverflow.com/questions/59862218/aws-how-to-save-streaming-data-to-database-hosted-on-ec2-ex-mysql-mongodb",
        "title": "AWS: How to save Streaming data to database hosted on EC2 ( ex. MySQL/ MongoDB )",
        "body": "<p>We can easily save data between different AWS Services for ex. Kinesis to DynamoDB; or AWS IoT to Redshift etc. </p>\n\n<p>But what is best strategy to save streaming data to suppose MongoDB ( which does NOT have AWS PaaS ; Atlas is there but it has no integrations with other AWS Services )</p>\n\n<p>I can see some third party solutions are there; but what is best strategy to implement on AWS itself...Is execution of lambda function for each insert (batching) the only option ?</p>\n"
    }, {
        "tags": ["iot", "influxdb", "kapacitor"],
        "is_answered": false,
        "view_count": 327,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1580306779,
        "creation_date": 1580275011,
        "question_id": 59960850,
        "link": "https://stackoverflow.com/questions/59960850/can-influxdb-have-continuous-queries-with-same-source-target-measurements-but",
        "title": "Can InfluxDB have Continuous Queries with same source &amp; target measurements but with different/new tags?",
        "body": "<p>Below is the scenario against which I have this question.</p>\n\n<p><strong>Requirement:</strong>\nPre-aggregate time series data within influxDb with granularity of seconds, minutes, hours, days &amp; weeks for each sensor in a device.</p>\n\n<p><strong>Current Proposal:</strong>\nCreate five Continuous Queries (one for each granularity level i.e. Seconds, minutes ...) for each sensor of a device in a different retention policy as that of the raw time series data, when the device is onboarded. </p>\n\n<p><strong>Limitation with Current Proposal:</strong>\nWith increased number of device/sensor (time series data source), the influx will get bloated with too many Continuous Queries (which is not recommended) and will take a toll on the influxDb instance itself.</p>\n\n<p><strong>Question:</strong>\nTo avoid the above problems, is there a possibility to create Continuous Queries on the same source measurement (i.e. raw timeseries measurement) but the aggregates can be differentiated within the measurement using new tags introduced to differentiate the results from Continuous Queries from that of the raw time series data in the measurement.</p>\n\n<p><strong>Example:</strong></p>\n\n<pre><code>CREATE CONTINUOUS QUERY \"strain_seconds\" ON \"database\"\nRESAMPLE EVERY 5s FOR 1m\nBEGIN\n  SELECT MEAN(\"strain_top\") AS \"STRAIN_TOP_MEAN\" INTO \"database\".\"raw\".\"strain\" FROM \"database\".\"raw\".\"strain\" GROUP BY time(1s),*\nEND\n</code></pre>\n"
    }, {
        "tags": ["raspberry-pi", "iot", "onnx"],
        "is_answered": true,
        "view_count": 2485,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1579048546,
        "creation_date": 1578913964,
        "question_id": 59715507,
        "link": "https://stackoverflow.com/questions/59715507/how-to-load-or-infer-onnx-models-in-edge-devices-like-raspberry-pi",
        "title": "How to load or infer onnx models in edge devices like raspberry pi?",
        "body": "<p>I just want to load onnx models in raspberry pi. How to load onnx models in edge devices?</p>\n"
    }, {
        "tags": ["amazon-web-services", "aws-lambda", "cloud", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 209,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1576900840,
        "creation_date": 1571334082,
        "question_id": 58438065,
        "link": "https://stackoverflow.com/questions/58438065/different-between-transaction-vs-request-in-aws-iot-limit",
        "title": "Different between transaction vs request in AWS IOT limit",
        "body": "<p>In most of the official documents to express throttling limits, AWS uses metrics like <strong>Requests per second</strong> or <strong>Requests per client</strong>. e.g. <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\" rel=\"nofollow noreferrer\">here</a>. But for <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_iot\" rel=\"nofollow noreferrer\">AWS IOT API throttling limit</a>, there are using a metric called <strong>Transactions per seconds</strong>. Is there an actual difference between \"Transactions per Second\" and \"Requests per second\" metrics or they are just the same?</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-iot-central"],
        "is_answered": true,
        "view_count": 146,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1576818023,
        "creation_date": 1552059704,
        "last_edit_date": 1576818023,
        "question_id": 55066519,
        "link": "https://stackoverflow.com/questions/55066519/see-real-telemetry-values-second-to-second-on-iot-central",
        "title": "See real telemetry values second to second on iot central",
        "body": "<p>I have used iot central of microsoft azure for two days but I haven't managed to graph the data that arrives second to second without an aggregate function (Average, minimum, maximum, sum, count), when I plot the data in the graphic, It is always shown as a aggregate function for example: The device send data every second but in the graphic one point can be from 09:39:03 to 09:39:16 and in this case it shows me the value of the average in this range of time </p>\n\n<p><a href=\"https://i.stack.imgur.com/NQWN0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NQWN0.png\" alt=\"average value\"></a></p>\n\n<p>The problem is that I need to see the data second to second to identify something irregular in the data, Is there a way to achieve it? The only way I have found is doing zoom in the data but I would have to use the zoom at each time interval to look for an irregularity in the data</p>\n"
    }, {
        "tags": ["authentication", "methods", "model", "blockchain", "iot"],
        "is_answered": true,
        "view_count": 58,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1576209607,
        "creation_date": 1576168224,
        "question_id": 59308974,
        "link": "https://stackoverflow.com/questions/59308974/what-is-authentication-mechanism-of-iot-node-with-blockchain",
        "title": "what is authentication mechanism of IoT node with blockchain?",
        "body": "<p>i am doing my project on integration of IoT devices, but  here i'm facing some difficulties for the the authentication of node with blockchain, there is PKI infrastructure integrated with blockchain. but i need some purely blockchain base method, if anyone know  about any model or mechanism which is use for authentication through blockcahin please help me. </p>\n"
    }, {
        "tags": ["azure-functions", "azure-cosmosdb", "iot", "azure-eventhub", "azure-stream-analytics"],
        "is_answered": false,
        "view_count": 64,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1576181345,
        "creation_date": 1576126527,
        "question_id": 59297943,
        "link": "https://stackoverflow.com/questions/59297943/data-processing-architecture-in-azure",
        "title": "Data processing architecture in Azure",
        "body": "<p>A system is to be designed where a number of sensors will transmit data to event hub. Each sensor will transmit number of data(max 15-20 (around 5 KB)) each second. </p>\n\n<ul>\n<li>One use case is that all of these raw data has to be transmitted to cosmosDB with minimum/no latency which i could achieve with below architecture.</li>\n</ul>\n\n<p>Raw Data -> Event Hub -> Azure Stream Analytics -> CosmosDB</p>\n\n<p><em>Note: The above use case is to determine connection status of sensor. So it has to be as quick as possible.</em></p>\n\n<ul>\n<li>Another use case is the indication property of sensor which is dependent on the last values from the sensors (at max previous 200 values).\nNow the problem is here. I tried using the following architecture.</li>\n</ul>\n\n<p>Raw Data -> Event Hub -> Azure Stream Analytics -> Azure Function -> CosmosDB</p>\n\n<p>So here, output from stream analytics goes to azure function where it gets the previous 200 values from cosmosDB does necessary calculation and then store the result again back to cosmosDB. But this process seems to be very slow.</p>\n\n<pre><code>COSMOS DB CONTAINERS\n//Sensor Property Container\n{ \n  id: \"sensor_id\",\n  connection:true, //This needs to be updated as soon as raw data is available\n  indication:\"RED\" //This depends on previous 200 values from sensor\n}\n\n//Sensor Raw Value Container\n{ \n  id: \"sensor_id\",\n  rawData: \"RAW_VALUE\",\n}\n</code></pre>\n\n<p>You can look at image below of what i am actually looking for.</p>\n\n<p><a href=\"https://i.stack.imgur.com/OmQ8P.png\" rel=\"nofollow noreferrer\">Data Flow Image</a></p>\n\n<p>I also tried adding user defined function in azure stream analytics to run algorithm within but it doesn't support cosmosDB as reference data.</p>\n"
    }, {
        "tags": ["apache-kafka", "iot"],
        "is_answered": true,
        "view_count": 680,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1576136512,
        "creation_date": 1576086426,
        "last_edit_date": 1576100231,
        "question_id": 59291387,
        "link": "https://stackoverflow.com/questions/59291387/dealing-with-kafka-producer-connection-loss",
        "title": "Dealing with Kafka Producer connection loss",
        "body": "<p>This is not so much of a coding question per se, but more of a architecture design for real-time streaming application. We have the following setup:</p>\n\n<ul>\n<li>Multiple embedded IoT devices in the field (so low memory, but option to have some extended local storage)</li>\n<li>They all are streaming their data in real-time to a Kafka cluster, acting as producers and then we have post-processing applications that act as consumers and help store the data in a database. </li>\n<li>Now sometimes these IoT devices would loose connection to one of the nodes in the Kafka cluster, since the network connections in the field are not always reliable. These sort of disconnections could last up to a day typically.</li>\n</ul>\n\n<p>Now I understand that Kafka takes care of nodes (acting as brokers) failing in the cluster, but what if I have a situation where the producer just does not have a good network connection and just cannot publish its data to the Kafka topic because it cannot see it?</p>\n\n<p>We cannot afford to loose any data, but the good news is that we have expandable storage options for the embedded IoT devices where we could save the data when the IoT device goes offline and then stream it when the connection is back up. Is this something that is recommended with Kafka? In particular I have the following questions:</p>\n\n<ol>\n<li>Does Kafka have a built in way for the producers to have some kind of offline on-disk (NOT in-memory) storage cache?</li>\n<li>How does Kafka deal with messages to topics that just cannot be sent, due to network connectivity issues? Is there a way to just schedule them in a queue and then wait until the connection to the cluster is back up?</li>\n<li>What kind of local storage options could I use which I can easily interface with as my on-disk cache?</li>\n<li>How about having a redundant local time-series database (on the embedded device's storage) just collecting all the data stream and then have an agent take care of the sending of the data to the Kafka cluster, and then clean the database up when it gets an acknowledgement from the Kafka broker?</li>\n<li>Are there any other ways to deal with these situations where the Kafka Producers have intermittent connection to the cluster and can just sent the stream data in chunks when it is connected?       </li>\n</ol>\n"
    }, {
        "tags": ["azure", "bigdata", "cloud", "iot", "azure-eventhub"],
        "is_answered": true,
        "view_count": 1609,
        "favorite_count": 1,
        "score": 3,
        "last_activity_date": 1574939254,
        "creation_date": 1574936302,
        "question_id": 59086482,
        "link": "https://stackoverflow.com/questions/59086482/understanding-azure-event-hubs-partitioned-consumer-pattern",
        "title": "Understanding Azure Event Hubs partitioned consumer pattern",
        "body": "<p>Azure Event Hub uses the partitioned consumer pattern described in the <a href=\"https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features\" rel=\"nofollow noreferrer\">docs</a>.\nI have some problems understanding the consumer side of this model when it comes to a real world scenario.</p>\n\n<p>So lets say I have 1000 messages send to the event hub with 4 partitions, not defining any partition Id. This means the messages will go to all partitions using the round-robin method.</p>\n\n<p>Now I want to have two applications distributing the messages to two different databases. My questions there are:</p>\n\n<ol>\n<li>Lets say for the first application, I want to store all messages in Database 1. This means, for maximum speed, In my consumer application I need to have 4 threads (consumers), each listening to one partition of the event hub, right? Each of them also has to store their own offset for the partition they're reading (checkpoint).</li>\n<li>Lets say my second application wants to filter the messages and only store a subset of them in Database 2. There I also need 4 consumers since I don't know which message goes to which partition, right?</li>\n<li>Also for the two applications I need to have two consumer groups, but why? Is the filtering of the messages defined in the consumer group? I don't get it really why I need this one, since the applications consumers store the partition checkpoints by themselves and I can do the filtering within the applications itself.</li>\n</ol>\n\n<p>I know there is the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.eventhubs.processor.eventprocessorhost?view=azure-dotnet\" rel=\"nofollow noreferrer\">EventProcessorHost</a> class but I want to understand the concept of the EventHub on a lower level.</p>\n"
    }, {
        "tags": ["video-streaming", "streaming", "iot", "hardware-interface", "hardware-programming"],
        "is_answered": false,
        "view_count": 12,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1574873792,
        "creation_date": 1574873792,
        "question_id": 59075064,
        "link": "https://stackoverflow.com/questions/59075064/what-configuration-i-need-for-real-time-streaming-with-audio-at-720p",
        "title": "What Configuration i need for real time streaming with audio at 720p?",
        "body": "<p>I am planning for IoT project that streams 720p RTSP or RTMP with audio at 60FPS with WIFI.\nI need to know what processor speed and what ram i will need on IoT board to stream it online with cloud and show on end mobile device.\nAlso any other hardware i need then please let me know.</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["apache-flink", "iot", "flink-streaming"],
        "is_answered": true,
        "view_count": 182,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1574746588,
        "creation_date": 1574739902,
        "question_id": 59043453,
        "link": "https://stackoverflow.com/questions/59043453/apache-flink-emit-output-records-in-flink-based-on-keyed-state-even-if-no-input",
        "title": "Apache Flink: Emit output records in Flink based on keyed state even if no input records have arrived for a given aggregation window",
        "body": "<p>I am trying to use Apache Flink for an IoT application. I have a bunch of devices that can be in one of several states. When a device changes state, it emits a message that includes an event timestamp and the state it changed to. For one device, this might look like this:</p>\n\n<p>{Device_id: 1, Event_Timestamp: 9:01, State: STATE_1}</p>\n\n<p>{Device_id: 1, Event_Timestamp: 9:03, State: STATE_2}</p>\n\n<p>For each device, I need to produce a five minute aggregate for the amount of time the device spent in each state for the given five minute window. In order to do this, I plan to use the keyed state to store the last state update for each device, so that I know what state the device was in for the beginning of the aggregation window. For example, assume the device with id \"1\" has a keyed state value that said it entered \"STATE_2\" at 8:58. Then the output of the aggregation for the 9:00 - 9:05 window would like like this (based on the two example events above):</p>\n\n<p>{Device_id: 1, Timestamp: 9:00, State: STATE_1, Duration: 120 seconds}</p>\n\n<p>{Device_id: 1, Timestamp: 9:00, State: STATE_2, Duration: 180 seconds}</p>\n\n<p>My problem is this: Flink will only open a window for a given device_id if there is an event for the window. This means that if a device does not change state for over 5 minutes, no record will enter the stream, so the window will not open. However, I need to emit a record that says that the device spent the entire five minutes in whatever the current state is based on what is stored in the keyed state. For example, Flink should emit a record for 9:05-9:10 that says the device with id \"1\" spent all 300 seconds in \"STATE_2\". </p>\n\n<p>Is there a way to output records for the amount of time each device spent in a given state for a five minute aggregation window EVEN IF the state does not change within those five minutes, and, thus, the device sends no events? If not, are there any workarounds I can use to get the output events I need for my application?</p>\n"
    }, {
        "tags": ["iot", "esp32", "cumulocity"],
        "is_answered": true,
        "view_count": 168,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1574105275,
        "creation_date": 1574089812,
        "question_id": 58917566,
        "link": "https://stackoverflow.com/questions/58917566/i-need-help-connecting-an-esp32-to-cumulocity",
        "title": "I need help connecting an ESP32 to Cumulocity",
        "body": "<p>I am very new to IOT. I dont really know much about anything but cant seem to find out anything because everywhere I read there is much assumed knowledge. I have the task of connecting an ESP32 to Cumulocity but dont have much idea where to start. I have seen on a cumulocity video that they have an \"agent\" for a large amount of devices which is something you can download that will sort out connecting the device to cumulocity for you however they do not have an agent for my ESP32 device which I am trying to connect. It would be much appreciated if someone could head me in the right direction about how I should go about connecting my ESP32 to Cumulocity's platform.</p>\n"
    }, {
        "tags": ["reactjs", "iot", "data-handling"],
        "is_answered": true,
        "view_count": 291,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1573487672,
        "creation_date": 1573484462,
        "question_id": 58803819,
        "link": "https://stackoverflow.com/questions/58803819/react-always-fetch-data-or-save-data-frontend",
        "title": "react: always fetch data or save data frontend",
        "body": "<p>So i am working on a IoT SaaS project in React.\nThe user selects a sensor and a time range and receives data visualized in charts with a resolution about 5 minutes.\nMy question is regarding best practices when handling fetching and saving of this data on the front-end.\nI have tried always fetching which works fine, but makes the system kind of slow.\nThis is especially true while users are quickly switching back and forth between sensors.\nI have also tried saving the data, just as a json in the react state.\nThis significantly increase performance, but has a lot of other problems.\nThe browser starts complaining about ram uses and can sometimes get out of memory errors.\nThere is also a lot of needed data handling as saving several non continuous data-ranges for the same sensor, locating and merging date-range overlaps etc...</p>\n\n<p>So i am wondering what is the best practice here, should i always fetch or save on front-end? are there any frameworks i could use helping me with the data handling front-end or do i have to do this manually.</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-sphere"],
        "is_answered": true,
        "view_count": 172,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1573038131,
        "creation_date": 1572823413,
        "last_edit_date": 1573038131,
        "question_id": 58685510,
        "link": "https://stackoverflow.com/questions/58685510/can-you-ssh-into-an-azure-sphere-mt-3620",
        "title": "Can you ssh into an azure sphere MT-3620?",
        "body": "<p>Is it possible to connect to some sort of shell on an MT-3620.</p>\n\n<p>I want to be able to see things like top and system performance.</p>\n"
    }, {
        "tags": ["c#", "iot"],
        "is_answered": true,
        "view_count": 103,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1572292767,
        "creation_date": 1540996697,
        "question_id": 53085938,
        "link": "https://stackoverflow.com/questions/53085938/microsoft-azure-iot-hub",
        "title": "Microsoft Azure Iot hub",
        "body": "<p>I have a device send frames of data to azure iot hub , What is the best practice to \"translate these frames\" and save it into SQL Db or Cosmos Db ?</p>\n"
    }, {
        "tags": ["hyperledger-fabric", "hyperledger", "blockchain", "iot"],
        "is_answered": false,
        "view_count": 276,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1572229049,
        "creation_date": 1542797193,
        "question_id": 53410393,
        "link": "https://stackoverflow.com/questions/53410393/hyperledger-fabric-iot-use-case",
        "title": "Hyperledger Fabric: IoT use case",
        "body": "<p><strong>Use case</strong>: a smart home which gathers raw data from all the sensors within it, processes them and extracts high level information from them. The owner of the house might want to share these information with other people, such as doctors, family members, friends... So, I'm trying to figure out which would be the best way to handle the access permissions on these data.  Right now all the information are carefully encrypted and stored in a database (untrusted) and only the people with the right keys can properly decrypt those data. </p>\n\n<p><strong>My idea</strong>: I want to use Hyperledger Fabric to store and manage the access permissions to these files and also to store the hash digest of the gathered information for immutability purposes. Once the smart home generates an high level information from the raw data, it stores it inside the database and then it issues a transaction to Hyperledger Fabric with the timestamp and the hash digest of the data. \nThe smart home owner can share these information with other people, issuing a transaction with the ID of this person and an identifier of the data he would have the access rights on. \nSo before accessing the information stored inside the encrypted database, the application would check if the requester has the valid permissions stored within the blockchain.</p>\n\n<p><strong>My doubts and questions</strong>: since I'm really new on this topic, even though I've read a lot about it, I don't know if this would be an improper use of the Hyperledger Fabric. All the use cases I read about it, store all the data with Hyperledger Fabric, without relying on an external cloud storage service.</p>\n\n<p>Since all the transactions would be stored in the blockchain and the blockchain is maintained by all the peers inside the same channel (btw I would use just one channel to keep everything), they may be able to access to the Hyperledger Fabric database and extract information about the smart home. Am I wrong? If not, how can I solve this issue? I could use the identity mixer feature to \"hide\" the transaction issuer, but still the transaction would be visible to all of the peers who keep the blockchain available. </p>\n"
    }, {
        "tags": ["mysql", "sql", "iot"],
        "is_answered": false,
        "view_count": 22,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1571832516,
        "creation_date": 1571829293,
        "last_edit_date": 1571829712,
        "question_id": 58521597,
        "link": "https://stackoverflow.com/questions/58521597/iot-mysql-query-sometimes-i-got-subquery-returns-more-than-1-row",
        "title": "IoT Mysql Query sometimes i got - Subquery returns more than 1 row",
        "body": "<p>I have te next Query:</p>\n\n<pre><code>SELECT IF(IF(atmp = 1 AND h_start1 &lt;= (SELECT HOUR(NOW())) AND\n             (SELECT HOUR(NOW())) &lt;= h_stop1 OR\n             h_start2 &lt;= (SELECT HOUR(NOW())) AND\n             (SELECT HOUR(NOW())) &lt;= h_stop2,\n             t_temp1,\n             set_temp) &lt; temp - modEn AND ccl = 1,\n          1,\n          0) clima,\n       IF(IF(atmp = 1 AND h_start1 &lt;= (SELECT HOUR(NOW())) AND\n             (SELECT HOUR(NOW())) &lt;= h_stop1 OR\n             h_start2 &lt;= (SELECT HOUR(NOW())) AND\n             (SELECT HOUR(NOW())) &lt;= h_stop2,\n             t_temp1,\n             set_temp) &gt; temp + modEn AND cct = 1,\n          1,\n          0) centrala,\n       'studio' camera\n  FROM (SELECT (SELECT date\n                  FROM termostat\n                 WHERE topic = 'control-clima'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') ccl,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'control-centrala'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') cct,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'auto-temp'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') atmp,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'mod'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') modEn,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 't-temp1'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') t_temp1,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 't-temp2'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') t_temp2,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'h-start1'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') h_start1,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'h-start2'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') h_start2,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'h-stop1'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') h_stop1,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'h-stop2'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') h_stop2,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'set-temp'\n                   AND incapere = 'studio'\n                   AND zi = 'miercuri') set_temp,\n               (SELECT date\n                  FROM termostat\n                 WHERE topic = 'studio/climat/temperatura') temp) t1\n</code></pre>\n\n<p>Everything goes good but un studio room chip, i have the problem <code>Subquery returns more than 1 row</code> \nWith the same code i can read heating and a/c data on speciefied room and day from the rest of the rooms... </p>\n\n<p>Maybe some of you are kind an help me on improvement of the query too (I know it is much space for better!)</p>\n\n<p>Thankyou guys!</p>\n"
    }, {
        "tags": ["mqtt", "iot", "node-red"],
        "is_answered": true,
        "view_count": 791,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1571335314,
        "creation_date": 1571328910,
        "question_id": 58436871,
        "link": "https://stackoverflow.com/questions/58436871/how-to-pass-mqtt-topic-data-to-postgres-on-node-red",
        "title": "How to pass MQTT Topic data to Postgres on Node-Red",
        "body": "<p>I am working on integrating MQTT Topic with Postgres DB on Node-Red.I have one MQTT Topic for e.g 'query/topic1' which is getting some value from a publisher, I want to save this value into Postgres database.For that, I created flow on Node-Red.I have created MQTT subscriber component which is feeding 'function' block which in turn is input to Postgres.I have written below function in 'function' component (named 'query' in the image) in Node-Red.</p>\n\n<pre><code>msg.payload = [\n    {\n        query: 'begin',\n    },\n    {\n        query: 'select * from table1 where field2 &gt; $param1',\n    params: {\n            param1: $msg.payload,\n        },\n        output: true,\n    },\n    {\n        query: 'commit',\n    },\n];\n\nreturn msg;\n</code></pre>\n\n<p>I want the \"$param1\" to be the value coming from MQTT topic, but I am confused how to do that.I tried using '$msg.payload' but it did not work.Can you please suggest. I am new to these stuffs.</p>\n\n<p><a href=\"https://i.stack.imgur.com/wfvA3.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/wfvA3.png\" alt=\"enter image description here\"></a></p>\n"
    }, {
        "tags": ["iot", "contiki"],
        "is_answered": false,
        "view_count": 800,
        "favorite_count": 0,
        "score": 3,
        "last_activity_date": 1571333442,
        "creation_date": 1482185865,
        "last_edit_date": 1486503595,
        "question_id": 41231909,
        "link": "https://stackoverflow.com/questions/41231909/contiki-cooja-not-showing-stats-in-collect-view",
        "title": "Contiki Cooja not showing stats in collect view",
        "body": "<p>I cannot get the graphs and statistics of collect view example. I have run the simulation for many minutes, but when I go to tools and select collect view the nodes portion in the next window is empty and I have also selected start collect and send commands to nodes but no luck nothing shows up on the graphs :</p>\n\n<p><img src=\"https://i.stack.imgur.com/GPCza.png\" alt=\"simulation picture click to view\"></p>\n"
    }, {
        "tags": ["cloud", "iot", "complex-event-processing", "stream-processing"],
        "is_answered": false,
        "view_count": 177,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1570769808,
        "creation_date": 1569693376,
        "last_edit_date": 1569694084,
        "question_id": 58148955,
        "link": "https://stackoverflow.com/questions/58148955/cep-for-edge-computing",
        "title": "CEP for edge computing",
        "body": "<p>In an IoT plataform, I need to define some rule-based behaviour, configured by domain experts through Web UIs. I have device measurments/events flowing through MQTT channels from IoT gateways to cloud, but I would prefer evaluate rules on edge instead of send more data to cloud. I'm looking for some tool that offers a DSL to write rules and write a minimal glue-code to collect the data that is evaluated. I know few of them (Apache Strom, Drools, Something done with Akka Streams) but it seems that are meant for cloud/distribuited environments, where scalability and fault tolerance are the most important aspects; on the other hand an edge computing app should just have low resource consumption, no scalability and fully configurable from remote. </p>\n\n<p><a href=\"https://i.stack.imgur.com/5IC26.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/5IC26.png\" alt=\"enter image description here\"></a></p>\n\n<p>I've also thought to build it on my own, but I can't believe does not exist something similar, since it should be a common use case in edge-driven IoT scenarios</p>\n"
    }, {
        "tags": ["iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 189,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1569427815,
        "creation_date": 1569421874,
        "question_id": 58100801,
        "link": "https://stackoverflow.com/questions/58100801/best-way-to-send-xml-from-iot-device-to-azure-iot-hub",
        "title": "Best way to send xml from iot device to azure iot hub?",
        "body": "<p>I have an energy monitor that can only output xml data via http post. I am looking to send this data to an azure-iot hub for processing and storage. What is the best way to send xml data to from several of these devices to the hub? I have looked at various gateways but havent found a simple, scalable, cost effective way to do this. I am open to having some sort of intermediary but they all  introduce a layer of complexity to simply sending the data to the hub.</p>\n"
    }, {
        "tags": ["firebase", "google-cloud-firestore", "nosql", "iot", "relation"],
        "is_answered": true,
        "view_count": 45,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1569034642,
        "creation_date": 1569034035,
        "question_id": 58036882,
        "link": "https://stackoverflow.com/questions/58036882/firestore-how-to-model-and-query-relation-of-2-collections-iot-use-case-v2",
        "title": "Firestore - How to model and query relation of 2 collections - IOT use case - v2",
        "body": "<p>extending to my question\n<a href=\"https://stackoverflow.com/questions/57994090/firestore-how-to-model-and-query-relation-of-2-collections-iot-use-case\">Firestore - How to model and query relation of 2 collections - IOT use case</a></p>\n\n<p>I'ved now seen a video on this, and this recommends modelling relations using document ID. \n<a href=\"https://www.youtube.com/watch?v=jm66TSlVtcc\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=jm66TSlVtcc</a> skip to 6:07</p>\n\n<p>I want to know if it would work in this case (modifying the example from my original question to fit to this youtube recommendation of firestore relation:</p>\n\n<p>Eg: I have 2 different collection - tracking and venue</p>\n\n<p>tracking &lt;-- collection</p>\n\n<pre><code>1. document(xyz123)\nvenueId = \"abcd1234\"\ntimestamp = 10/09/2019 10:00\n\n2. document(xyz567)\nvenueId = \"efgh3456\"\ntimestamp = 10/09/2019 11:00\n</code></pre>\n\n<p>venue &lt;-- collection</p>\n\n<pre><code>1. document(abcd1234) &lt;-- notice i shift the device_unique_identifier here instead\nname = \"room A\"\n// device_unique_identifier = \"abcd1234\" &lt;-- this is unique name\n\n2. document(efgh3456) &lt;-- notice i shift the device_unique_identifier here instead\nname = \"room B\"\n// device_unique_identifier = \"efgh3456\" &lt;-- this is unique name\n</code></pre>\n\n<p>Main question:\nI would like to query document xyz123 and get the name of the venue in the row. So the output would be:</p>\n\n<pre><code>document(xyz123)\ndevice_unique_identifier = \"abcd1234\"\ntimestamp = 10/09/2019 10:00\nvenue.name = \"room A\"\n</code></pre>\n\n<p>On another possible extra question, when inserting the tracking data, would it be possible to auto insert the venue data as an object in firestore backend without the need to query venue the data ?</p>\n"
    }, {
        "tags": ["firebase", "google-cloud-firestore", "nosql", "iot", "relation"],
        "is_answered": true,
        "view_count": 151,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1568939741,
        "creation_date": 1568813827,
        "last_edit_date": 1568854509,
        "question_id": 57994090,
        "link": "https://stackoverflow.com/questions/57994090/firestore-how-to-model-and-query-relation-of-2-collections-iot-use-case",
        "title": "Firestore - How to model and query relation of 2 collections - IOT use case",
        "body": "<p>Would it be possible to do relation in this sense in firestore ?\nwhereby I want to relate a field in collection to another field in another collection</p>\n\n<p>Eg: I have 2 different collection - tracking and venue</p>\n\n<p>tracking &lt;-- collection</p>\n\n<pre><code>1. document(xyz123)\ndevice_unique_identifier = \"abcd1234\"\ntimestamp = 10/09/2019 10:00\n\n2. document(xyz567)\ndevice_unique_identifier = \"efgh3456\"\ntimestamp = 10/09/2019 11:00\n</code></pre>\n\n<p>venue &lt;-- collection</p>\n\n<pre><code>1. document(zyx123)\nname = \"room A\"\ndevice_unique_identifier = \"abcd1234\" &lt;-- this is unique name\n\n2. document(zyx345)\nname = \"room B\"\ndevice_unique_identifier = \"efgh3456\" &lt;-- this is unique name\n</code></pre>\n\n<p>I would like to query document xyz123 and get the name of the venue in the row.\nSo the output would be:</p>\n\n<pre><code>document(xyz123)\ndevice_unique_identifier = \"abcd1234\"\ntimestamp = 10/09/2019 10:00\nvenue.name = \"room A\"\n</code></pre>\n\n<p>Here is a screenshot how the data may look like:\n<a href=\"https://i.imgur.com/oTL6ROI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.imgur.com/oTL6ROI.png\" alt=\"tracking\"></a>\n<a href=\"https://i.imgur.com/ri12dvt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.imgur.com/ri12dvt.png\" alt=\"place\"></a>\nReason for tracking data, in a realtime use case, doesnt have the luxury (time) to query the name in venue collection, so insertion (writing) have to be in this way (meaning only the device_unique_identifier is available for insertion). Therefore, to do the relation, we would only do it in the query.</p>\n\n<p>I would like advise how to model and query such a relation.</p>\n"
    }, {
        "tags": ["hardware", "iot", "wireless", "z-wave"],
        "is_answered": false,
        "view_count": 45,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1568798204,
        "creation_date": 1568797397,
        "question_id": 57988948,
        "link": "https://stackoverflow.com/questions/57988948/will-z-wave-penatrate-through-concrete-wall-which-module-to-be-seleted-to",
        "title": "Will Z-Wave penatrate through concrete wall..???, which module to be seleted to do so..?",
        "body": "<p>I am looking for a Wireless PAN set-up, for IoT application development. area to be covered is around 2000 square feet. which is best Protocol, with low cost. </p>\n\n<p>I have listed out and compared all available wireless technologies, bit of confusion in which one to be selected.</p>\n\n<p>i expect a low cost, low maintenance, low power solution.</p>\n"
    }, {
        "tags": ["kubernetes", "architecture", "iot", "horizontal-scaling", "kubernetes-jobs"],
        "is_answered": false,
        "view_count": 27,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1568759535,
        "creation_date": 1568759535,
        "question_id": 57982818,
        "link": "https://stackoverflow.com/questions/57982818/suggestions-on-breaking-down-an-iot-application-consisting-of-jobs-and-services",
        "title": "Suggestions on breaking down an IOT application consisting of jobs and services hosted in kubernetes in a way which would enable horizontal scaling",
        "body": "<p>I have an IOT application which is architected in the following way :-</p>\n\n<p>There are plants which has its own sets of devices.</p>\n\n<p>Now the entire pipeline is deployed in kubernetes consisting of the following units :-</p>\n\n<ol>\n<li>A job which wakes up every x seconds, reads data from all plants, pushes the data to a mqtt broker.</li>\n<li>An MQTT broker.</li>\n<li>A subscriber service which receives data from all plants and pushes it to a timeseries database.</li>\n<li>Jobs running at intervals of 5min, 15min, 1hr, 4hr, 1day and performs downsampling on the data of all the projects and pushes it to separate downsampled tables.</li>\n<li>Jobs running every day to check if there was in holes/gaps in the data and tries to fill it up if possible.</li>\n</ol>\n\n<p>Now this works fine for few Plants, but when the number of plants increases it becomes difficult perform data retrieval, push, downsampling using single service/job as it becomes too much memory intensive and chokes at multiple places. As a temporary fix scaling it vertically fixes the issue to some extent but in that case I need to put all the pods in a single machine which I am scaling vertically and scaling multiple nodes vertically is quite expensive.\nHence I am planning to break down the system in a way so that I can scale horizontally and I am looking for suggestions for the possible ways I can achieve this.</p>\n"
    }, {
        "tags": ["c", "assembly", "microcontroller", "iot", "pic"],
        "is_answered": true,
        "view_count": 972,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1568670349,
        "creation_date": 1568661369,
        "question_id": 57963143,
        "link": "https://stackoverflow.com/questions/57963143/which-language-can-be-used-to-program-pic10f200-202-204-206",
        "title": "Which language can be used to program PIC10F200/202/204/206?",
        "body": "<p>In a pursuit to use a low cost, less pin micro-controller, I have ended up with the PIC series PIC10F200/202/204/206 as a prospective choice. My use cases for them involve using just one or two pins to control external peripherals like DC motors, LEDs. And using at most one OR two sensors for input. I have some experience with Arduino but no experience with PIC series at all.</p>\n\n<p>Looking at the projects over the internet, it seems that the higher series of PIC micro-controllers like PIC16*, PIC18* can be programmed in both C and assembly language. But for the series - PIC10F200/202/204/206, I see programs being developed only in assembly language. I have read somewhere that it is due to less memory available in the PIC10F* micro-controllers. I can program in C, but have no idea about assembly language. It seems difficult to learn as well, and hence could be a dead end for me for now if assembly is the only language I can use.</p>\n\n<p>So, I want to ask, is it NOT possible to program these specific microcontrollers using C language? Is it really a constraint that I need to use assembly language only? </p>\n\n<p>OR </p>\n\n<p>Is it just my misunderstanding and the PIC10F* series can be programmed with C language just like PIC16* and PIC18* series.</p>\n\n<p>Please let me know if you need any further information from my side before answering this.</p>\n\n<p>Thanks,</p>\n\n<p>Anurag</p>\n"
    }, {
        "tags": ["arduino", "raspberry-pi", "iot"],
        "is_answered": true,
        "view_count": 649,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1568008927,
        "creation_date": 1567853073,
        "last_edit_date": 1568008927,
        "question_id": 57832925,
        "link": "https://stackoverflow.com/questions/57832925/calculate-the-speed-of-car-using-raspberry-pi-and-sensors",
        "title": "Calculate the speed of car using raspberry pi and sensors",
        "body": "<p>I have senario where I need to calculate the speed of car.\nI am going to mount raspberry pi inside car and will be using some kind of sensor to calculate the speed of car in km per hour.</p>\n\n<p>I need that sensor recommendation.</p>\n\n<p>Which sensor should I using to get the speed of car in km per hour??</p>\n"
    }, {
        "tags": ["websocket", "iot", "sensors"],
        "is_answered": false,
        "view_count": 15,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1566394216,
        "creation_date": 1566390916,
        "question_id": 57591762,
        "link": "https://stackoverflow.com/questions/57591762/can-you-provide-information-about-websocket",
        "title": "can you provide information about websocket",
        "body": "<p>I am doing research on websocket in the world of IoT, but the scope of information I have is quite small. I like the suggestion, if you can share information about the website, if you can, thank you.</p>\n\n<p>I read several papers about IoT, including the application of websocket in the queuing system, there is also a comparative analysis of the performance of Xbee and Websocket.</p>\n"
    }, {
        "tags": ["sql", "json", "azure-sql-database", "iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 827,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1566218835,
        "creation_date": 1566209456,
        "last_edit_date": 1566209526,
        "question_id": 57554632,
        "link": "https://stackoverflow.com/questions/57554632/how-to-store-json-data-with-dynamic-keys-in-sql-server",
        "title": "How to store JSON data with dynamic keys in SQL Server",
        "body": "<p>I currently have several IoT devices connected to Azure IoT Hub publishing information and storing it in Azure SQL Server.</p>\n\n<p>I would like to expand the capabilities of these devices to allow for the storage of some dynamic JSON data and understand the best way to structure this information.</p>\n\n<p>The 2 examples show information being sent from 2 different IoT assets, the only keys that are guaranteed to be common to both are the TimeStamp, AssetId, RPM, Pwr and Runhrs. All other keys will be dynamic.</p>\n\n<p>I have considered using tagging as described here: <a href=\"https://stackoverflow.com/questions/20856/recommended-sql-database-design-for-tags-or-tagging\">Recommended SQL database design for tags or tagging</a></p>\n\n<p>However, I do not know how this solution would work for multiple tags against one record.</p>\n\n<pre><code>{\n   \"TimeStamp\":\"2019-04-23T18:25:43.511Z\",\n   \"AssetId\":\"25896321A\",\n   \"RPM\":1000,\n   \"Pwr\":100,\n   \"PF\":1.00,\n   \"Gfrq\":50.0,\n   \"Vg1\":11000,\n   \"Vg2\":10987,\n   \"Vg3\":10785,\n   \"Vg12\":0,\n   \"Vg23\":0,\n   \"Vg31\":0,\n   \"Ig1\":0,\n   \"Ig2\":0,\n   \"Ig3\":0,\n   \"Mfrq\":50.0,\n   \"Vm1\":227,\n   \"Vm2\":228,\n   \"Vm3\":229,\n   \"Vm12\":393,\n   \"Vm23\":396,\n   \"Vm31\":395,\n   \"MPF\":0.00,\n   \"SRO\":0.000,\n   \"VRO\":50.0,\n   \"CPUT\":33.6,\n   \"Unknown1\":0,\n   \"GasP\":0.01,\n   \"Mode\":\"AUT\",\n   \"kWhour\":13188243,\n   \"Runhrs\":28187,\n   \"Numstr\":3312,\n   \"Unknown2\":122113663,\n   \"Unknown3\":0.00,\n   \"OilLev\":103,\n   \"OilT\":45,\n   \"ThrPos\":null,\n   \"CCPres\":-0.01,\n   \"AirInT\":29,\n   \"RecAT\":36,\n   \"Unknown4\":100,\n   \"ActPwr\":0,\n   \"ActDem\":0,\n   \"ActPfi\":0,\n   \"CylA1\":51,\n   \"CylA2\":51,\n   \"CylA3\":51,\n   \"CylA4\":51,\n   \"CylA5\":51,\n   \"CylA6\":50,\n   \"CylB1\":53,\n   \"CylB2\":53,\n   \"CylB3\":53,\n   \"CylB4\":53,\n   \"CylB5\":52,\n   \"CylB6\":53,\n   \"JWTin\":50,\n   \"JWTout\":50,\n   \"JWGKin\":36,\n   \"Unknown5\":8211,\n   \"Unknown6\":2,\n   \"CH4\":0,\n   \"BIN\":\"1010001111000001\"\n}\n</code></pre>\n\n<p>or </p>\n\n<pre><code>{\n   \"TimeStamp\":\"2019-04-23T18:28:20.511Z\",\n   \"AssetId\":\"28547896Z\",\n   \"RPM\":1000,\n   \"Pwr\":100,\n   \"PF\":1.00,\n   \"Gfrq\":50.0,\n   \"Vg1\":11000,\n   \"Vg2\":10987,\n   \"Vg3\":10785,\n   \"Vg12\":0,\n   \"Vg23\":0,\n   \"Vg31\":0,\n   \"Ig1\":0,\n   \"Ig2\":0,\n   \"Ig3\":0,\n   \"Mfrq\":50.0,\n   \"Vm1\":227,\n   \"Vm2\":228,\n   \"Vm3\":229,\n   \"Vm12\":393,\n   \"Vm23\":396,\n   \"Vm31\":395,\n   \"MPF\":0.00,\n   \"SRO\":0.000,\n   \"VRO\":50.0,\n   \"CPUT\":33.6,\n   \"Unknown1\":0,\n   \"GasP\":0.01,\n   \"Mode\":\"AUT\",\n   \"kWhour\":13188243,\n   \"Runhrs\":28187,\n   \"Numstr\":3312,\n   \"Unknown2\":122113663,\n   \"Unknown3\":0.00,\n   \"OilLev\":103,\n   \"OilT\":45,\n   \"ThrPos\":null,\n   \"CCPres\":-0.01,\n   \"AirInT\":29,\n   \"RecAT\":36,\n   \"Unknown4\":100,\n   \"ActPwr\":0,\n   \"ActDem\":0,\n   \"ActPfi\":0,\n   \"CylA1\":51,\n   \"CylA2\":51,\n   \"CylA3\":51,\n   \"CylA4\":51,\n   \"CylA5\":51,\n   \"CylA6\":50,\n   \"CylA7\":51,\n   \"CylA8\":50,\n   \"CylB1\":53,\n   \"CylB2\":53,\n   \"CylB3\":53,\n   \"CylB4\":53,\n   \"CylB5\":52,\n   \"CylB6\":53,\n   \"CylB7\":51,\n   \"CylB8\":50,\n   \"JWTin\":50,\n   \"CH4\":0,\n   \"BIN\":\"1010001111000001\"\n}\n</code></pre>\n\n<p>We have over 300 IoT devices publishing this information at a minimum of every 10 minutes. Therefore, 144 records per day minimum.</p>\n\n<p>What would be the best way to store / structure the dynamic JSON data?</p>\n\n<p>Queries will be performed from a web reporting platformed and will be required to be fast.</p>\n"
    }, {
        "tags": ["cassandra", "hbase", "iot", "phoenix", "opentsdb"],
        "is_answered": true,
        "view_count": 711,
        "favorite_count": 4,
        "score": 1,
        "last_activity_date": 1565343488,
        "creation_date": 1475414275,
        "last_edit_date": 1475695226,
        "question_id": 39817430,
        "link": "https://stackoverflow.com/questions/39817430/real-time-analytics-time-series-database",
        "title": "Real time analytics Time series Database",
        "body": "<p>I'm looking for a distributed Time series database which is free to use in a cluster setup up mode and production ready plus it has to fit well in the hadoop ecosystem.</p>\n\n<p>I have an IOT project which is basically around 150k Sensors which send data every 10 minutes or One hour, so I'm trying to look at time series database that has useful functions like aggregating metrics, Down-sampling, pre-aggregate (roll-ups) i have found this comparative in this Google stylesheet document <a href=\"https://docs.google.com/spreadsheets/d/1sMQe9oOKhMhIVw9WmuCEWdPtAoccJ4a-IuZv4fXDHxM/edit#gid=1102272647\" rel=\"nofollow\">time series database comparative</a> .</p>\n\n<p>I have tested Opentsdb, the data model of the hbaserowkey really suits my use case :  but the functions that sill need to be developed for my use case are :</p>\n\n<ul>\n<li>aggregate multiples metrics </li>\n<li>do rollups </li>\n</ul>\n\n<p>I have tested also keirosDB which is a fork of opentsdb with a richer API and it uses Cassandra as a backend storage the thing is that their API does all what my looking for downsampling rollups querying multiples metrics and a lot more.</p>\n\n<p>I have tested Warp10.io and Apache Phoenix which i have read here <a href=\"http://hortonworks.com/blog/hood-ambari-metrics-grafana/\" rel=\"nofollow\">Hortonworks link</a> that it will be used by Ambari Metrics so i assume that its well suited for time series data too.</p>\n\n<p>My question is as of now what's the best Time series Database to do real time analytics with requests performance under 1S for all the type of requests example : we want the average of the aggregated data sent by 50 sensors in a period of 5 years resampled by months ?</p>\n\n<p>Such requests I assume can't be done under 1S so I believe for such requests we need some rollups/ pre aggregate  mechanism, but I'm not so sure because there's a lot of tools out there and i can't decide which one suits my need the best.</p>\n"
    }, {
        "tags": ["node.js", "rest", "mongoose", "iot"],
        "is_answered": true,
        "view_count": 275,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1564748359,
        "creation_date": 1564733806,
        "last_edit_date": 1564735740,
        "question_id": 57322433,
        "link": "https://stackoverflow.com/questions/57322433/creating-rest-api-to-handle-json-data-from-different-sensors-on-node-js-mongod",
        "title": "Creating REST API to handle JSON data from different sensors on Node.js &amp; MongoDB",
        "body": "<p>I am trying to make a REST API to handle JSON data from sensors(thermometer, hygrometer) to store and process temperature and humidity data. However at the moment, I am not getting the data directly from sensors yet so I am planning on sending dummy data to the node.js server from a client through http GET/POST requests.</p>\n\n<p>I am using Node.js as a server and I'm trying to save into mongodb using mongoose.</p>\n\n<p>When trying to design this system using mvc design pattern, I was at first trying to only make one sensor.model.js &amp; sensor.controller.js but the problem arose when I had to deal with two different sensor data where each sends its temperature data or humidity data. So I wasn't sure how I should design the API. \nI am supposing that it'd be a better option to just POST each sensor data separately to such as \"localhost:3000/sensors/thermometer/\" and \"localhost:3000/sensors/hygromometer/\". I can now successfully send POST requests to \"localhost:3000/sensors/thermometer/\" and \"localhost:3000/sensors/hygromometer/\" but I want to able to send GET method to fetch all data from '/sensors' sorted by sensor_type. How can I make this possible? Is there any good way to come up with this?</p>\n\n<p>I put codes for sensor.js and thermometer.js below. hydrometer.js is the exact same as thermometer.js so I did not bother to put it.</p>\n\n<p>Thank you so much in advance.</p>\n\n<pre><code>// sensors.model.js\n\nconst mongoose = require('mongoose');\n\nconst sensorSchema = new mongoose.Schema({\n    _id: mongoose.Schema.Types.ObjectId,\n    // this method below doesn't work.\n    sensor_type: {type: String, ref: 'Hygro'},\n    sensor_type: {type: String, ref: 'Thermo'},\n    //time : { type : Date, default: Date.now },\n    temperature: {type: Number},\n    humidity: {type: Number}\n});\n\nmodule.exports = mongoose.model('Sensor', sensorSchema);\n\n//____________________________________________________________________________\n\n// sensors.route.js\n\nrouter.get('/', (req, res, next) =&gt; {\n    Sensor.find()\n    .select('_id sensor_type temperature humidity')\n    .exec()\n    .then(docs =&gt; {\n        res.status(200).json({\n          sensors: docs.map(doc =&gt; {\n            return {\n                _id: doc._id,\n                sensor_type: doc.sensor_type,\n                temperature: doc.temperature,\n                humidity: doc.humidity + \"%\"\n                }\n          })\n        });\n    })\n    .catch(err =&gt; {\n        res.status(500).json({\n          error : err\n        });\n    });\n\n//___________________________________________________________________________\n\n\n// thermometer.model.js\n\nconst mongoose = require('mongoose');\n\nconst thermoSchema = new mongoose.Schema({\n    _id: mongoose.Schema.Types.ObjectId,\n    sensor_type: {type: String, required: true},\n    temperature: {type: Number, required: true}\n});\n\nmodule.exports = mongoose.model('Thermo', thermoSchema);\n\n//___________________________________________________________________________\n\n\n// thermometer.route.js\n\nrouter.post('/', (req, res, next) =&gt; {\n    // create sensor object\n    const thermo = new Thermo({\n      _id: new mongoose.Types.ObjectId(),\n      sensor_type: req.body.sensor_type,\n      temperature: req.body.temperature\n    });\n    //save thermo obj into the db\n    thermo\n    .save()\n    .then(result =&gt; {\n      console.log(result);\n      res.status(201).json({\n        message: 'Created sensor data successfully',\n        createdSensor_data: {\n          sensor_type: result.sensor_type,\n          temperature: result.temperature,\n          _id: result._id\n        }\n      });\n    })\n    .catch(err =&gt; {\n      console.log(err);\n      res.status(500).json({\n          error: err\n      });\n    });\n}\n</code></pre>\n"
    }, {
        "tags": ["amazon-web-services", "amazon-dynamodb", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 179,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1564373597,
        "creation_date": 1544699303,
        "last_edit_date": 1564373597,
        "question_id": 53760496,
        "link": "https://stackoverflow.com/questions/53760496/how-to-map-iot-stream-data-to-a-indexed-dynamo-db-column",
        "title": "How to map IoT Stream Data to a indexed Dynamo DB column",
        "body": "<p>I generate an IoT Data Stream and send it to the AWS IoT Core.\nI have a DynamoDB with 4 columns: <code>id</code>, <code>timestamp</code>, <code>data1</code> and <code>data2</code>. <code>data1</code> is a String, <code>data2</code> is a Map (of Data).</p>\n\n<p>Then I created an Action. Via SQL I grab all the fields on the IoT Stream, but when I choose \"Insert to DB\", I only have 2 fields and the option to write ALL Data in one column.</p>\n\n<p><a href=\"https://i.stack.imgur.com/JNVHk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/JNVHk.png\" alt=\"Screenshot\"></a></p>\n\n<p>Is it possible to write one of the data in \"Prg\" to its own field?\nDo I have to create a Lambda Function, get the Data out of the Map and write it to DynamoDB back split? Hope there is a better way :).</p>\n"
    }, {
        "tags": ["time", "synchronization", "iot", "contiki", "testbed"],
        "is_answered": false,
        "view_count": 130,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1563990398,
        "creation_date": 1563932502,
        "last_edit_date": 1563950300,
        "question_id": 57174140,
        "link": "https://stackoverflow.com/questions/57174140/delay-latency-calculation-for-zolertia-re-motes",
        "title": "Delay/Latency calculation for Zolertia Re-motes",
        "body": "<p>I intend to calculate the latency of packets sent by multiple (24) sensor nodes to a single sink using <code>CSMA/CA MAC in Contiki-ng</code>. </p>\n\n<p>Apparently, there is no synchronization mechanism available for the zoul platform. The <strong>TSCH</strong> has a mechanism for latency calculation due to its very design. However, this does not exist for <strong>CSMA MAC</strong>. </p>\n\n<p>Any suggestions, please!</p>\n"
    }, {
        "tags": ["python", "raspberry-pi", "cloud", "iot", "sensors"],
        "is_answered": false,
        "view_count": 40,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1562711614,
        "creation_date": 1562603571,
        "last_edit_date": 1562604232,
        "question_id": 56939164,
        "link": "https://stackoverflow.com/questions/56939164/datas-from-sensor-to-website-or-mobile-application",
        "title": "Datas from sensor to website or mobile application",
        "body": "<p>I'm new to this computer science and coding since I'm from mechanical background. Actually I'm under going an project in which I need to sense data from sensor using raspberry pi. I need to send those data to cloud database or any other database and then I should make them to see in website or mobile application? \nIs this possible? If possible, what should I learn for this long process? Please tell me the process involved?\nCurrently I'm learning Python (django) and mySQL what should I learn more? \nActually this is an IOT project! \nThank you in advance! </p>\n"
    }, {
        "tags": ["node.js", "mongodb", "iot", "node-red"],
        "is_answered": true,
        "view_count": 6786,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1562448740,
        "creation_date": 1450349080,
        "last_edit_date": 1450355001,
        "question_id": 34332601,
        "link": "https://stackoverflow.com/questions/34332601/how-to-extract-value-from-mongodb-using-node-red",
        "title": "How to extract value from MongoDB using Node-RED?",
        "body": "<p>I would like to extract value from <strong>MongoDB</strong> using <strong>Node-RED</strong>. Here in the below mentioned flow, <strong>RFID Reader running of Raspberry Pi</strong> reads <strong>RFID(ex:\"badgeID\":12)</strong> and produce value using MQTT, and same value is subscribe in Node-RED and this subscribe value is passed to <strong>MongoDB node</strong> in Node-RED in order to filter value based on this badgeID. In MongoDB database, structure of one document is looks like </p>\n\n<pre><code>{ \"_id\": \"5631a6ba7de98c4a497dfdcb\", \"badgeID\": \"1\", \"tempValue\": 25,\n\"unitofMeasurement\": \"C\" }\n</code></pre>\n\n<p>. For that my flow in Node-RED is as follows: </p>\n\n<pre><code>[{\"id\":\"3015a0de.cfea6\",\"type\":\"mongodb\",\"z\":\"6ab069e2.954f98\",\"hostname\":\"127.0.0.1\",\n\"port\":\"27017\",\"db\":\"iotsuiteuser\",\"name\":\"ProfileDB\"},{\"id\":\"5a6e14aa.a591ec\",\"type\":\"mqtt-\nbroker\",\"z\":\"6ab069e2.954f98\",\"broker\":\"test.mosquitto.org\",\"port\":\"1883\",\"clientid\":\"\",\n\"usetls\":false,\"verifyservercert\":true,\"compatmode\":true,\"keepalive\":\"15\",\"cleansession\":true,\n\"willTopic\":\"\",\"willQos\":\"0\",\"willRetain\":\"false\",\"willPayload\":\"\",\"birthTopic\":\"\",\"birthQos\n\":\"0\",\"birthRetain\":\"false\",\"birthPayload\":\"\"},{\"id\":\"2ec6f370.d1390c\",\"type\":\"mqtt in\",\"z\":\"6ab069e2.954f98\",\"name\":\"RFID Reader Subscriber\",\"topic\":\"badgeDetected\",\"broker\":\"5a6e14aa.a591ec\",\"x\":137,\"y\":151,\"wires\":[[\"fc22a30d.03dd6\"]]},{\"id\":\"fc22a30d.03dd6\",\"type\":\"function\",\"z\":\n\"6ab069e2.954f98\",\"name\":\"Proximity\",\"func\":\"sensorMeasurement=JSON.parse(msg.payload);\\nmsg.payload=sensorMeasurement.badgeID;\n\\nreturn msg;\\n\",\"outputs\":1,\"noerr\":0,\"x\":365,\"y\":188,\"wires\":[[\"48d08989.b72f78\"]]},{\"id\":\n\"bb7c3bfd.4483c8\",\"type\":\"debug\",\"z\":\"6ab069e2.954f98\",\"name\":\"Debug\",\"active\":true,\"console\":\n\"false\",\"complete\":\"payload[0].tempValue\",\"x\":930,\"y\":221,\"wires\":[]},{\"id\":\n\"48d08989.b72f78\",\"type\":\"mongodb in\",\"z\":\"6ab069e2.954f98\",\"mongodb\":\"3015a0de.cfea6\",\"name\":\"ProfileDB\",\"collection\":\"ProfileDB\",\"operation\":\"find\",\"x\":613,\"y\":191,\"wires\":[[\"bb7c3bfd.4483c8\"]]}]\n</code></pre>\n\n<p>Output display in the Node-RED debug console is as follows:</p>\n\n<pre><code>[ { \"_id\": \"5631a6ba7de98c4a497dfdcb\", \"badgeID\": \"1\", \"tempValue\": 25,\n\"unitofMeasurement\": \"C\" }, { \"_id\": \"5631a6cd7de98c4a497dfdcc\", \"badgeID\": \"2\", \n \"tempValue\": 28, \"unitofMeasurement\": \"C\" }, { \"_id\": \"5631b84023f7c97bc1044178\", \"badgeID\": \"3\", \"tempValue\": 31, \"unitofMeasurement\": \"c\" }, { \"_id\": \"565175ded239daf9794b1d48\", \"badgeID\":\n\"4\", \"tempValue\": 24, \"unitOfMeasurement\": \"C\" }, { \"_id\": \"567287cc5e18a39f297395d6\", \n\"badgeID\": \"69d65035\", \"tempValue\": 33, \"unitofMeasurement\": \"C\" } ]\n</code></pre>\n\n<p>I want to select tempValue where badgeID=\"69d65035\". Here the problem is it displays all the document in the given collection. How to do this ? Am i going on wrong path ?\nSnippet of document in the MongoDB database is as follows: \n<a href=\"https://i.stack.imgur.com/hnc7X.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/hnc7X.png\" alt=\"enter image description here\"></a></p>\n"
    }, {
        "tags": ["blockchain", "iot"],
        "is_answered": false,
        "view_count": 163,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1562105712,
        "creation_date": 1562097859,
        "question_id": 56859352,
        "link": "https://stackoverflow.com/questions/56859352/protocol-stack-of-a-blockchain-based-network",
        "title": "protocol stack of a blockchain-based network?",
        "body": "<p>I am newbie to block chain. AFAIK, blockchain is a distributed database that maintains a continuously-growing list of records called blocks secured from tampering and revision.</p>\n\n<p>How to explain blockchain in terms of OSI 7-layer model? Is it based on service in the application layer?\nThx</p>\n"
    }, {
        "tags": ["apache-kafka", "synchronization", "iot", "distributed-system", "stream-processing"],
        "is_answered": true,
        "view_count": 1065,
        "favorite_count": 3,
        "score": 8,
        "last_activity_date": 1559775687,
        "creation_date": 1558961879,
        "question_id": 56326591,
        "link": "https://stackoverflow.com/questions/56326591/synchronize-data-from-multiple-data-sources",
        "title": "Synchronize Data From Multiple Data Sources",
        "body": "<p>Our team is trying to build a predictive maintenance system whose task is to look at a set of events and predict whether these events depict a set of known anomalies or not.</p>\n\n<p>We are at the design phase and the current system design is as follows:</p>\n\n<ul>\n<li>The events may occur on multiple sources of an IoT system (such as cloud platform, edge devices or any intermediate platforms)</li>\n<li>The events are pushed by the data sources into a message queueing system (currently we have chosen Apache Kafka).</li>\n<li>Each data source has its own queue (Kafka Topic).</li>\n<li>From the queues, the data is consumed by multiple inference engines (which are actually neural networks). </li>\n<li>Depending upon the feature set, an inference engine will subscribe to\nmultiple Kafka topics and stream data from those topics to continuously output the inference.</li>\n<li>The overall architecture follows the single-responsibility principle meaning that every component will be separate from each other and run inside a separate Docker container.</li>\n</ul>\n\n<h1>Problem:</h1>\n\n<p>In order to classify a set of events as an anomaly, the events have to occur in the same time window. e.g. say there are three data sources pushing their respective events into Kafka topics, but due to some reason, the data is not synchronized.\nSo one of the inference engines pulls the latest entries from each of the kafka topics, but the corresponding events in the pulled data do not belong to the same time window (say 1 hour). That will result in invalid predictions due to out-of-sync data.</p>\n\n<h1>Question</h1>\n\n<p>We need to figure out how can we make sure that the data from all three sources are pushed in-order so that when an inference engine requests entries (say the last 100 entries) from multiple kakfa topics, the corresponding entries in each topic belong to the same time window?</p>\n"
    }, {
        "tags": ["streaming", "iot", "rule-engine", "complex-event-processing", "flink-cep"],
        "is_answered": true,
        "view_count": 5663,
        "favorite_count": 6,
        "score": 5,
        "last_activity_date": 1558797037,
        "creation_date": 1495632083,
        "question_id": 44159628,
        "link": "https://stackoverflow.com/questions/44159628/best-approach-to-construct-a-real-time-rule-engine-for-our-streaming-events",
        "title": "Best approach to construct a real-time rule engine for our streaming events",
        "body": "<p>We are at the beginning of building an IoT cloud platform project. There are certain well known portions to achieve complete IoT platform solution. One of them is <strong>real-time rule processing/engine</strong> system which is needed to understand that streaming events are matched with any rules defined dynamically by end users with readable format (SQL or Drools if/when/then etc.)</p>\n\n<p>I am so confused because there are lots of products, projects (Storm, Spark, Flink, Drools, Espertech etc.) in internet so, considering we have 3-person development team (a junior, a mid-senior, a senior), what would it be the best choice ? </p>\n\n<ul>\n<li>Choosing one of the streaming projects such as Apache Flink and learn well ? </li>\n<li>Choosing one of the complete solution (AWS, Azure etc.)</li>\n</ul>\n"
    }, {
        "tags": ["mongodb", "apache-spark", "google-bigquery", "iot", "fluentd"],
        "is_answered": true,
        "view_count": 218,
        "favorite_count": 0,
        "closed_date": 1465452494,
        "score": -1,
        "last_activity_date": 1557441871,
        "creation_date": 1464697006,
        "last_edit_date": 1557441871,
        "question_id": 37545540,
        "link": "https://stackoverflow.com/questions/37545540/iot-streaming-architecture",
        "title": "IoT Streaming Architecture",
        "body": "<p>I just started learning about IoT and data streaming. Apologies if this question seems too obvious or generic.</p>\n\n<p>I am working on a school project, which involves streaming data from hundreds (maybe thousands) of Iot sensors, storing said data on a database, then retrieving that data for display on a web-based UI.</p>\n\n<p>Things to note are:</p>\n\n<ol>\n<li>fault-tolerance and the ability to accept incomplete data entries</li>\n<li>the database has to have the ability to load and query data by stream</li>\n</ol>\n\n<p><br></p>\n\n<p>I've looked around on Google for some ideas on how to build an architecture that can support these requirements. Here's what I have in mind:</p>\n\n<ol>\n<li>Sensor data is collected by <i>FluentD</i> and converted into a stream</li>\n<li><i>Apache Spark</i> manages a cluster of <i>MongoDB</i> servers\n<br>&nbsp;&nbsp;&nbsp;&nbsp; a. the <i>MongoDB</i> servers are connected to the same storage\n<br>&nbsp;&nbsp;&nbsp;&nbsp; b. <i>Spark</i> will handle fault-tolerance and load balancing between <i>MongoDB</i> servers</li>\n<li><i>BigQuery</i> will be used for handling queries from UI/web application.</li>\n</ol>\n\n<p>My current idea of a IoT streaming architecture :\n<a href=\"https://i.stack.imgur.com/gsC0B.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/gsC0B.png\" alt=\"enter image description here\"></a>\n<br>\nThe question now is whether this architecture is feasible, or whether it would work at all. I'm open to any ideas and suggestions.</p>\n\n<p>Thanks in advance!</p>\n"
    }, {
        "tags": ["json", "azure", "parsing", "iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 66,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1557225922,
        "creation_date": 1557223342,
        "question_id": 56020108,
        "link": "https://stackoverflow.com/questions/56020108/where-i-can-parse-iot-data-in-azure-so-i-can-afterwards-save-it-to-sql-db",
        "title": "Where I can parse IoT data in Azure so I can afterwards save it to SQL DB",
        "body": "<p>I'm working on some IoT integrations and I am wondering where in the Azure I can parse my IoT data (JSON data).</p>\n\n<p>My earlier workflow was this; sensor pushes data -> iot hub -> stream analytics jobs -> sql database. Stream analytics job works fine but I have heard that it is not \"right\" way to parse data in Azure. So what is the right and best way to do that. I need to save it to SQL database.</p>\n"
    }, {
        "tags": ["django", "websocket", "django-rest-framework", "iot"],
        "is_answered": false,
        "view_count": 485,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1556883230,
        "creation_date": 1556883230,
        "question_id": 55969015,
        "link": "https://stackoverflow.com/questions/55969015/how-to-build-a-real-time-iot-sever-using-django-rest-api-and-websockets",
        "title": "How to build a real time iot sever using django rest api and websockets?",
        "body": "<p>I'm building a real time iot server using django and I'm fairly new to django. I have used django REST api so far to get data using HTTP requests, by using JSON. Now I want to further expand my project with the use of django websockets. How can I use websockets with REST api to capture real time data from iot? Or is it even possible to use websockets with REST api.</p>\n"
    }, {
        "tags": ["raspberry-pi", "cloud", "iot", "catia", "solidworks"],
        "is_answered": true,
        "view_count": 325,
        "favorite_count": 0,
        "score": 3,
        "last_activity_date": 1556125511,
        "creation_date": 1544301868,
        "last_edit_date": 1545021211,
        "question_id": 53686812,
        "link": "https://stackoverflow.com/questions/53686812/connecting-cad-model-solidworks-autocad-or-catia-with-realtime-measurements-f",
        "title": "Connecting CAD model (Solidworks, AutoCAD or CATIA) with realtime measurements from Raspbery Pi or Arduino Sensor",
        "body": "<p>To present my question I will simplify my example.</p>\n\n<p>I will connect a sprocket on a step motor and measure acceleration with an accelerometer. The data will be captured by using either an Arduino or Raspberry pi sensor setup. The measurements will then be stored in a cloud-based environment or somehow similar and be send to the CAD model (that's the idea).</p>\n\n<p>Basically what I would like to achieve is to:</p>\n\n<ul>\n<li><p>connect the movement of the step motor with the SW/CATIA/AutoCAD model (if the physical sprocket is spinning, so is the one in the CAD model),</p></li>\n<li><p>in case that the measurements identify a problem in the assembly, the critical/weak component would be somehow highlighted inside the CAD model.</p></li>\n</ul>\n\n<p>Has anyone an idea how this could be done or if it is even possible? </p>\n"
    }, {
        "tags": ["iot", "eclipse-ditto", "eclipse-iot", "digital-twin"],
        "is_answered": true,
        "view_count": 113,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1555332553,
        "creation_date": 1554775351,
        "last_edit_date": 1555332553,
        "question_id": 55584080,
        "link": "https://stackoverflow.com/questions/55584080/whether-the-live-mqtt-messages-will-be-stored-in-mongodb-within-ditto-solution",
        "title": "Whether the live mqtt messages will be stored in mongoDB within Ditto solution? How can I check if these messages stored?",
        "body": "<p>From Ditto architecture, MongoDB should store all MQTT messages.  Not sure the live messages?  When I go into MongoDB, query collection, can't find anything although I find \"things\" db has certain size.  Is there anyway to check if mqtt messages have been stored in mongoDB?</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["sql", "azure", "timestamp", "iot", "azure-stream-analytics"],
        "is_answered": true,
        "view_count": 768,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1555005198,
        "creation_date": 1554905574,
        "last_edit_date": 1554967154,
        "question_id": 55614746,
        "link": "https://stackoverflow.com/questions/55614746/azure-stream-analytics-timestamp-by-in-query-doesnt-works-on-job-but-works-fi",
        "title": "Azure Stream Analytics &#39;TimeStamp By&#39; in query doesn&#39;t works on job but works fine on test",
        "body": "<p>I'm working on a IoT project. I've a Raspberry pi which send data to an IoTHub on Azure. That Hub forwards that data to an Azure Stream Analytics Job. In my query I try to aggregate(here an average) all the data in a TumblingWindow of 1 minute but as a Timestamp I use a custom datetime sent in the data. </p>\n\n<p>I've tried many things but nothing seemed to work. the stream job seems to ignore the datetime I provide and just aggregate everything based on the arrival time. \nOnly when using the \"Test\" functionality by uploading a json file does it works.</p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>SELECT\n    DateAdd(minute, -1, system.Timestamp) as TumblingWindowStartTime, \n    system.TimeStamp as TumblingWindowEndTime, \n    event.DeviceId as DeviceId, \n    AVG(event.Temperature) as TemperatureAverage,\n    Count(*) as MeasurementsCount\nINTO\n    [input]\nFROM\n    [output] as event\nTIMESTAMP BY MeasuredOn\nGROUP BY\n    event.DeviceId,\n    TumblingWindow(minute, 1)\n</code></pre>\n\n<p>Type of data I send</p>\n\n<pre><code>[\n   {\n      \"Temperature\":13.426585352712585,\n      \"DeviceId\":\"UlyssesPi\",\n      \"MeasuredOn\":\"2019-04-09T11:20:30.1027311Z\"\n   },\n   {\n      \"Temperature\":16.81523611620778,\n      \"DeviceId\":\"UlyssesPi\",\n      \"MeasuredOn\":\"2019-04-09T11:20:35.2281002Z\"\n   },\n   ...\n]\n</code></pre>\n\n<p>What I'm expecting is what I get when using the \"Test\" functionality on Azure Stream Analytics :</p>\n\n<pre><code>TUMBLINGWINDOWSTARTTIME || TUMBLINGWINDOWENDTIME || DEVICEID || TEMPERATUREAVERAGE || MEASUREMENTSCOUNT\n\"2019-04-09T11:20:0...     \"2019-04-09T11:21:0...   UlyssesPi   14.674093214798454    6\n\"2019-04-09T11:21:0...     \"2019-04-09T11:22:0...   UlyssesPi   18.612186615873217    12\n\"2019-04-09T11:22:0...     \"2019-04-09T11:23:0...   UlyssesPi   12.799415359568199    12\n</code></pre>\n\n<p>but instead I get the following:</p>\n\n<pre><code>TUMBLINGWINDOWSTARTTIME || TUMBLINGWINDOWENDTIME || DEVICEID || TEMPERATUREAVERAGE || MEASUREMENTSCOUNT\n\"2019-04-09 11:22:0...     \"2019-04-09 11:23:0...   UlyssesPi   15,4994594331363      30\n</code></pre>\n\n<p>So how come the results aren't the same? Is it something I did wrong? What can I do?</p>\n\n<p>Thanks for your help.</p>\n"
    }, {
        "tags": ["iot", "stm32", "clock", "zigbee", "real-time-clock"],
        "is_answered": true,
        "view_count": 55,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1554922963,
        "creation_date": 1554918272,
        "question_id": 55618524,
        "link": "https://stackoverflow.com/questions/55618524/is-there-an-autonimus-realtime-clock-wilth-month-loss-less-than-10milliseconds",
        "title": "Is there an autonimus realtime clock wilth month loss less than 10milliseconds?",
        "body": "<p>Looking for a realtime clock for IoT project. Need a millisecond resolution for my app protocol and its loss is critical. So I wonder if there is an  autonimus realtime clock (with a battery) that will loose less than 10ms per month and work for a year?</p>\n"
    }, {
        "tags": ["java", "python", "raspberry-pi", "iot"],
        "is_answered": false,
        "view_count": 31,
        "favorite_count": 0,
        "closed_date": 1554743038,
        "score": 0,
        "last_activity_date": 1554743221,
        "creation_date": 1554741798,
        "question_id": 55578062,
        "link": "https://stackoverflow.com/questions/55578062/can-we-introduce-some-java-or-any-other-language-wrapper-on-top-of-python-code-i",
        "title": "Can we introduce some java or any other language wrapper on top of python code in order to conceal/encrypt our python code from being re-engineered?",
        "body": "<p>I am currently, working on deep learning algorithm with an objective of devising optimum method for edge devices where trade-off between the accuracy &amp; speed will be minimum. For this reason, I want to hide my code for saving my work being concluded in span of last 6 months from getting public in few minutes. Any other way or hint will be appreciated for doing same.</p>\n"
    }, {
        "tags": ["database", "iot", "grafana", "influxdb", "sensors"],
        "is_answered": true,
        "view_count": 824,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1554721150,
        "creation_date": 1554673321,
        "last_edit_date": 1554721150,
        "question_id": 55563915,
        "link": "https://stackoverflow.com/questions/55563915/influxdb-how-to-backfill-measurement-tags-that-were-previously-not-measured-if",
        "title": "InfluxDB: How to backfill measurement tags that were previously not measured (if possible at all)?",
        "body": "<p>I'm started logging data from my Smart Meter using Node-RED about a month ago, it looked like this json data (the payload is the important bit):</p>\n\n<pre><code>{\n  \"topic\":\"stat/smartmeter/all\",\n  \"payload\":\"{\n    \\\"kwh_low_tarrif\\\":866.696,\n    \\\"kwh_high_tarrif\\\":902.156,\n    \\\"current_tarrif\\\":1,\n    \\\"current_watt_draw\\\":485,\n    \\\"gas_timestamp\\\":1554675307000,\n    \\\"gas_total\\\":326.509,\n    \\\"kwh_combined\\\":1768.852\n  }\",\n  \"qos\":0,\n  \"retain\":false,\n  \"_topic\":\"stat/smartmeter/all\",\n  \"_msgid\":\"db4ebc0.72b9a48\"\n}\n</code></pre>\n\n<p>The problem with this data is that I did my electrical &amp; gas cost calculations in my Grafana dashboard:</p>\n\n<p><a href=\"https://i.stack.imgur.com/NQGlA.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NQGlA.png\" alt=\"grafana smartmeter dashboard\"></a></p>\n\n<p>I hardcoded the costs into the dashboard using a math function in the InfluxDB data selection:</p>\n\n<p><a href=\"https://i.stack.imgur.com/aLYcb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/aLYcb.png\" alt=\"grafana data query for InfluxDB (the math statement is the important bit)\"></a></p>\n\n<p>There you can see I used the value (or price rather) of 0.230662 euro's per kWh of used electricity. Now silly me never thought about wanting to be able to run calculations over multiple years where this price would fluctuate, so once I discovered my electricity provider's public API endpoint where I could read out the prices for my specific plan, I added it to the measurements, so now the json-data looks like this:</p>\n\n<pre><code>{\n  \"topic\":\"stat/smartmeter/all\",\n  \"payload\":\"{\n    \\\"kwh_low_tarrif\\\":866.696,\n    \\\"kwh_high_tarrif\\\":902.156,\n    \\\"kwh_low_price\\\":0.230662,\n    \\\"kwh_high_price\\\":0.230662,\n    \\\"current_tarrif\\\":1,\n    \\\"current_watt_draw\\\":485,\n    \\\"current_kwh_price\\\":0.230662,\n    \\\"gas_timestamp\\\":1554675307000,\n    \\\"gas_total\\\":326.509,\n    \\\"gas_price\\\":0.804565,\n    \\\"kwh_combined\\\":1768.852\n  }\",\n  \"qos\":0,\n  \"retain\":false,\n  \"_topic\":\"stat/smartmeter/all\",\n  \"_msgid\":\"db4ebc0.72b9a48\"\n}\n</code></pre>\n\n<p>The only problem (and my main questions with it) now is that:</p>\n\n<p><strong>1) How do I write a query that uses this value in the price calculation?</strong> The query I'm using now (from the screenshot above) is:</p>\n\n<pre><code>SELECT distinct(\"kwh_combined\")  * 0.230662 FROM \"smartmeter\" WHERE $timeFilter GROUP BY time($__interval) fill(linear)\n</code></pre>\n\n<p><strong>2) How do I backfill data?</strong> (write those electric &amp; gas prices into the database from the beginning of my logging, adding it to the measurements I took back then)</p>\n\n<p>I would much rather have the values I previously had hardcoded in my panel set into the measurements I already took instead of having to write an exception for when the measurements aren't present or 'null'.. I mean, the data itself is as static as can be since the prices haven't changed, so it can't be THAT hard, can it? Even if it need to rebuild the data, can I just re'insert it into a new data collection and ADD the fields myself?</p>\n\n<p>Please tell me this is doable for InfluxDB...</p>\n\n<p>I mean, in MySQL it would a simple ALTER TABLE statement with perhaps a simple insert on the records which had null values.</p>\n\n<p>.. or is it unreasonable for me to ask a time-series logging system to be able to alter it's data structure of already logged data and am I asking too much of InfluxDB?</p>\n"
    }, {
        "tags": ["postgresql", "database-design", "architecture", "iot"],
        "is_answered": false,
        "view_count": 40,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1554557251,
        "creation_date": 1554488478,
        "question_id": 55541316,
        "link": "https://stackoverflow.com/questions/55541316/database-interaction-in-an-iot-network",
        "title": "Database interaction in an IOT network",
        "body": "<p>Suppose we have several (100) nodes in an IOT network. Each node has limited resources. There is a postgresql database server running in one of these nodes. Now every node has several (4-5) processes which need to interact with this server to perform some insert and select queries. Each query response has to be as fast as possible for the process to work as it should. Now i think of some ways to do this are :</p>\n\n<ol>\n<li>Each process in a node makes one database client and performs queries.     </li>\n<li>All processes in a node send their queries to a destination in localhost itself from where the queries are performed through an optimum number of database clients. This way we have some sort of control over the number of database clients like  optimisation of queries getting performed through a priority queue implementation or  performing queries in separate thread/process through a separate  database client in each thread/process. In this case somewhat we have the control over the optimisation of number of clients,number of threads/processes , priority of in what order queries must be executed.</li>\n<li>Each node sends all queries through some network protocol directly to the database server which then uses a limited number of database clients performing queries now in its own localhost database and then returning the response to each node through same channel. This way it increases the latency but keeps number of clients minimum. Plus we can also implement some optimisation here running each client in different process/thread etc. Database interaction can be faster in this case since number of clients can be kept minimum, it is running in localhost machine itself  but it adds some overhead to transfer the query response data back to the node's process.     </li>\n</ol>\n\n<p>In order to keep the resource usage as minimum as possible in every node and queries response as fast as possible , what is the best strategy to solve this problem ? </p>\n"
    }, {
        "tags": ["data-visualization", "iot", "saas", "azure-maps"],
        "is_answered": false,
        "view_count": 94,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1554308502,
        "creation_date": 1544534730,
        "last_edit_date": 1554308502,
        "question_id": 53725111,
        "link": "https://stackoverflow.com/questions/53725111/is-there-any-service-of-map-and-gps-logger-for-iot-devices-as-freelike-azure-m",
        "title": "Is there any service of Map and GPS Logger for IoT Devices as free?(like Azure Maps in Azure IoT Central)",
        "body": "<p>I want to visualization latitude and longitude data taken from IoT Devices.\nBut I couldn't find service is able to plot GPS log data.\nFor example, AT&amp;T M2X can visualization and log data that just sensor data (like humidity, temperature, and so on) but it can't visualize map from data. </p>\n\n<p>At last found Azure Maps, but it needs to register the credit card.\nIf needed to pay for the amount of user data, but I want to start map visualization with no pay option setting first.</p>\n\n<p>I desire service keeps below three points, 1. no need pay option setting first, 2. it can post data from HTTP protocol(GET/POST), 3. any kinds of map type is ok (google map, BingMap, OpenStreetMap, and so on)</p>\n\n<p>I'm sorry that my English is so bad. \nI look forward to your reply. \nthanks.</p>\n"
    }, {
        "tags": ["rest", "filter", "cassandra", "iot"],
        "is_answered": false,
        "view_count": 215,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1553692586,
        "creation_date": 1553106870,
        "question_id": 55267918,
        "link": "https://stackoverflow.com/questions/55267918/data-filtering-on-temperature-humidity-and-air-quality-raw-sensor-data",
        "title": "data filtering on temperature , humidity and air quality raw sensor data",
        "body": "<p>I have several sensors measuring temperature, humidity , and air quality.\nThe data is collected through an IoT platform and stored to a Cassandra Cluster. For the data distribution to all other applications from Cassandra Cluster, I have created a Rest Api in java.</p>\n\n<p>The data collection scripts are written in C and python . While the Rest Api in java.</p>\n\n<p>My question is the following.</p>\n\n<p>I want to filter my raw sensor data in order to exclude values such as high &amp; low temperatures etc. in order to have as low as possible fault ratio.  have read about the t Kalman Filtering , but I don't want it to happen in real time , I prefer to filter my data straight on the Cassandra nodes. </p>\n\n<p>My best guess right now is to create a service using for ex. java , and filtering data with a time gap excluding the undesirable values.</p>\n\n<p>For example a service that will triggered once a day and it will exclude all the \"bad\" values from the time of last activation since the last record.</p>\n\n<p>Is there any suggestion for an approach like that?\nOr does anyone have a better suggestion ?\nOr even better some publication that can guide me through the process .</p>\n\n<p>Thanks in advance.</p>\n"
    }, {
        "tags": ["database", "blockchain", "iot", "smartcontracts"],
        "is_answered": false,
        "view_count": 53,
        "favorite_count": 0,
        "score": -2,
        "last_activity_date": 1553164835,
        "creation_date": 1552674169,
        "question_id": 55188676,
        "link": "https://stackoverflow.com/questions/55188676/blockchain-usage-in-smart-home",
        "title": "Blockchain usage in smart home",
        "body": "<p>I am currently gathering information for my thesis about smart home implementaton with a use of blockchain. The blockchain usage is a requirement from my thesis supervisor and I have huge problem with finding out how blockchain technology may be useful in home automation.</p>\n\n<p>What I have already considered is that there are two types of blockchain which I can use: private and public.</p>\n\n<p>The public blockchain won't be useful at all, because of long time to achieve consensus and every transaction costs money (fee for miners).</p>\n\n<p>I also don't see any adventage of private blockchain over regular database in such application. There are two reasons:</p>\n\n<p>-I won't be able to store blockchain on every smart home device, because they all have limited space. So If I need to store blockchain in some centralised way, I think it looses it's immutability adventage.</p>\n\n<p>-The public key cryptography is a very nice thing, but I can archieve that also in a regular database, so I don't see the need to implement blockchain for that.</p>\n\n<p>So am I not seeing something? How use of blockchain may be helpful is such a small project?</p>\n\n<p>Thanks in advance for any advice! :)</p>\n"
    }, {
        "tags": ["azure", "azure-storage", "iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 510,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1553151735,
        "creation_date": 1553149838,
        "question_id": 55274906,
        "link": "https://stackoverflow.com/questions/55274906/iot-hub-maximum-uploaded-file-size",
        "title": "IoT Hub - maximum uploaded file size",
        "body": "<p>What is the maximum size of a file uploaded through IoT Hub? Is it 256KB like the maximum message size (according to <a href=\"https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/iot-hub-limits.md\" rel=\"nofollow noreferrer\">documentation</a>)</p>\n"
    }, {
        "tags": ["database", "architecture", "iot", "data-synchronization"],
        "is_answered": false,
        "view_count": 36,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1552690333,
        "creation_date": 1552541844,
        "last_edit_date": 1552643800,
        "question_id": 55155609,
        "link": "https://stackoverflow.com/questions/55155609/how-to-transfer-rules-and-configuration-to-edge-devices",
        "title": "How to transfer rules and configuration to edge devices?",
        "body": "<p>In our application we have a server which contains entities along with their relations and processing rules stored in DB. To that server there will be n no.of clients like raspberry pi , gateways, android apps are connected. </p>\n\n<p>I want to push configuration &amp; processing rules to those clients, so when they read some data they can process on their own. This is to make the edge devices self sustainable, avoid outages when server/network is down. </p>\n\n<p>How to push/pull the configuration. I don't want to maintain DBs at client and configure replication. But the problem is maintenance and patching of DBs for those no.of client will be tough.</p>\n\n<p>So any other better alternative.?</p>\n\n<p>At the same time I have to push logs to upstream (server). </p>\n\n<p>Thanks in advance.</p>\n"
    }, {
        "tags": ["c#", "azure", "iot", "azure-iot-hub"],
        "is_answered": false,
        "view_count": 439,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1551886965,
        "creation_date": 1545992874,
        "question_id": 53957039,
        "link": "https://stackoverflow.com/questions/53957039/azure-iot-hub-reducing-data-down-upload",
        "title": "Azure IoT hub - reducing data down/upload",
        "body": "<p>I am working with Azure IoT hub and have a test environment running at the moment. However we are using 4g data subscriptions (Mobile) to upload measurements from our devices and I have done a lot to reduce the data that we gather and upload every 10 minutes. However when measuring the data consumption I still see quite a big overhead. My data take up around 300 bytes compressed, but after measuring data with NetBalancer I can see that after 1 day my application have sent 1.2 MB and received 2.3 MB. I am using the MQTT protocol as that should have the smallest footprint.</p>\n\n<p>I can't seem to find any best practice or similar to reduce the data sent on the wire using IoT hub. Any help is highly appreciated! :) </p>\n"
    }, {
        "tags": ["hadoop", "iot"],
        "is_answered": false,
        "view_count": 56,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1551672242,
        "creation_date": 1551594010,
        "last_edit_date": 1551672242,
        "question_id": 54966132,
        "link": "https://stackoverflow.com/questions/54966132/using-hadoop-in-iot-domain-smart-street-lighting-systems",
        "title": "Using Hadoop in IoT domain - Smart Street Lighting Systems",
        "body": "<p>I think I have come up with a project idea and I need some help in evaluating it.</p>\n\n<p>Title: <strong>Smart Grid System with energy saving function</strong></p>\n\n<p>Time series data refers to the data recorded over a period of time and can be generated by an equipment and collected by sensors in IOT.\nExample:</p>\n\n<pre><code>daily air temperature or monthly precipitation in a specific location.\n</code></pre>\n\n<p><a href=\"https://www.researchgate.net/figure/fig2_261479865\" rel=\"nofollow noreferrer\">Click here to see a SMART GRID SYSTEM</a></p>\n\n<p>In case of Smart Grid System shown above,the time series data can be generated by the sensor unit and that data can be collected by the sensor in the nearby smart street light.</p>\n\n<p>For instance, when a person comes near a sensor unit and not near the street light, it will trigger the sensor unit which in turn will make the sensor unit to generate time series data which will be instantaneously be delivered to the sensors present in street light and street lights will turn on immediately and hence won\u2019t need to turn on when the person comes right in front of it, which is actually meaningful.</p>\n\n<p>Here we can take the time series data to be the distance between the object or person approaching and the sensor in the street light let us say \u2018d\u2019.\nThe methodology is whenever the distance \u2018d\u2019 is less than the cutoff distance \u2018x\u2019 ie the desired distance within which we want the lights to get turned on,the sensor will switch the light on and whenever the distance becomes(because we have been observing the distance over a period of time as it is a time series data) greater than \u2018x\u2019 sensors should turn off the light.</p>\n\n<p>There are basically three ways to process the time series data:</p>\n\n<ol>\n<li>Use of Relational Database</li>\n<li>Use of Real-time Database</li>\n<li>Use of Hadoop(HBase)</li>\n</ol>\n\n<p>We generally go for the third approach to process the time series data because the first two approaches have many disadvantages.</p>\n\n<p>Now the issue is that, how am I to do the simulation for this? I have read about Hadoop and its use in building IoT Architectures but how do I incorporate the use of Hadoop to this idea? </p>\n\n<p>What else do I need to be able to complete this project?</p>\n\n<p>Current Skill set: Proficient in Python/Opencv, Basic DL. \nAny help in improving my question and how can I take this project further is appreciated! </p>\n"
    }, {
        "tags": ["javascript", "django", "leaflet", "iot"],
        "is_answered": false,
        "view_count": 239,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1550801878,
        "creation_date": 1550775985,
        "question_id": 54814457,
        "link": "https://stackoverflow.com/questions/54814457/capturing-an-id-hidden-inside-leaflet-popup-on-click-requires-two-clicks-to-upda",
        "title": "Capturing an ID hidden inside Leaflet popup on click requires two clicks to update [Javascript]",
        "body": "<p>I have a really annoying issue with javascript in my Django project. Currently building a webbapp which reads data from sensors placed in manholes for water-temperature measurements. We display these sensors as markers on a Leafletmap with the pipe-system between each manhole.\nI'm currently storing the sensor-id as a hidden variable in each manhole and then grabbing these to build a D3 graph displaying the temperature data for the specific manhole that has been clicked.</p>\n\n<pre><code>onEachFeature: (feature, layer) =&gt; {\n\n                    for (let i = 0; i &lt; place.length; i++) {\n\n                        if (place[i].fields.pnamn === feature.properties.pnamn) {\n                            sensorid = place[i].fields.eui;\n                        }\n                    }\n\n                    var popupText = \"&lt;strong&gt;\" + feature.properties.pnamn + \"&lt;p id='popupText' style='display:none'&gt;\" + sensorid + \"&lt;/p&gt;\" + \"&lt;/strong&gt;\";\n                    layer.bindPopup(popupText);\n                },\n\n[......] }).on('click', onClick).on('popupclose', startZoomer).addTo(map);\n</code></pre>\n\n<p>The id in question is the sensorid in the p-element. It works as it should, except for the extremely annoying fact that you can just click on a new manhole to update the graph without clicking twice on the new one or by clicking anywhere on the map.</p>\n\n<p>I capture the sensorid in the function below and this is where I believe the problem is hiding. Can't really wrap my head around why this is happening and would appreciate any help at this point in time! </p>\n\n<pre><code>                function onClick() {\n                let id = document.getElementById(\"popupText\").innerText;\n\n                urlen = urlen.replace(/([A-Z])\\w+/, id);\n                console.log(id);\n                console.log(urlen);\n\n                var x = document.getElementById(\"chart-area\");\n                if (x.style.display === \"none\") {\n                    x.style.display = \"block\";\n                } else {\n                    x.style.display = \"block\";\n                }\n\n                map.scrollWheelZoom.disable();\n\n\n                update();\n\n            }\n</code></pre>\n\n<p>Where the building of the new id for updating the graph is happening is the first 4 rows under the function initialization. The rest is for locking the map for mousewheel scroll when a popup is open so that user can scroll between the graph and map as they are stacked on top of eachother.</p>\n\n<p>It's as I said extremely annoying for me and unacceptable when the system's put to use to have to click twice, and if you don't know this happens it can skew your view as it does update if you just click between manholes but you get the manhole you clicked before the current.</p>\n\n<p>Please help me.</p>\n"
    }, {
        "tags": ["javascript", "performance", "rest", "ibm-cloud", "iot"],
        "is_answered": true,
        "view_count": 154,
        "favorite_count": 0,
        "score": -2,
        "last_activity_date": 1550056989,
        "creation_date": 1536609340,
        "last_edit_date": 1550056989,
        "question_id": 52264607,
        "link": "https://stackoverflow.com/questions/52264607/iot-how-to-calculate-time-complexity-of-an-api-in-cloud-iot-platform",
        "title": "IOT: How to calculate time complexity of an API in cloud IOT Platform",
        "body": "<p>I would like to determine performance metrics for the API which interacts with IoT Devices hosted in the cloud,  Example I have two option Amazon cloud and IBM cloud for IOT how I determine which one is the best.</p>\n\n<ol>\n<li>I have to measure the wait time of REST API which is available in an\nIoT platform. Is there any way I can follow to measure the wait time or performance aspects of the API? </li>\n<li>Do we have any standard performance benchmarks or metrics which should be evaluated for APIs?</li>\n</ol>\n"
    }, {
        "tags": ["firebase", "firebase-realtime-database", "iot", "esp8266", "arduino-esp8266"],
        "is_answered": true,
        "view_count": 4210,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1549453441,
        "creation_date": 1546010717,
        "last_edit_date": 1546011646,
        "question_id": 53960711,
        "link": "https://stackoverflow.com/questions/53960711/esp8266-can-not-connect-with-firebase-errorsetting-number-failed",
        "title": "Esp8266 can not connect with firebase - error[setting /number failed]",
        "body": "<p>I would like to connect via the esp8266 card to my firebase database. I tried to use the demo code that provides the firebase library for Arduino, but it does not work. I correctly enter my wi-fi data and I can establish a connection, but when I try to connect to the database the Arduino console I get the following error: setting/number failed:</p>\n\n<p><a href=\"https://i.stack.imgur.com/iDex1.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/iDex1.png\" alt=\"enter image description here\"></a>\nCode:</p>\n\n<pre><code>#include &lt;ESP8266WiFi.h&gt;\n#include &lt;FirebaseArduino.h&gt;\n\n// Set these to run example.\n#define FIREBASE_HOST \"-.firebaseio.com\"\n#define FIREBASE_AUTH \"----\"\n#define WIFI_SSID \"----\"\n#define WIFI_PASSWORD \"----\"\n\nvoid setup() {\n  Serial.begin(9600);\n\n  // connect to wifi.\n  WiFi.begin(WIFI_SSID, WIFI_PASSWORD);\n  Serial.print(\"connecting\");\n  while (WiFi.status() != WL_CONNECTED) {\n    Serial.print(\".\");\n    delay(500);\n  }\n  Serial.println();\n  Serial.print(\"connected: \");\n  Serial.println(WiFi.localIP());\n\n  Firebase.begin(FIREBASE_HOST, FIREBASE_AUTH);\n}\n\nint n = 0;\n\nvoid loop() {\n  // set value\n  Firebase.setFloat(\"number\", 42.0);\n  // handle error\n  if (Firebase.failed()) {\n      Serial.print(\"setting /number failed:\");\n      Serial.println(Firebase.error());  \n      return;\n  }\n  delay(1000);\n\n  // update value\n  Firebase.setFloat(\"number\", 43.0);\n  // handle error\n  if (Firebase.failed()) {\n      Serial.print(\"setting /number failed:\");\n      Serial.println(Firebase.error());  \n      return;\n  }\n  delay(1000);\n\n  // get value \n  Serial.print(\"number: \");\n  Serial.println(Firebase.getFloat(\"number\"));\n  delay(1000);\n\n  // remove value\n  Firebase.remove(\"number\");\n  delay(1000);\n\n  // set string value\n  Firebase.setString(\"message\", \"hello world\");\n  // handle error\n  if (Firebase.failed()) {\n      Serial.print(\"setting /message failed:\");\n      Serial.println(Firebase.error());  \n      return;\n  }\n  delay(1000);\n\n  // set bool value\n  Firebase.setBool(\"truth\", false);\n  // handle error\n  if (Firebase.failed()) {\n      Serial.print(\"setting /truth failed:\");\n      Serial.println(Firebase.error());  \n      return;\n  }\n  delay(1000);\n\n  // append a new value to /logs\n  String name = Firebase.pushInt(\"logs\", n++);\n  // handle error\n  if (Firebase.failed()) {\n      Serial.print(\"pushing /logs failed:\");\n      Serial.println(Firebase.error());  \n      return;\n  }\n  Serial.print(\"pushed: /logs/\");\n  Serial.println(name);\n  delay(1000);\n}\n</code></pre>\n\n<p>I would like to establish a connection with my firebase database.</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 686,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1548847569,
        "creation_date": 1486978368,
        "question_id": 42200771,
        "link": "https://stackoverflow.com/questions/42200771/minimum-requirements-azure-iot-hub",
        "title": "Minimum requirements Azure IoT Hub",
        "body": "<p>Does someone know the bare minimum required for a device to connect to the Azure IoT Hub and to send and receive messages (in terms of memory usage, processing power etc.). Or is this really case-specific?</p>\n\n<p>I couldn't find a conclusive answer on the web. Does someone have experience with this?</p>\n\n<p>Many thanks!</p>\n"
    }, {
        "tags": ["amazon-web-services", "azure", "google-cloud-platform", "iot", "pii"],
        "is_answered": false,
        "view_count": 193,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1548745583,
        "creation_date": 1548712116,
        "last_edit_date": 1548745583,
        "question_id": 54410762,
        "link": "https://stackoverflow.com/questions/54410762/architecture-of-iot-with-pii-protection-regulation-e-g-gdpr",
        "title": "Architecture of IoT with PII Protection Regulation (e.g. GDPR)",
        "body": "<p>I have a hard time searching for a reference architecture for IoT that specifies a way of reaching PII Protection regulation compliance (e.g. GDPR). There are lots of articles with general suggestions like <a href=\"https://www.i-scoop.eu/internet-of-things-guide/iot-regulation/\" rel=\"nofollow noreferrer\">this</a> or <a href=\"https://internetofbusiness.com/gdpr-10-steps-to-prepare-iot-projects-expert-panel/\" rel=\"nofollow noreferrer\">this</a>, but what I am really seeking is a specific architecture design.</p>\n\n<p>Say, taking Google Cloud Platform as an example, there is the following reference architecture (<a href=\"https://cloud.google.com/solutions/iot/\" rel=\"nofollow noreferrer\">link</a>): \n<a href=\"https://i.stack.imgur.com/YBzeU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/YBzeU.png\" alt=\"enter image description here\"></a><br>\nIt does not tell anything about the PII protection, but I can think about implementing or taking a COTS tokenization service (<a href=\"https://www.protegrity.com/data-security-gateway/\" rel=\"nofollow noreferrer\">a security gateway</a>), which then shall be deployed to the Edge (which is located in the same country, where a personal information has to be stored under the regulation rules). </p>\n\n<ol>\n<li>Does this mean that IoT always has to contain some on-premise edge (playing a single entry point role for all my devices and hubs/gateways) with such the tokenization service installed (of course, for those countries that are under the regulation) if I want to have a storage layer on a public cloud only?</li>\n<li>What if I need to store all telemetry that I collect from my smart home, and there is a camera that captures people coming to my door, or even a microphone that has accidentally caught my friend's words like \"I am John Doe, my social security number is , and I will not be at my house, which is located at  GPS location for the next 2 days\" and my system need to store them as the microphone's state? Does it mean in 'GDPR' locations I need to use on-premise storage instead of a cloud one, to be full compliant, just in case?</li>\n<li>Does it all mean that if I need to design a multi-tenant multi-national business of smart homes, that includes countries under the PII Protection regulation, I must always consider a hybrid solution for data storage (on-premise + cloud) to mitigate risks like described in the previous item? </li>\n</ol>\n"
    }, {
        "tags": ["iot", "m2m"],
        "is_answered": false,
        "view_count": 108,
        "favorite_count": 1,
        "score": 4,
        "last_activity_date": 1547368449,
        "creation_date": 1496941487,
        "last_edit_date": 1496948808,
        "question_id": 44441754,
        "link": "https://stackoverflow.com/questions/44441754/how-to-select-iot-m2m-sim-card-service",
        "title": "How to select IoT M2M SIM card service",
        "body": "<p>I've searched and nothing came up on here. I've developed an Remote Monitor for small wind turbines in Peru, using and Arduino Uno, Raspberry Pi and USB Modem. </p>\n\n<p>I want to try and reduce our monthly data costs (currently have a Claro peruvian SIM card in the USB modem, costing around $9 a month)</p>\n\n<p>Has anyone got any experience with the numerous M2M (Machine 2 Machine) SIM companies that are available out there.</p>\n\n<p>Hologram so far seem to be the clearest in pricing structure, numerous companies want to quote you.</p>\n\n<p><a href=\"https://hologram.io/pricing/\" rel=\"nofollow noreferrer\">https://hologram.io/pricing/</a></p>\n\n<p>Our projects will be sending around 14 bits of data every 10 minutes. Any current users of other M2M sim cards your opinions and feedback would be very much appreciated.</p>\n"
    }, {
        "tags": ["amazon-web-services", "mqtt", "iot"],
        "is_answered": false,
        "view_count": 184,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1547187046,
        "creation_date": 1547186219,
        "last_edit_date": 1547187046,
        "question_id": 54141092,
        "link": "https://stackoverflow.com/questions/54141092/how-to-distribute-reads-of-an-mqtt-topic-from-aws-iot-over-multiple-consumers",
        "title": "How to distribute reads of an MQTT topic from AWS Iot over multiple consumers?",
        "body": "<p>I'm using AWS IoT to get device logs from multiple moving devices. Currently, all devices can publish their status to the AWS IoT Core using MQTT. Now I'd like to create an application service from outside AWS (servers are in my company) that will subscribe and consume device status from AWS IoT. I'm facing the issue like that, the app backend can get data from AWS IoT. However, it cannot be distributed or load balanced. Just the last connected consumer can get the data. How can I distribute/scale the reads from AWS IoT or any alternative/suggested solutions?\nThanks in advance!</p>\n"
    }, {
        "tags": ["events", "stream", "time-series", "mqtt", "iot"],
        "is_answered": true,
        "view_count": 494,
        "favorite_count": 1,
        "score": 3,
        "last_activity_date": 1546616938,
        "creation_date": 1481936398,
        "last_edit_date": 1546616938,
        "question_id": 41194394,
        "link": "https://stackoverflow.com/questions/41194394/best-practice-for-iot-stream-data-processing",
        "title": "Best practice for IoT stream data processing",
        "body": "<p>I assume that there are hundreds and thousands IoT devices that publish the data to the (broker)MQTT cluster via the MQTT protocol, behind the broker i have the data processing module which subscribe the data from the broker and maintain a status table for all these devices. The number of the devices is still rising, therefor I have to scale out the broker cluster and data processing module accordingly, for the MQTT broker such as Kafka/Rabbit MQ/Hive MQ can be scaled out very easily, but for the data processing module I'm not quite sure whether there is any best practice, or any framework/architecture can achieve this very easily:</p>\n\n<p>I assume I have to create many daemon processes with hundreds and thousands threads to listen on the MQTT broker, the question is how to scale out these services dynamically?</p>\n\n<p>Thanks.</p>\n"
    }, {
        "tags": ["amazon-web-services", "architecture", "monitoring", "iot"],
        "is_answered": true,
        "view_count": 39,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1545614948,
        "creation_date": 1545595997,
        "question_id": 53906853,
        "link": "https://stackoverflow.com/questions/53906853/aws-architecture-for-simple-monitoring-application",
        "title": "AWS Architecture for Simple Monitoring Application",
        "body": "<p>I am new to AWS and trying to figure out what services to use for the following purpose:\n1) read open source data on wave, wind, currents from a govt website (ndbc) every 15mins\n2) Apply transformation to this data to convert to a target variable (busing a trained regression model)\n3) Plot the target variable time history and show it as a dashboard that can be monitored by the user</p>\n\n<p>Can someone suggest what services would need to be used and what the basic architecture would look like?</p>\n\n<p>Thanks,\nSJ</p>\n"
    }, {
        "tags": ["analytics", "iot", "kaa"],
        "is_answered": true,
        "view_count": 257,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1545148247,
        "creation_date": 1531920489,
        "last_edit_date": 1536316898,
        "question_id": 51403271,
        "link": "https://stackoverflow.com/questions/51403271/can-kaa-iot-platform-provides-predictive-digital-twin",
        "title": "Can Kaa IoT platform provides predictive digital twin?",
        "body": "<p>I have been studying about kaa iot platform from couple of days. I have came across that it provides implementation of digital twin but i no where found that kaa only registers the devices data as digital twins or can produce statistical analysis for preventive maintenance.</p>\n\n<p>Our application needs to perform predictive/statistical analysis on device data collected as digital twins to identify any failure cases.</p>\n"
    }, {
        "tags": ["python", "python-3.x", "iot"],
        "is_answered": false,
        "view_count": 75,
        "favorite_count": 1,
        "score": 1,
        "last_activity_date": 1544695746,
        "creation_date": 1544695527,
        "last_edit_date": 1544695746,
        "question_id": 53759340,
        "link": "https://stackoverflow.com/questions/53759340/correct-way-to-implement-infinte-loops-in-python3",
        "title": "Correct way to implement infinte loops in Python3",
        "body": "<p>I am new to the python coding and is still learning, so most probably this will be a stupid question for many. But what I am looking for is logic, not code.</p>\n\n<p>My Scenario:\nI have an IoT sensor, that will send me data over serial as JSON files on regular intervals. The file will always be the same name. Now I am writing a python script to read this JSON file in an infinite loop. \nThe idea is that if there is no data in the JSON, then the loop should just break and send me a message. Otherwise, just parse the dictionary.</p>\n\n<p>I have written it like this with a try and except.</p>\n\n<pre><code>def readDataInLoop(self, ):\n    while True:            \n\n        try:\n            # Reading json\n            with open('dummyzigbee.json') as f:\n                data = json.load(f)\n\n                # Check Data: If Data is empty, exit loop with error message\n                if not data:\n                    print(\"Error!! No data recieved ...\")\n                    print(\"Conenction lost at {} \".format(time.asctime(time.localtime(time.time()))))                    \n                    break                    \n\n                else:\n                    # If data is recieved then parse the data\n                    self.parseData(data)\n        except:\n            pass\n\n        # Give some rest to processor\n        time.sleep(1)\n</code></pre>\n\n<p>This seems to work but I am looking for an efficient manner. I am not sure if this is a good approach or a bad approach.</p>\n"
    }, {
        "tags": ["database", "amazon-web-services", "architecture", "bigdata", "iot"],
        "is_answered": false,
        "view_count": 116,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1543637650,
        "creation_date": 1543226147,
        "last_edit_date": 1543226495,
        "question_id": 53478569,
        "link": "https://stackoverflow.com/questions/53478569/metrics-collection-and-analysis-architecture",
        "title": "Metrics collection and analysis architecture",
        "body": "<p>We are working on HomeKit-enabled IoT devices. HomeKit is designed for consumer use and does not have the ability to collect metrics (power, temperature, etc.), so we need to implement it separately.</p>\n\n<p>Let's say we have 10 000 devices. They send one collection of metrics every 5 seconds. So each second we need to receive 10000/5=2000 collections. The end-user needs to see graphs of each metric in the specified period of time (1 week, month, year, etc.). So each day the system will receive 172,8 millions of records. Here come a lot of questions.</p>\n\n<p>First of all, there's no need to store all data, as the user needs only graphs of the specified period, so it needs some aggregation. What database solution fits it? I believe no RDMS will handle such amount of data. Then, how to get average data of metrics to present it to the end-user?</p>\n\n<p>AWS has shared time-series data processing architecture:\n<a href=\"https://i.stack.imgur.com/p1zVe.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/p1zVe.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Very simplified I think of it this way:</p>\n\n<ol>\n<li>Devices push data directly to DynamoDB using HTTP API</li>\n<li>Metrics are stored in one table per 24 hours</li>\n<li>At the end of the day some procedure runs on Elastic Map Reduce and\nproduces ready JSON files with data required to show graphs per time\nperiod.</li>\n<li>Old tables are stored in RedShift for further applications.</li>\n</ol>\n\n<p>Has anyone already done something similar before? Maybe there is simpler architecture?</p>\n"
    }, {
        "tags": ["rest", "api", "httprequest", "iot"],
        "is_answered": false,
        "view_count": 15,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1543288907,
        "creation_date": 1543288907,
        "question_id": 53492253,
        "link": "https://stackoverflow.com/questions/53492253/how-to-rate-limit-for-mobility-solution",
        "title": "How to Rate Limit for Mobility Solution?",
        "body": "<p>I feel that, IP based rate limit (API request thersold per second /minute) is not the reliable solution for Mobile IOT projects as many users use the same IP address to connect to API server. Please advise the best approach and Pros &amp; Cons</p>\n"
    }, {
        "tags": ["cassandra", "data-modeling", "iot", "cassandra-3.0"],
        "is_answered": true,
        "view_count": 481,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1542735209,
        "creation_date": 1542232472,
        "question_id": 53309291,
        "link": "https://stackoverflow.com/questions/53309291/cassandra-data-modeling-iot-best-practices",
        "title": "Cassandra Data modeling IoT best practices",
        "body": "<p>I am fairly new to Cassandra and I am trying to understand how to design my tables for IoT sensors.</p>\n\n<p>The idea is to have several devices, each with several sensors attached to it sending data periodically (up to around 200000 values per device per day per sensor)</p>\n\n<p>I'd like to be able to query for the latest value of a sensor for a specific list of sensors and devices in more or less real-time. Also devices do not always send data and may be down for long periods of time.</p>\n\n<p>After a lot of reading I came up with something like this</p>\n\n<pre><code>CREATE TABLE \"sensor_data\" (\n    deviceid TEXT,\n    sensorid TEXT,\n    ts timestamp,\n    value TEXT,\n    PRIMARY KEY ((deviceid, sensorid), ts)\n) WITH CLUSTERING ORDER BY (ts DESC);\n</code></pre>\n\n<p>The idea behind this would be to perform one query per device and sensor such as </p>\n\n<pre><code>Select deviceid, sensorid, ts, value where deviceid = \"device1\" and sensorid = \"temperature\" limit 1\n</code></pre>\n\n<p>And run this for each device and sensor. It's not one query to return it all (Which would be ideal) but seems to be fast enough to run for potentially up to 100 sensors or so (With possibilities for parallelizing the queries) for a few devices.</p>\n\n<p>However from what I have read so far, I understand this would give me a lot of columns for my row and it might be complicated in terms of long term storage and Cassandra limitations.</p>\n\n<p>I am thinking that maybe adding something like the date to the table like so (as seen on some blogs and guides) might be a good idea</p>\n\n<pre><code>CREATE TABLE \"sensor_data\" (\n    deviceid TEXT,\n    sensorid TEXT,\n    date TEXT\n    ts timestamp,\n    value TEXT,\n    PRIMARY KEY ((deviceid, sensorid, date), ts)\n) WITH CLUSTERING ORDER BY (ts DESC);\n</code></pre>\n\n<p>And then query like</p>\n\n<pre><code>Select deviceid, sensorid, date, ts, value where deviceid = \"device1\" and sensorid = \"temperature\" and date = \"2018-11-14\" limit 1\n</code></pre>\n\n<p>Does that even make sense? It feels like it might mitigate storage issues and allow for easier archiving of old data in the future however how do I go about querying for the latest value of a specific sensor and device if that device was down for a day or more? Do I really have to query for 1 day, if nothing is found, query the previous day and so forth (Maybe limit it to only the last few days or so)?</p>\n\n<p>Are there better ways to handle this in Cassandra or am I in the right direction?</p>\n"
    }, {
        "tags": ["java", "tcp", "iot"],
        "is_answered": true,
        "view_count": 355,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1542540005,
        "creation_date": 1542536400,
        "question_id": 53359800,
        "link": "https://stackoverflow.com/questions/53359800/java-tcp-server-for-communicating-with-an-iot-device",
        "title": "Java TCP server for communicating with an IoT device",
        "body": "<p>I have developed an embedded system which sends data to a server as TCP requests. I can't use more-frequent HTTP requests, because of its data overhead. Less package length will result in less energy consumption and less communication expenses. </p>\n\n<p>The server has to listen to a special port, get the data from device and store in a table. </p>\n\n<p>As I explored, Java servlets + Apache Tomcat is a popular solution but in this case should not be used, because a Java servlet is more suitable for HTTP-based connections.</p>\n\n<p>Is there a better solution for this type of communication?</p>\n"
    }, {
        "tags": ["microservices", "iot"],
        "is_answered": true,
        "view_count": 123,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1542387423,
        "creation_date": 1542240079,
        "last_edit_date": 1542387423,
        "question_id": 53310576,
        "link": "https://stackoverflow.com/questions/53310576/transform-an-iot-application-from-monolothic-to-microservices",
        "title": "Transform an IoT application from monolothic to microservices",
        "body": "<p>I have a system composed of 3 sensors (Temperature, humidity, camera) attached to Arduino, 1 cloud, and 1 Mobile phone. I developed a monolithic IoT application that has different tasks needed to be executed in these three different locations (Arduino, cloud Mobile). all these sensors have common tasks which are: data detection, data transferring (executed on Arduino), data saving, data analysis and data notification (on the cloud), data visualization (on Mobile).</p>\n\n<p>The problem here I know that a microservice is independent and it has its database. How to transform this application that I have to a one using microservice architecture? the first idea is representing each task as a microservice.</p>\n\n<p>At the first, I considered each task as a component and I thought to represent each one as a microservice but they are linked. I mean that the output of the previous task is the input of the present one, So I can't make it like this because they aren't independent. Another thing for data collection microservice it should be placed on Arduino and the data should be sent to the cloud to be stored there in the database, so here we have a distant DB. For the data collection, I have the same idea as you since there are different things (sensors) so there will be diff microservices like (temperature data collection, camera data collection...).</p>\n"
    }, {
        "tags": ["python", "django", "raspberry-pi", "iot", "bulk-load"],
        "is_answered": true,
        "view_count": 1003,
        "favorite_count": 1,
        "score": 1,
        "last_activity_date": 1541948328,
        "creation_date": 1541945701,
        "question_id": 53249599,
        "link": "https://stackoverflow.com/questions/53249599/sending-data-to-django-backend-from-raspberrypi-sensor-frequency-bulk-update",
        "title": "Sending data to Django backend from RaspberryPi Sensor (frequency, bulk-update, robustness)",
        "body": "<p>I\u2019m currently working on a Raspberry Pi/Django project slightly more complex that i\u2019m used to. (i either do local raspberry pi projects, or simple Django websites; never the two combined!)</p>\n\n<p>The idea is two have two Raspberry Pi\u2019s collecting information running a local Python script, that would each take input from one HDMI feed (i\u2019ve got all that part figured out - I THINK) using image processing. Now i want these two Raspberry Pi\u2019s (that don\u2019t talk to each other) to connect to a backend server that would combine, store (and process) the information gathered by my two Pis</p>\n\n<p>I\u2019m expecting each Pi to be working on one frame per second, comparing it to the frame a second earlier (only a few different things he is looking out for) isolate any new event, and send it to the server. I\u2019m therefore expecting no more than a dozen binary timestamped data points per second.</p>\n\n<p>Now what is the smart way to do it here ? </p>\n\n<ul>\n<li>Do i make contact to the backend every second? Every 10 seconds?</li>\n<li>How do i make these bulk HttpRequests ? Through a POST request? Through a simple text file that i send for the Django backend to process? (i have found some info about \u201cbulk updates\u201d for django but i\u2019m not sure that covers it entirely)</li>\n<li>How do i make it robust? How do i make sure that all data what successfully transmitted before deleting the log locally ? (if one call fails for a reason, or gets delayed, how do i make sure that the next one compensates for lost info?</li>\n</ul>\n\n<p><strong>Basically, i\u2019m asking advise for making a IOT based project, where a sensor gathers bulk information and want to send it to a backend server for processing, and how should that archiving process be designed.</strong></p>\n\n<p>PS: i expect the image processing part (at one fps) to be fast enough on my Pi Zero (as it is VERY simple); backlog at that level shouldn\u2019t be an issue.</p>\n\n<p>PPS: i\u2019m using a django backend (even if it seems a little overkill) \n    a/ because i already know the framework pretty well\n    b/ because i\u2019m expecting to build real-time performance indicators from the combined data points gathered, using django, and displaying them in (almost) real-time on a webpage.</p>\n\n<p>Thank you very much !</p>\n"
    }, {
        "tags": ["iot", "blockchain", "corda"],
        "is_answered": true,
        "view_count": 233,
        "favorite_count": 1,
        "score": -1,
        "last_activity_date": 1541683898,
        "creation_date": 1511996129,
        "question_id": 47563228,
        "link": "https://stackoverflow.com/questions/47563228/corda-dlt-for-iot",
        "title": "Corda DLT for IoT",
        "body": "<p>I'm interested in create a smart home environment using the concept of Blockchain, so I was introduced to Corda and I would like to know if it is possible to set every IoT device as a node at the Corda network. \nIf not, is there a framework or another way to do it? </p>\n"
    }, {
        "tags": ["amazon-web-services", "transform", "iot", "pipeline", "aws-iot-analytics"],
        "is_answered": true,
        "view_count": 48,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1541051254,
        "creation_date": 1537767944,
        "last_edit_date": 1541051254,
        "question_id": 52473298,
        "link": "https://stackoverflow.com/questions/52473298/not-storing-this-message-in-datastore-missingattributenames",
        "title": "Not storing this message in Datastore, missingAttributeNames",
        "body": "<p>I got this error from AWS IoT Analytics service after message is Transform in lambda:\nmy lambda get as input a json format string</p>\n\n<pre><code>{\n\"id\": \"223\",\n\"data\": \"valid-timestamp,1,2,3,4,5\"\n}\n</code></pre>\n\n<p>The data key holds my IoT data values on a specific timespan</p>\n\n<p>The lambda parse the above input and return array of dict:</p>\n\n<pre><code>[\n   {\n       \"id\": \"1\",\n       \"timestamp\": \"valid-timestamp1\",\n        \"value-1\": \"1\",\n        \"value-2\": \"2\",\n         \"value-3\": \"3\"\n   },\n   {\n       \"id\": \"1\",\n         \"timestamp\": \"valid-timestamp1\",\n        \"value-1\": \"1\",\n        \"value-2\": \"2\",\n         \"value-3\": \"3\"\n   }\n]\n</code></pre>\n\n<p>I did not succeeded to create a my_data_store\nI would be happy if someone can assist.\nThanks</p>\n"
    }, {
        "tags": ["database", "google-cloud-platform", "bigdata", "iot"],
        "is_answered": true,
        "view_count": 2644,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1540796353,
        "creation_date": 1540159839,
        "last_edit_date": 1540796353,
        "question_id": 52920359,
        "link": "https://stackoverflow.com/questions/52920359/store-large-iot-data-at-high-frequency-to-the-cloud",
        "title": "Store large IoT data at high frequency to the cloud",
        "body": "<p>I am building an IoT device that will be producing 200Kb of data per second, and I need to save this data to storage. I currently have about 500 devices, I am trying to figure out what is the best way to store the data? And the best database for this purpose? In the past I have stored data to GCP's BigQuery and done processing by using compute engine instance groups, but the size of the data was much smaller.</p>\n"
    }, {
        "tags": ["raspberry-pi", "sensors", "iot"],
        "is_answered": true,
        "view_count": 397,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1540556605,
        "creation_date": 1468722968,
        "question_id": 38417392,
        "link": "https://stackoverflow.com/questions/38417392/raspberry-pi-sensor-details-with-raspberry-pi",
        "title": "raspberry-pi - sensor details with Raspberry pi?",
        "body": "<p>Is it, in any way, possible to get a sensor details (e.g. manufacturer, type, model, function, etc.) using a raspberry pi?\nThanks.</p>\n"
    }, {
        "tags": ["mysql", "node.js", "get", "iot"],
        "is_answered": false,
        "view_count": 123,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1540407685,
        "creation_date": 1540387015,
        "last_edit_date": 1540407685,
        "question_id": 52970030,
        "link": "https://stackoverflow.com/questions/52970030/node-javascript-backend-send-data-from-mysqldb-to-esp8266",
        "title": "Node JavaScript Backend Send data from MysqlDB to ESP8266",
        "body": "<p>I have been building an IoT lamp that I want to be able to turn off and on with my phone.</p>\n\n<p>I have working code to upload data from the ESP 8266 to my back-end -> my phpMyAdmin MySQL database. But when I try to send values to the device it won't work if I use some sort of</p>\n\n<p><code>con.query(\"SELECT * FROM switch\", function (err, result) { }</code>. </p>\n\n<p>It doesn't work with .then and send the data from there either. See comments below (1 work, 2-3 don't).</p>\n\n<p>I have tried with many different things pool as you see below among them.</p>\n\n<pre><code>const express = require('express');\nconst router = express();\nvar mysql = require(\"mysql\");\n\nvar con = mysql.createConnection({\n    host: \"localhost\",\n    user: \"root\",\n    password: \"\",\n    database: \"light\"\n});\n\nvar pool = mysql.createPool({\n    connectionLimit: 100,\n    connectTimeout: 500000,\n    acquireTimeout: 500000,\n    queueLimit: 30,\n    host: 'localhost',\n    user: 'root',\n    password: '',\n    database: 'light',\n    multipleStatements: true,\n});\n\ncon.connect(function (err) {\n    if (err) throw err;\n});\nrouter.get('/', (req, res, next) =&gt; {\n// res.send(\"hello\"); // 1 WORKS\n    var GetLight = function () {\n        return new Promise(function (resolve, reject) {\n            con.query(\"SELECT * FROM switch\", function (err, result) {\n\n                // res.send(\"hello\"); //2 Dont work\n                if (err) {\n                    return reject(err);\n                } else {\n                    return resolve(result);\n                }\n\n            });\n        });\n    }\n\n    GetLight().then(Resultat =&gt; {\n\n       //res.status(200).json(Resultat); //3 Dont work\n\n    }).catch(err =&gt; {\n        console.log(err);\n        res.status(500).json({\n            error: err\n        });\n    });\n});\n</code></pre>\n"
    }, {
        "tags": ["python", "html", "screen-scraping", "iot"],
        "is_answered": false,
        "view_count": 25,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1540108929,
        "creation_date": 1539580226,
        "last_edit_date": 1539585377,
        "question_id": 52810061,
        "link": "https://stackoverflow.com/questions/52810061/how-can-i-pull-data-from-a-webpage-and-store-it-google-sheets-trending-data-fro",
        "title": "How can I pull data from a webpage and store it google sheets? Trending data from enviormental monitor",
        "body": "<p>I have this environmental monitor from a company that went out of business but it's useful, it has an NDIR co2 sensor, tempature and RH. It has an ethernet port and webserver. </p>\n\n<p>You can go to different urls such as <strong>127.0.0.1/setget?%sen-3</strong>\nand it outputs:</p>\n\n<pre><code>handler.setget(%sen-3)\n500\n</code></pre>\n\n<p>where 500 would be the current room co2 concentration in ppm. Can someone point me in the direction to go about scraping this into a database and charting it to see trends in co2. Ex: how much does co2 rise while sleeping, how much does it change with occupancy, etc. I was thinking Google Sheets and Google Charts would be a good start. How can I scrape this page and store it in Google Sheets? Python? I only have experience with HTML, PHP but don't mind learning python if it isn't too complicated of a project. </p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["python", "networkx", "iot"],
        "is_answered": false,
        "view_count": 31,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1539798721,
        "creation_date": 1539798721,
        "question_id": 52860866,
        "link": "https://stackoverflow.com/questions/52860866/what-are-internet-of-things-specific-parameters-network-metrics-to-focus-on-wh",
        "title": "What are Internet of Things specific-parameters (network metrics) to focus on when writing a program",
        "body": "<p>I am trying to model an IoT Network by using the library \"NetworkX\" by creating a graph, but I don't know what are the parameters and network metrics that I should measure through my program.</p>\n\n<p>I want to cover as much details as I can in my python program (about nodes, flows or any thing else) and by the end, I measure the performance of this network</p>\n\n<p>Until now, I work on these:</p>\n\n<ul>\n<li><p>Total number of nodes within the IoT network,</p></li>\n<li><p>The latency time (the amount of time it takes to receive a DATA / answer).</p></li>\n</ul>\n\n<p>Any ideas ?</p>\n\n<p>Thank you</p>\n"
    }, {
        "tags": ["project", "computer-science", "iot"],
        "is_answered": false,
        "view_count": 41,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1539751285,
        "creation_date": 1539751285,
        "question_id": 52847403,
        "link": "https://stackoverflow.com/questions/52847403/i-have-a-project-idea-on-smart-lighting-system-how-can-i-do-the-simulation-for",
        "title": "I have a project idea on Smart Lighting System. How can I do the simulation for this?",
        "body": "<p>Currently I am working on this project to provide the layout of a smart street light system with energy saving function based on sensor network for energy management. The proposal is an autonomous-distributed-controlled light system, in which the lights turn on before pedestrians come and turn off or reduce power when there is no one by means of a distributed-installed sensor network. </p>\n\n<p>I will be adding a few things to the project for energy reduction but what I need to know is how do I perform the simulation to show that this approach would reduce energy consumption? </p>\n"
    }, {
        "tags": ["c#", "raspberry-pi2", "iot", "windows-10-iot-core", "windowsiot"],
        "is_answered": true,
        "view_count": 19191,
        "favorite_count": 6,
        "score": 8,
        "last_activity_date": 1539549606,
        "creation_date": 1450169015,
        "last_edit_date": 1450169385,
        "question_id": 34284498,
        "link": "https://stackoverflow.com/questions/34284498/rfid-rc522-raspberry-pi-2-windows-iot",
        "title": "RFID RC522 Raspberry PI 2 Windows IOT",
        "body": "<p>I'm looking for a way to use the RFID \"RC522\" on a Raspberry Pi 2.0 on Windows IOT.</p>\n\n<p>It is of course not offical compatible...</p>\n\n<p>The offical one (OM5577 demo board) is way to expensive in France (i haven't found any reseller who sold it without a lot of shipping cost (total cost is around 80$)).</p>\n\n<p>The RC522 is cheap (&lt;10$). It works great on Arduino and on Raspberry Pi 2.0 on linux. But unfortunatly not yet on Windows IOT. </p>\n\n<p>I'm actually using an arduino as a bridge... It isn't an optimal solution; but work well and cost always half the price than the OM5577.</p>\n\n<p>I've found <a href=\"https://github.com/miguelbalboa/rfid\" rel=\"noreferrer\">this</a> project and try to convert them into a VS (Visual C++) project with the Windows IOT SIP and IO... I miserably fail...</p>\n\n<p>In my dream I would be able to use this device in C# with the standard windows IOT \"ProximityDevice\" class.</p>\n\n<p>Have you any idea for me?</p>\n\n<p>Thanks in advance.</p>\n"
    }, {
        "tags": ["apache-kafka", "mqtt", "iot"],
        "is_answered": true,
        "view_count": 38012,
        "favorite_count": 12,
        "score": 40,
        "last_activity_date": 1538742963,
        "creation_date": 1464008743,
        "last_edit_date": 1495535378,
        "question_id": 37391827,
        "link": "https://stackoverflow.com/questions/37391827/what-is-the-difference-between-mqtt-broker-and-apache-kafka",
        "title": "What is the difference between MQTT broker and Apache Kafka",
        "body": "<p>I am developing a mobile messaging app. I was going through technology needed and found two MQTT &amp; Apache Kafta. To me both seems doing the same thing in the same way (in terms of subscribing &amp; publishing to a topic). </p>\n\n<p>I heard that MQTT is fit for mobiles as it is very light weight ? So basically what is the difference between these two and what are the advantage of each on other?</p>\n"
    }, {
        "tags": ["amazon-web-services", "amazon-s3", "iot", "aws-iot", "amazon-machine-learning"],
        "is_answered": false,
        "view_count": 1878,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1538663678,
        "creation_date": 1538563085,
        "last_edit_date": 1538571504,
        "question_id": 52625202,
        "link": "https://stackoverflow.com/questions/52625202/aws-s3-storage-and-schema",
        "title": "AWS S3 storage and schema",
        "body": "<p>I have an IOT sensor which sends the following message to IoT MQTT Core topic:</p>\n\n<pre><code>{\"ID1\":10001,\"ID2\":1001,\"ID3\":101,\"ValueMax\":123}\n</code></pre>\n\n<p>I have added ACT/RULE which stores the incoming message in an S3 Bucket with the timestamp as a key(each message is stored as a seperate file/row in the bucket).</p>\n\n<p>I have only worked with SQL databases before, so having them stored like this is new to me. </p>\n\n<p>1) Is this the proper way to work with S3 storage?</p>\n\n<p>2) How can I visualize the values in a schema instead of separate files?</p>\n\n<p>3) I am trying to create ML Datasource from the S3 Bucket, but get the error below when Amazon ML tries to create schema: </p>\n\n<blockquote>\n  <p>\"Amazon ML can't retrieve the schema. If you've just created this\n  datasource, wait a moment and try again.\"</p>\n</blockquote>\n\n<p>Appreciate all advice there is! </p>\n"
    }, {
        "tags": ["highcharts", "iot"],
        "is_answered": false,
        "view_count": 65,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1534411526,
        "creation_date": 1534329351,
        "question_id": 51857138,
        "link": "https://stackoverflow.com/questions/51857138/highcharts-no-telemetric-timeline-chart",
        "title": "Highcharts - no telemetric timeline chart",
        "body": "<p>I want to create a combination between a standard telemetry graph and a graph with non-telemetric values spread over a time axis.</p>\n\n<p>I have a number of examples of graphs for example:\n<a href=\"https://i.stack.imgur.com/3Xs3z.png\" rel=\"nofollow noreferrer\">not telemetric data example</a></p>\n\n<p>I thought to use either a bullet graph or in the x range.\nBut I was able to use these in a way that is synchronized with the normal graph.</p>\n\n<p>\u05bfI am happy to hear ideas for a similar solution</p>\n"
    }, {
        "tags": ["amazon-web-services", "mqtt", "iot", "messagebroker", "aws-iot"],
        "is_answered": true,
        "view_count": 120,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1534330241,
        "creation_date": 1533215725,
        "question_id": 51654641,
        "link": "https://stackoverflow.com/questions/51654641/what-is-the-difference-between-jobs-and-messages-in-aws-iot",
        "title": "What is the difference between Jobs and Messages in AWS IoT?",
        "body": "<p>Jobs and Messages are both just transactions of text between AWS IoT service and devices.</p>\n\n<p>Why should I use jobs than messages or the other way around?</p>\n"
    }, {
        "tags": ["python", "firebase", "firebase-realtime-database", "iot"],
        "is_answered": true,
        "view_count": 291,
        "favorite_count": 0,
        "closed_date": 1534197363,
        "score": 1,
        "last_activity_date": 1534174197,
        "creation_date": 1534164668,
        "last_edit_date": 1534174197,
        "question_id": 51822832,
        "link": "https://stackoverflow.com/questions/51822832/how-to-retrieve-firebase-data-in-sequence",
        "title": "How to retrieve firebase data in sequence",
        "body": "<p><a href=\"https://i.stack.imgur.com/vSMLZ.png\" rel=\"nofollow noreferrer\">This is how the data stored in Firebase</a></p>\n\n<p><a href=\"https://i.stack.imgur.com/vSMLZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vSMLZ.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/7mzvd.png\" rel=\"nofollow noreferrer\">This is the output</a></p>\n\n<p><a href=\"https://i.stack.imgur.com/7mzvd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/7mzvd.png\" alt=\"enter image description here\"></a></p>\n\n<p>This is the code</p>\n\n<pre><code>from firebase import firebase\nurl = \"https://xxxx.firebaseio.com/\"\nfb = firebase.FirebaseApplication(url,None)\nHumidity = fb.get(\"/Humidity\",None)\nfor key, value in Humidity.items():\n    print(\"Humidity :{}\".format(value[\"hum\"]))\n</code></pre>\n"
    }, {
        "tags": ["apache-storm", "apache-flink", "iot", "ignite", "complex-event-processing"],
        "is_answered": false,
        "view_count": 100,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1533684477,
        "creation_date": 1533259899,
        "last_edit_date": 1533260153,
        "question_id": 51664163,
        "link": "https://stackoverflow.com/questions/51664163/iot-usecase-with-flink-or-storm-and-ignite",
        "title": "IoT usecase with Flink or Storm and Ignite",
        "body": "<p>I am weighing the pros and cons of design approaches for an IoT use case using Flink or Storm and Ignite. \nConsider I created a Rule Flow stating if any vehicle crossed the threshold speed limit set in the rule, then the flow should trigger an action to send SMS to drivers number. The flow I created is saving as a JSON in MongoDB. I have a list of 1000 OBD devices (which is continuously sending the data 3 times in a second) and associated drivers mobile numbers in another table in DB. In my design, I'm considering using an In memory DB too. Data collection is handling with Kafka. </p>\n\n<p>What would be the fastest and most scalable implementation approach to update the flow with data (Unique ID of OBD device and Driver Mobile Number)? I have in DB and streaming data from each OBD device (current speed of the vehicle). </p>\n"
    }, {
        "tags": ["iot", "platform", "kaa"],
        "is_answered": true,
        "view_count": 73,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1533577870,
        "creation_date": 1532968990,
        "question_id": 51598691,
        "link": "https://stackoverflow.com/questions/51598691/what-files-specfically-contain-the-kaa-node-ip-address",
        "title": "What files specfically contain the Kaa-node IP address",
        "body": "<p>I have a quick question. </p>\n\n<p>I've spent a couple of hours Googling this but I can't seem to find an answer. </p>\n\n<p>Where exactly in the Kaa C SDK is the host IP address of the kaa-node embedded? I know that each time you update the IP address of the Sandbox using the Management page, you have to regenerate the SDK. But in my case, the SDK that has generated has taken me many hours to debug and fix, and I would not like to repeat that process all over again. </p>\n\n<p>Instead, I would prefer just fixing the one or two files that contain the updated IP address. But does anyone know which files these are? </p>\n\n<p>I've heard that they are in the bootstrap extension header files. Is this true? </p>\n\n<p>Thank you!</p>\n"
    }, {
        "tags": ["ibm-cloud", "iot", "watson-iot"],
        "is_answered": true,
        "view_count": 240,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1533566432,
        "creation_date": 1533317369,
        "last_edit_date": 1533560643,
        "question_id": 51677502,
        "link": "https://stackoverflow.com/questions/51677502/how-can-i-use-db2-as-historical-data-storage-for-watson-iot-platform-instead-of",
        "title": "How can I use Db2 as historical data storage for Watson IoT Platform instead of NoSQL?",
        "body": "<p>I have an instance of Watson IoT Platform running and I am interested in storing the data from each event in a relational database (I am in control of how the data is formatted from the devices so knowing what the JSON will look like isn't a problem). I have found lots of documentation online, such as:</p>\n\n<p><a href=\"https://developer.ibm.com/recipes/tutorials/create-dashdb-data-warehouse-for-reporting-on-elevator-device-data/\" rel=\"nofollow noreferrer\">https://developer.ibm.com/recipes/tutorials/create-dashdb-data-warehouse-for-reporting-on-elevator-device-data/</a></p>\n\n<p>But I'm not sure this will apply since I'm not using the same services. Essentially what I want to do is store rows of data in my Db2 instance on the Cloud. Is this as simple as connecting a Cloud Foundry application to respond to new events from IoTP and writing them to the database? Or is there a simpler method?</p>\n"
    }, {
        "tags": ["iot"],
        "is_answered": false,
        "view_count": 77,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1532721397,
        "creation_date": 1528971261,
        "question_id": 50855046,
        "link": "https://stackoverflow.com/questions/50855046/storing-data-coming-form-cupcarbon-into-database",
        "title": "Storing data coming form cupcarbon into database",
        "body": "<p>I am creating an IoT simulation using Cupcarbon IoT simulator. I want to store the simulation data in some database. Anyone, please help me how to do that.</p>\n"
    }, {
        "tags": ["amazon-web-services", "iot", "aws-iot"],
        "is_answered": false,
        "view_count": 226,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1529487010,
        "creation_date": 1496215808,
        "question_id": 44278391,
        "link": "https://stackoverflow.com/questions/44278391/how-to-check-whether-a-certain-field-is-present-in-aws-iot-message-or-not",
        "title": "How to check whether a certain field is present in AWS IoT message or not?",
        "body": "<p>I have two IoT rules in which one rule stores valid data to a table in DynamoDB, and other rule which stores invalid data to another table in DynamoDB. </p>\n\n<p>This works fine as long as all fields are present. But if any field is not present IoT throws: <strong>Error while evaluating where clause: Undefined result</strong></p>\n\n<p>For example here is expected message: </p>\n\n<pre><code>{\n    field_1: 'value1',\n    field_2: 'value2',\n    field_3: 'value3'\n}\n</code></pre>\n\n<p>SQL rule for Valid Data:</p>\n\n<p><code>select field_1 as x, field_2 as y, field_3 as z WHERE cast(field_1 as Int) &gt; 10</code></p>\n\n<p>SQL rule for Invalid Data:</p>\n\n<p><code>select field_1 as x, field_2 as y, field_3 as z WHERE NOT ( cast(field_1 as Int) &gt; 10 )</code></p>\n\n<p>If any field is missing as shown below, above rule won't work.</p>\n\n<pre><code>{\n    field_1: 'value1',\n    field_2: 'value2'\n}\n</code></pre>\n\n<p>What is the way to detect whether a certain field is present or not?</p>\n"
    }, {
        "tags": ["iot", "blockchain"],
        "is_answered": false,
        "view_count": 102,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1528983725,
        "creation_date": 1528961538,
        "question_id": 50851976,
        "link": "https://stackoverflow.com/questions/50851976/local-blockchain-vs-centralized-database-in-a-smart-home-environment",
        "title": "Local blockchain vs. centralized database in a smart home environment",
        "body": "<p>I am working with a smart home (IoT) environment that creates sensible data and I want to secure the immutability by storing the hash of that data on a public blockchain.</p>\n\n<p>I want to use a local blockchain within the smart home environment to store data produced by IoT devices. The reason for using a private local blockchain instead of a public one is that I want to keep the user's data private.</p>\n\n<p>Another approach would be to use a centralized database system to store all the data of the smart home environment and take the hash of the entire DB to store it on the public ledger.</p>\n\n<p><strong>Are there reasons for using a blockchain over a centralized system in this case?</strong></p>\n\n<p>The only benefit I could come up with is that the hash of the blockchain is already present whereas the hash of the centralized DB has to be calculated and could take a while.</p>\n"
    }, {
        "tags": ["c", "base64", "iot"],
        "is_answered": false,
        "view_count": 472,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1528918554,
        "creation_date": 1528917958,
        "question_id": 50844834,
        "link": "https://stackoverflow.com/questions/50844834/c-convert-a-timestamp-uint-32-into-base64",
        "title": "C - convert a timestamp (uint_32) into base64",
        "body": "<p>For some IoT communication I need to transmit some information over SMS, some of them being timestamp precise to the seconds, in order to save as much place as possible, I would like to encode them in base64</p>\n\n<p>How to do this in C ? (if possible without malloc)</p>\n\n<p>(of course I'm glad to hear if there's a more efficient, \"no too custom\" method of encoding an int32 in a SMS-compliant way)</p>\n"
    }, {
        "tags": ["mongodb", "rest", "raspberry-pi", "iot"],
        "is_answered": true,
        "view_count": 475,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1528706030,
        "creation_date": 1528702107,
        "last_edit_date": 1528702292,
        "question_id": 50792434,
        "link": "https://stackoverflow.com/questions/50792434/store-sensor-data-in-mongodb",
        "title": "Store sensor data in MongoDB",
        "body": "<p>I'm working with Raspberry Pi trying to send the sensor data by using REST API back to Node.js (Express) and then store or update data inside MongoDB overtime. Since I have to work with many devices, how can I store data accordingly to its device. Is it a good idea to override objectId as raspberry pi serial no. and request POST/PUT to update its data overtime.</p>\n"
    }, {
        "tags": ["real-time", "iot", "apache-flink", "flink-streaming", "bigdata"],
        "is_answered": true,
        "view_count": 2515,
        "favorite_count": 5,
        "score": 4,
        "last_activity_date": 1528507233,
        "creation_date": 1478522114,
        "last_edit_date": 1506102531,
        "question_id": 40465335,
        "link": "https://stackoverflow.com/questions/40465335/apache-flink-multiple-window-aggregations-and-late-data",
        "title": "Apache Flink: Multiple Window Aggregations and Late Data",
        "body": "<p>We plan to use Apache Flink with a huge IOT Setup. Customers are going to send us some kind of structured sensor data (like sensor_id, sensor_type, sensor_value, timestamp). We have no control when each customer sends this data, most likely in real-time, but we have no guarantee. We store all events in RabbitMQ/Kafka. UPDATE: We can assume that events per sensor come in order.</p>\n\n<p>Before starting implementing a possible streaming pipeline, we are interested in solutions for the following challenges:</p>\n\n<ol>\n<li>Multiple Window Aggregations</li>\n</ol>\n\n<p>We store all raw sensor data into Cassandra. Further, we want to aggregate the sensor data by sensor_id on multiple time windows (e.g. 15 sek, 1 min, 15 min, 1 hour, 1 day). What is the recommended way to achieve the desired output efficiently with Flink streaming?</p>\n\n<ol start=\"2\">\n<li>Very late data</li>\n</ol>\n\n<p>As already mentioned we have no control over <code>when</code> the data is sent. For example, a customer might experience network failures and therefore data might arrive late. How is the recommended way to handle this? How can we use watermarking if we can only guarantee good watermarks by sensor_id (because each customer has its own time/issues/failures)? We can add some allowed lateness (like 6 - 12 hours or so), is that managable with flinks in memory window storage? What happens after this allowed lateness? Should we store the really late data into another kafka topic and doing batch processing continuously? Lastly, some customers upload csv files with their collected sensor data. Does this guide as well to an batch approach?</p>\n\n<ol start=\"3\">\n<li>Future data</li>\n</ol>\n\n<p>What happens to a stream, when some customer send us data that is far in the future, due to misconfigured sensors (as we have no control over it)?</p>\n\n<p>We are curious about your recommendations. Thanks.</p>\n"
    }, {
        "tags": ["amazon-web-services", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 539,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1528071192,
        "creation_date": 1455181250,
        "question_id": 35334799,
        "link": "https://stackoverflow.com/questions/35334799/is-it-a-best-practice-to-keep-copies-of-the-device-state-in-thing-shadow-and-a-d",
        "title": "Is it a best practice to keep copies of the device state in thing shadow and a database like DynamoDB?",
        "body": "<p>Is it a best practice to keep a copy of the device shadow in a database like DynamoDB?</p>\n\n<ul>\n<li>This would be helpful when we have to query on the aggregated device\ndata. For Eg. \"GET me all the devices with state='ON'\"</li>\n<li>The problem here would be synchronising the copies of data in the\ndevice shadow and database</li>\n</ul>\n\n<p>Any suggestions on this?\ufeff</p>\n"
    }, {
        "tags": ["node.js", "iot", "node-red"],
        "is_answered": false,
        "view_count": 303,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1527729372,
        "creation_date": 1526027532,
        "last_edit_date": 1526028351,
        "question_id": 50288241,
        "link": "https://stackoverflow.com/questions/50288241/how-to-reuse-the-mysql-database-node-in-node-red",
        "title": "How to reuse the mysql database node in node red?",
        "body": "<p>Currently i am working on node red project. In that i want to update three different table from node red dashboard based on user inputs.</p>\n\n<p>So i want to access database more than 20 times. So i am using a database node from node-red-node-mysql palette in all 20 place. Now it is working fine. but if i  want to configure an another database then i need to change in all nodes in my flows. In real time also it is difficult. As just an idea, Is it possible to use only one database node for all my flows and split the output of the database to different nodes based on the message? If yes please give me some example. </p>\n"
    }, {
        "tags": ["google-bigquery", "google-cloud-platform", "iot", "google-cloud-iam"],
        "is_answered": false,
        "view_count": 127,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1527174530,
        "creation_date": 1496929968,
        "last_edit_date": 1527174530,
        "question_id": 44437654,
        "link": "https://stackoverflow.com/questions/44437654/how-to-implement-iot-with-gcp-what-are-the-limits-of-both-cloud-projects-and-se",
        "title": "How to implement IoT with GCP: What are the limits of both cloud projects and service accounts per project? To what number can they be increased?",
        "body": "<p>In short: What are the limits of both cloud projects and service accounts per project? How can they be increased? Is the architecture a good idea at all?</p>\n\n<p>I am developing an IoT application with tens of thousands of planned devices in the field, having a few devices per customer and hundreds of customers. Every device will continuously (24/7) stream measurement data directly to BigQuery with one dataset (or table) per device at sample rates of at least 100Hz.</p>\n\n<p>Of course, every device needs to be authenticated and authorized to gain tightly restricted access to its cloud dataset. As stated in the Auth Guide API keys are not very secure. Therefore, the most appropriate solutions seems to have one service account per customer with one account key per device (as suggested in <a href=\"https://cloud.google.com/solutions/iot-overview\" rel=\"nofollow noreferrer\">this GCP article</a>). However, the <a href=\"https://cloud.google.com/iam/docs/faq\" rel=\"nofollow noreferrer\">FAQs of Cloud IAM</a> state that the number of service accounts is limited to 100 per project. </p>\n\n<ul>\n<li>This limit could be reached quickly. If so, how easily/costly is it to increase this limit towards thousands or tens of thousands of service accounts per project?</li>\n<li>In such a scenario also the number of needed projects could easily grow to hundreds or thousands. Would this be feasible?</li>\n<li>Is this overall concept practical or are there better approaches within GCP?</li>\n</ul>\n"
    }, {
        "tags": ["reporting", "iot", "cumulocity"],
        "is_answered": true,
        "view_count": 165,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1526983310,
        "creation_date": 1508240010,
        "question_id": 46789152,
        "link": "https://stackoverflow.com/questions/46789152/best-practice-to-create-automatic-reports-with-cumulocity",
        "title": "Best practice to create automatic reports with Cumulocity",
        "body": "<p>I am wanting to create automated reports using the data I have collected with Cumulocity. I would like to automate the creation of the reports so that they aggregate accumulated timeseries, alarm &amp; event data into a single report that could be automatically sent to relevant parties by e-mail on a fixed time interval (weekly, for example).</p>\n\n<p>I understand how to do this with the APIs that Cumulocity provides, but I am interested in experiences of more specific implementations (I imagine I am not the first person thinking about this). Should the data that I want to aggregate be retrieved with CEL (Cumulocity Event Language) and then the report created in the desired format (pdf) with my own script? Is it possible to host this kind of script in Cumulocity or do I have to host it myself?</p>\n"
    }, {
        "tags": ["amazon-web-services", "amazon-dynamodb", "iot", "aws-iot"],
        "is_answered": true,
        "view_count": 540,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1525924291,
        "creation_date": 1525856125,
        "question_id": 50249272,
        "link": "https://stackoverflow.com/questions/50249272/how-to-handle-aws-iot-streaming-data-in-relational-database",
        "title": "How to handle AWS IOT streaming data in relational database",
        "body": "<p><strong>Generic information</strong> :-i am designing solution for one of IOT problem approach in which data is continuously  streaming from plc(programmable logic controller),plc have different tags these tags are representation of telemetry data and data will be continuously streaming from these tags, each of devices will have alarm tags which will be 0 or 1 , 1 means there is an equipment failure \n<strong>problem statement</strong>:- i have to read the alarm tag and raise a ticket if any of alarm tag value is 1  and i have to stream these alerts to dashboard and also  i have to maintain the ticket history too,so the operator can update the ticket status too</p>\n\n<p><strong>My solution</strong>:- i am using aws IOT , i am getting data in dynamo db then i am using dynamo db stream to check if any new item is added in alarm table and if it will trigger lambda function (which i have implemented in java) lambda function opens a new ticket in relational database using hibernate.</p>\n\n<p><strong>problem with my approach</strong>:-the aws iot data is continuously streaming in alarm table at a very fast rate and this is opening a lot of connection before it can be closed that's taking my relational database down</p>\n\n<p>please let me know if other good design approach can i adopt?</p>\n"
    }, {
        "tags": ["google-bigquery", "time-series", "bigdata", "scalability", "iot"],
        "is_answered": true,
        "view_count": 78,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1525650680,
        "creation_date": 1525644752,
        "question_id": 50205008,
        "link": "https://stackoverflow.com/questions/50205008/best-design-technology-to-store-fast-growing-amount-of-data",
        "title": "Best design/technology to store fast growing amount of data",
        "body": "<p>I need to store signals coming from million devices every 1 minute, where each signal object has 4 attributes plus the timestamp:</p>\n\n<ul>\n<li>Device ID, always the same</li>\n<li>Attr1, always the same (device model)</li>\n<li>Attr2, changes every 6 months aprox. (device fixed location)</li>\n<li>Attr3, changes every 2-4 weeks (device firmware version)</li>\n</ul>\n\n<p>With the collected data I need to get some reports, like \"How many devices with attr2 checked in last month\". The restriction here is that I may need to filter and group by any attribute, and not only the device ID.</p>\n\n<p>My first approach was to have a model with nested records in bigquery, but I'm not sure this would be the best solution.</p>\n\n<p>Which database and schema would you recommend me to solve this problem? </p>\n\n<p>Thank you!</p>\n"
    }, {
        "tags": ["mongodb", "analytics", "iot"],
        "is_answered": false,
        "view_count": 107,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1525532502,
        "creation_date": 1525531722,
        "last_edit_date": 1525532502,
        "question_id": 50190699,
        "link": "https://stackoverflow.com/questions/50190699/iot-data-storage-best-practice",
        "title": "IoT Data Storage Best Practice",
        "body": "<p>Fairly new to analytics and looking for some guidance on how to store data in a MongoDB to be analysed in the future.</p>\n\n<p>An example would be a temperature sensor that reports once every 10 minutes. After 1 year, we would like to run a report that shows the temperature over time.</p>\n\n<p>Would a new document be created every time a new value is reported? </p>\n\n<pre><code>{\n  id: 'xyz',\n  date : \"Sat May 05 2018 00:00:00\",\n  value: 20.0,\n  unit: 'Celsius'\n},\n\n{\n  id: 'xyz',\n  date : \"Sat May 05 2018 00:10:00\",\n  value: 19.0,\n  unit: 'Celsius'\n}\n</code></pre>\n\n<p>Or would one document get updated with the new information? </p>\n\n<pre><code>{\n  _id: 'xyz',\n  values:[\n    {\n      date : \"Sat May 05 2018 00:00:00\",\n      value: 20.0,\n      unit: 'Celsius'\n    },\n    {\n      date : \"Sat May 05 2018 00:10:00\",\n      value: 19.0,\n      unit: 'Celsius'\n    }\n  ]\n}\n</code></pre>\n\n<p><a href=\"https://www.mongodb.com/blog/post/using-mongodb-for-real-time-analytics\" rel=\"nofollow noreferrer\">upset and $inc</a> look useful. But seem to overwrite the old values.</p>\n"
    }, {
        "tags": ["java", "azure", "hbase", "iot", "azure-hdinsight"],
        "is_answered": false,
        "view_count": 283,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1525145374,
        "creation_date": 1524637806,
        "last_edit_date": 1524638144,
        "question_id": 50015423,
        "link": "https://stackoverflow.com/questions/50015423/azure-hdinsight-hbase-data-insertion-failed",
        "title": "Azure - HDInsight Hbase Data Insertion Failed",
        "body": "<p>Currently we are migrating our IoT platform as a PAAS service. We are using HDInsight Hbase for all IoT data insertion. Now i am able to create and delete tables in the HBase from java application. But i am not able insert or select any data from the HDInight Hbase table. Please suggest me if anything is missing in code level.</p>\n\n<p>HBase Insert Java Code:</p>\n\n<pre><code>// TODO Auto-generated method stub\n   // define some people\n Configuration config = HBaseConfiguration.create();\n\n // Example of setting zookeeper values for HDInsight\n // in code instead of an hbase-site.xml file\n //\n  config.set(\"hbase.zookeeper.quorum\",\n             \"zk1:2181,zk2:2181,zk3:2181\");\n config.set(\"hbase.zookeeper.property.clientPort\", \"2181\");\n config.set(\"hbase.cluster.distributed\", \"true\");\n //\n //NOTE: Actual zookeeper host names can be found using Ambari:\n //curl -u admin:PASSWORD -G \"https://CLUSTERNAME.azurehdinsight.net/api/v1/clusters/CLUSTERNAME/hosts\"\n\n //Linux-based HDInsight clusters use /hbase-unsecure as the znode parent\n config.set(\"zookeeper.znode.parent\",\"/hbase-unsecure\");\n System.out.println(\"1 - \" + config);\n String[][] people = {\n     { \"1\", \"Marcel\", \"Haddad\", \"marcel@fabrikam.com\"},\n     { \"2\", \"Franklin\", \"Holtz\", \"franklin@contoso.com\" },\n     { \"3\", \"Dwayne\", \"McKee\", \"dwayne@fabrikam.com\" },\n     { \"4\", \"Rae\", \"Schroeder\", \"rae@contoso.com\" },\n     { \"5\", \"Rosalie\", \"burton\", \"rosalie@fabrikam.com\"},\n     { \"6\", \"Gabriela\", \"Ingram\", \"gabriela@contoso.com\"} };\n\n HTable table = new HTable(config, \"people\");\n System.out.println(\"2 - \" + table);\n // Add each person to the table\n //   Use the `name` column family for the name\n //   Use the `contactinfo` column family for the email\n for (int i = 0; i&lt; people.length; i++) {\n     Put person = new Put(Bytes.toBytes(people[i][0]));\n     person.add(Bytes.toBytes(\"name\"), Bytes.toBytes(\"first\"), Bytes.toBytes(people[i][1]));\n     person.add(Bytes.toBytes(\"name\"), Bytes.toBytes(\"last\"), Bytes.toBytes(people[i][2]));\n     person.add(Bytes.toBytes(\"contactinfo\"), Bytes.toBytes(\"email\"), Bytes.toBytes(people[i][3]));\n     System.out.println(\"3 - \" + person);\n     table.put(person);\n     System.out.println(\"4 - \" + table);\n }\n // flush commits and close the table\n System.out.println(\"5 - \" + table);\n table.flushCommits();\n table.close();\n System.out.println(\"6 - \" + table);\n</code></pre>\n\n<p>Error : </p>\n\n<pre><code>2083 [main] INFO  o.a.h.h.z.RecoverableZooKeeper - Process identifier=hconnection-0x524d6d96 connecting to ZooKeeper ensemble=zk0-bdtrin.un52sso10ikejkjuhwkxemgbfa.rx.internal.cloudapp.net:2181,zk4-bdtrin.un52sso10ikejkjuhwkxemgbfa.rx.internal.cloudapp.net:2181,zk1-bdtrin.un52sso10ikejkjuhwkxemgbfa.rx.internal.cloudapp.net:2181\n7616 [main] WARN  o.a.h.c.Configuration - hbase-site.xml:an attempt to override final parameter: dfs.support.append;  Ignoring.\n7616 [main] WARN  o.a.h.h.u.DynamicClassLoader - Failed to identify the fs of dir /hbase/lib, ignored\njava.io.IOException: No FileSystem for scheme: wasb\n      at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2584) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:354) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296) ~[hadoop-common-2.6.1.jar:?]\n      at org.apache.hadoop.hbase.util.DynamicClassLoader.&lt;init&gt;(DynamicClassLoader.java:104) [hbase-common-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.protobuf.ProtobufUtil.&lt;clinit&gt;(ProtobufUtil.java:238) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.ClusterId.parseFrom(ClusterId.java:64) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:75) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:879) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:635) [hbase-client-1.1.0.jar:1.1.0]\n      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_151]\n      at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [?:1.8.0_151]\n      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_151]\n      at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_151]\n      at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:420) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.client.ConnectionManager.createConnectionInternal(ConnectionManager.java:329) [hbase-client-1.1.0.jar:1.1.0]\n      at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:144) [hbase-client-1.1.0.jar:1.1.0]\n      at com.trinity.iot.storm.topology.HbaseTest.main(HbaseTest.java:34) [classes/:?]\n7697 [main] WARN  o.a.h.c.Configuration - hbase-site.xml:an attempt to override final parameter: dfs.support.append;  Ignoring.\n7750 [main] INFO  o.a.h.h.z.RecoverableZooKeeper - Process identifier=hconnection-0x2d0399f4 connecting to ZooKeeper ensemble=zk1:2181,zk2:2181,zk3:2181\n</code></pre>\n\n<p>hbase-site.xml</p>\n\n<pre><code> &lt;configuration&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;\n      &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;dfs.support.append&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.bucketcache.combinedcache.enabled&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.bucketcache.ioengine&lt;/name&gt;\n      &lt;value&gt;file:/mnt/hbase/cache.data&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.bucketcache.percentage.in.combinedcache&lt;/name&gt;\n      &lt;value&gt;&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.bucketcache.size&lt;/name&gt;\n      &lt;value&gt;81920&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.bulkload.staging.dir&lt;/name&gt;\n      &lt;value&gt;/apps/hbase/staging&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.client.keyvalue.maxsize&lt;/name&gt;\n      &lt;value&gt;1048576&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.client.retries.number&lt;/name&gt;\n      &lt;value&gt;35&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;\n      &lt;value&gt;100&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;\n      &lt;value&gt;&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;\n      &lt;value&gt;org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.custom-extensions.root&lt;/name&gt;\n      &lt;value&gt;/hdp/ext/2.6/hbase&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.defaults.for.version.skip&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.fs.shutdown.hook.wait&lt;/name&gt;\n      &lt;value&gt;600000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hregion.majorcompaction&lt;/name&gt;\n      &lt;value&gt;0&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hregion.majorcompaction.jitter&lt;/name&gt;\n      &lt;value&gt;0.50&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hregion.max.filesize&lt;/name&gt;\n      &lt;value&gt;10737418240&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hregion.memstore.block.multiplier&lt;/name&gt;\n      &lt;value&gt;4&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hregion.memstore.flush.size&lt;/name&gt;\n      &lt;value&gt;134217728&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hregion.memstore.mslab.enabled&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hstore.blockingStoreFiles&lt;/name&gt;\n      &lt;value&gt;100&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hstore.compaction.max&lt;/name&gt;\n      &lt;value&gt;10&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hstore.compaction.max.size&lt;/name&gt;\n      &lt;value&gt;32212254720&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.hstore.compactionThreshold&lt;/name&gt;\n      &lt;value&gt;3&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.local.dir&lt;/name&gt;\n      &lt;value&gt;${hbase.tmp.dir}/local&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.distributed.log.splitting&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.info.bindAddress&lt;/name&gt;\n      &lt;value&gt;0.0.0.0&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.info.port&lt;/name&gt;\n      &lt;value&gt;16010&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.namespace.init.timeout&lt;/name&gt;\n      &lt;value&gt;2400000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.port&lt;/name&gt;\n      &lt;value&gt;16000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.ui.readonly&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.master.wait.on.regionservers.timeout&lt;/name&gt;\n      &lt;value&gt;30000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.region.server.rpc.scheduler.factory.class&lt;/name&gt;\n      &lt;value&gt;org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.executor.openregion.threads&lt;/name&gt;\n      &lt;value&gt;20&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.global.memstore.size&lt;/name&gt;\n      &lt;value&gt;0.4&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.handler.count&lt;/name&gt;\n      &lt;value&gt;100&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.hlog.blocksize&lt;/name&gt;\n      &lt;value&gt;134217728&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt;\n      &lt;value&gt;16030&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.optionalcacheflushinterval&lt;/name&gt;\n      &lt;value&gt;7200000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.port&lt;/name&gt;\n      &lt;value&gt;16020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt;\n      &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.rest.port&lt;/name&gt;\n      &lt;value&gt;8090&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.rootdir&lt;/name&gt;\n      &lt;value&gt;/hbase&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.rpc.protection&lt;/name&gt;\n      &lt;value&gt;authentication&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.rpc.timeout&lt;/name&gt;\n      &lt;value&gt;90000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.rs.cacheblocksonwrite&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.security.authentication&lt;/name&gt;\n      &lt;value&gt;simple&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.security.authorization&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.shutdown.hook&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.superuser&lt;/name&gt;\n      &lt;value&gt;hbase&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.tmp.dir&lt;/name&gt;\n      &lt;value&gt;/tmp/hbase-${user.name}&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;\n      &lt;value&gt;2181&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;\n      &lt;value&gt;zk01,zk2,zk3&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hbase.zookeeper.useMulti&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hfile.block.cache.size&lt;/name&gt;\n      &lt;value&gt;0.40&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;hfile.index.block.max.size&lt;/name&gt;\n      &lt;value&gt;131072&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;io.storefile.bloom.block.size&lt;/name&gt;\n      &lt;value&gt;131072&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;phoenix.functions.allowUserDefinedFunctions&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;phoenix.query.timeoutMs&lt;/name&gt;\n      &lt;value&gt;60000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;zookeeper.recovery.retry&lt;/name&gt;\n      &lt;value&gt;6&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;\n      &lt;value&gt;120000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;\n      &lt;value&gt;/hbase-unsecure&lt;/value&gt;\n    &lt;/property&gt;\n\n  &lt;/configuration&gt;\n</code></pre>\n\n<p>pom.xml:</p>\n\n<pre><code>&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n  &lt;groupId&gt;HDInsight-HbaseTest&lt;/groupId&gt;\n  &lt;artifactId&gt;HDInsight-HbaseTest&lt;/artifactId&gt;\n  &lt;version&gt;1&lt;/version&gt;\n  &lt;name&gt;HDInsight-HbaseTest&lt;/name&gt;\n  &lt;description&gt;HDInsight-HbaseTest&lt;/description&gt;\n\n  &lt;dependencies&gt;\n   &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;\n     &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;\n     &lt;version&gt;1.1.2&lt;/version&gt;\n &lt;/dependency&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt;\n     &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt;\n     &lt;version&gt;4.4.0-HBase-1.1&lt;/version&gt;\n &lt;/dependency&gt;\n &lt;!-- https://mvnrepository.com/artifact/jdk.tools/jdk.tools --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;jdk.tools&lt;/groupId&gt;\n            &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8.0_151&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n\n   &lt;build&gt;\n    &lt;!--  &lt;sourceDirectory&gt;src&lt;/sourceDirectory&gt; --&gt;\n     &lt;resources&gt;\n     &lt;resource&gt;\n         &lt;directory&gt;src/main/resources&lt;/directory&gt;\n         &lt;filtering&gt;false&lt;/filtering&gt;\n         &lt;includes&gt;\n         &lt;include&gt;hbase-site.xml&lt;/include&gt;\n         &lt;/includes&gt;\n     &lt;/resource&gt;\n     &lt;/resources&gt;\n     &lt;plugins&gt;\n     &lt;plugin&gt;\n         &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n         &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                 &lt;version&gt;3.3&lt;/version&gt;\n         &lt;configuration&gt;\n             &lt;source&gt;1.8&lt;/source&gt;\n             &lt;target&gt;1.8&lt;/target&gt;\n         &lt;/configuration&gt;\n         &lt;/plugin&gt;\n     &lt;plugin&gt;\n         &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n         &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n         &lt;version&gt;2.3&lt;/version&gt;\n         &lt;configuration&gt;\n         &lt;transformers&gt;\n             &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer\"&gt;\n             &lt;/transformer&gt;\n         &lt;/transformers&gt;\n         &lt;/configuration&gt;\n         &lt;executions&gt;\n         &lt;execution&gt;\n             &lt;phase&gt;package&lt;/phase&gt;\n             &lt;goals&gt;\n             &lt;goal&gt;shade&lt;/goal&gt;\n             &lt;/goals&gt;\n         &lt;/execution&gt;\n         &lt;/executions&gt;\n     &lt;/plugin&gt;\n     &lt;/plugins&gt;\n &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n"
    }, {
        "tags": ["node.js", "http", "sap", "iot"],
        "is_answered": false,
        "view_count": 206,
        "favorite_count": 1,
        "score": 1,
        "last_activity_date": 1524897900,
        "creation_date": 1497182241,
        "question_id": 44483583,
        "link": "https://stackoverflow.com/questions/44483583/nodejs-generating-duplicate-http-requests",
        "title": "Nodejs generating duplicate http-requests",
        "body": "<p>Hope some can help with my issue. i'm using below nodejs code from this <a href=\"https://www.sap.com/developer/tutorials/iot-part11-hcp-services-ti.html\" rel=\"nofollow noreferrer\">SAP Tutorial</a> to read Sensor values post them per HTTP. All works pretty fine, but for the fact that every record is posted twice(see <a href=\"https://i.stack.imgur.com/35IJ3.png\" rel=\"nofollow noreferrer\">Screenshot</a>). i'm not versed with server-side JS and don't know why the duplicates.Agreed, the values not aways the same, but for further processing i'd like to have single datasets per timestamp. Could someone please help me locate the issue and if possible, provide a solution/workaround?</p>\n\n<p>Also the script reads and transmits the data every 10s. Am looking for a way to set the interval to maybe 3mins. I would appreciate every bit of help here as well</p>\n\n<pre><code> /*     sensorTag IR Temperature sensor example\n*  Craig Cmehil, SAP SE (c) 2015\n*/\n\n/* Choose the proper HTTP or HTTPS, SAP Cloud Platformrequires HTTPS */\nvar http = require('https');\n\nvar SensorTag = require('sensortag');\nvar lv_temp;\nvar lv_humid;\nvar lv_deviceid = \"\";\nvar DEBUG_VALUE = true;\nvar xtimestamp;\nvar date = new Date();\nvar time = date.getTime ();\n\n// SAP Cloud Platform connection details\nvar portIoT = 443;\nvar pathIoT = '/com.sap.iotservices.mms/v1/api/http/data/';\nvar hostIoT = 'iotmmsXXXXXXXXXXtrial.hanatrial.ondemand.com';\nvar authStrIoT = 'Bearer XXXXXXXXXXXX';\nvar deviceId = 'XXXXXX-XXXX-XXXX-XXXX-XXXXXXXXX';\nvar messageTypeID = 'XXXXXXXXXXXX';\n\nvar options = {\n    host: hostIoT,\n  port: portIoT,\n    path: pathIoT + deviceId,\n    agent: false,\n    headers: {\n       'Authorization': authStrIoT,\n       'Content-Type': 'application/json;charset=utf-8',\n     'Accept': '*/*'\n    },\n    method: 'POST',     \n};\n\n/***************************************************************/\n/* Coding to access TI SensorTag and values of various sensors */\n/***************************************************************/\n\nconsole.log(\"If not yet activated, then press the power button.\");\nSensorTag.discover(function(tag) {\ntag.on('disconnect', function() {\n    console.log('disconnected!');\n    process.exit(0);\n});\n\nfunction connectExecute() {\n    console.log('Connect Device and Execute Sensors');\n    tag.connectAndSetUp(enableSensors);\n}\n\nfunction enableSensors() {\n    /* Read device specifics */\n    tag.readDeviceName(function(error, deviceName) {\n        console.log('Device Name = ' + deviceName);\n    });\n    tag.readSystemId(function(error, systemId) {\n        console.log('System ID = ' + systemId);\n        lv_deviceid = systemId;\n    });\n    tag.readSerialNumber(function(error, serialNumber) {\n        console.log('Serial Number = ' + serialNumber);\n    });\n    tag.readFirmwareRevision(function(error, firmwareRevision) {\n        console.log('Firmware Rev = ' + firmwareRevision);\n    });\n    tag.readHardwareRevision(function(error, hardwareRevision) {\n        console.log('Hardware Rev = ' + hardwareRevision);\n    });\n    tag.readHardwareRevision(function(error, softwareRevision) {\n        console.log('Software Revision = ' + softwareRevision);\n    });\n    tag.readManufacturerName(function(error, manufacturerName) {\n        console.log('Manufacturer = ' + manufacturerName);\n    });\n    /* Enable Sensors */\n    console.log(\"Enabling sensors:\");\n    console.log('\\tenableIRTemperatureSensor');\n    tag.enableIrTemperature(notifyMe);\n    console.log('\\tenableHumidity');\n    tag.enableHumidity(notifyMe);\n    console.log(\"*********************************************\");\n    console.log(\" To stop press both buttons on the SensorTag \");\n    console.log(\"*********************************************\");\n}\n\nfunction notifyMe() {\n    tag.notifySimpleKey(listenForButton);\n    setImmediate(function loop () {\n        tag.readIrTemperature(function(error, objectTemperature, ambientTemperature){\n            lv_obj = objectTemperature.toFixed(1);\n            lv_ambient = ambientTemperature.toFixed(1);\n            });\n        tag.readHumidity(function(error, temperature, humidity) {\n            lv_temp = temperature.toFixed(1);\n            lv_humid = humidity.toFixed(1);\n        });\n        if(DEBUG_VALUE)\n            console.log(\"Sending Data: \" + lv_deviceid + \" \" + lv_temp + \" \" + lv_humid);\n        setSensorData(lv_temp, lv_humid);\n        setTimeout(loop, 10000);\n    });\n  }\n\nfunction listenForButton() {\n    tag.on('simpleKeyChange', function(left, right) {\n        if (left &amp;&amp; right) {\n            tag.disconnect();\n        }\n   });\n}\n\nconnectExecute();\n});\n\n/******************************************************************/\n/* FUNCTION to get Temperature from the Sensor &amp; update into HANA */\n/******************************************************************/\nfunction setSensorData(lv_temp,lv_humid){\ndate = new Date();\n  time =date.getTime();\n\nvar data = {\n    \"mode\":\"sync\",\n    \"messageType\": messageTypeID,\n    \"messages\": [{\n        \"timestamp\": time,\n        \"temperature\": lv_temp,\n        \"humidity\": lv_humid\n    }]\n  };\nvar strData = JSON.stringify(data);\nif(DEBUG_VALUE)\n    console.log(\"Data: \" + strData);\nif(strData.length &gt; 46){\n    if(DEBUG_VALUE)\n        console.log(\"Sending Data to server\");\n    /* Process HTTP or HTTPS request */\n    options.agent = new http.Agent(options);\n    var request_callback = function(response) {\n        var body = '';\n        response.on('data', function (data) {\n            body += data;\n        });\n        response.on('end', function () {\n            if(DEBUG_VALUE)\n                console.log(\"REQUEST END:\", response.statusCode);\n        });\n        response.on('error', function(e) {\n            console.error(e);\n        });    \n    }\n    var request = http.request(options, request_callback);\n    request.on('error', function(e) {\n        console.error(e);\n    });\n    request.write(strData);\n    request.end();\n}else{\n    if(DEBUG_VALUE)\n        console.log(\"Incomplete Data\");\n}\n}\n</code></pre>\n"
    }, {
        "tags": ["stream", "streaming", "iot", "azure-iot-hub", "azure-stream-analytics"],
        "is_answered": true,
        "view_count": 181,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1524896878,
        "creation_date": 1524862402,
        "question_id": 50070169,
        "link": "https://stackoverflow.com/questions/50070169/how-to-create-a-delayed-sliding-window-in-azure-stream-analytics",
        "title": "How to create a delayed sliding window in Azure Stream Analytics",
        "body": "<p>I would like to calculate the rate of change between the two following values in my stream:</p>\n\n<ul>\n<li>AVG(value) in SlidingWindow of 1mn</li>\n<li>AVG(value) in SlidingWindow of 1mn in the previous minute</li>\n</ul>\n\n<p>The only thing I can't find in the documentation is how to create a \"delayed\" sliding window, meaning that it begins 2mn before and ends 1mn before the actual time so I can make some calculations such as the rate of change.</p>\n"
    }, {
        "tags": ["rest", "ibm-cloud", "mqtt", "iot"],
        "is_answered": true,
        "view_count": 8166,
        "favorite_count": 4,
        "score": 9,
        "last_activity_date": 1524846613,
        "creation_date": 1455736469,
        "question_id": 35465664,
        "link": "https://stackoverflow.com/questions/35465664/ibm-iot-foundation-when-to-use-mqtt-and-when-to-use-rest-for-event-submission",
        "title": "IBM IoT Foundation: When to use MQTT and when to use REST for event submission?",
        "body": "<p>The IBM IoT Foundation allows devices to submit events to the IBM cloud for consumption and recording.  There appears to be two primary mechanisms to achieve the transmission of events ... MQTT and REST (HTTP POST requests).  Assuming that a project will have sensors with direct TCP connectivity to IBM cloud over the Internet, what might we consider as the potential distinctions between the two technologies?  What factors would case us to choose MQTT or REST as the technology to use?  Are there any substantial performance differences at the final mile at the IBM end that would say that one technology is preferred over another?</p>\n"
    }, {
        "tags": ["nosql", "iot", "metrics", "kairosdb"],
        "is_answered": false,
        "view_count": 71,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1524732305,
        "creation_date": 1519640550,
        "question_id": 48986172,
        "link": "https://stackoverflow.com/questions/48986172/kairodb-metric-names-or-tags-for-iot",
        "title": "KairoDB metric names or tags for IoT",
        "body": "<p>We have a lot of sensors like energymeters and want to store the data using kairosdb. Before we used a simple SQL store, where each sensor has it's own table where each measurement is one row.</p>\n\n<p>a measturement is published to the system from the sensor via a JSON object:</p>\n\n<pre><code>{\n    time: 12363453,\n    volt: 238.33,\n    ampere: 9.3,\n    watts: 29.0,\n}\n</code></pre>\n\n<p>So, for example for two sensors on two devices, we have this in our DB:</p>\n\n<p>Sensor A1234:</p>\n\n<pre><code>id, time, volt, ampere, watts\n12, &lt;unix-ts&gt;, 238.33, 9.3, 29.0\n13, &lt;unix-ts&gt;, 238.21, 9.1, 23.8\n...\n</code></pre>\n\n<p>Sensor B5678:</p>\n\n<pre><code>id, time, volt, ampere, watts\n75, &lt;unix-ts&gt;, 230.12, 3.9, 19.5\n76, &lt;unix-ts&gt;, 234.65, 2.8, 24.5\n...\n</code></pre>\n\n<p>Now, we're investigating what's the best solution to store the same information but using KairosDB instead.</p>\n\n<p>Our goal is to answser some \"questions\" like: \n- the latest volt, watt, ampere \n- what was the average ampere between x (start timestamp) and y (end timestamp)\n- what was the sum watts over all (or a subset) sensors between two dates</p>\n\n<p>and so on.</p>\n\n<p>So, what would be the best approach for choosing metric names and/or tags?</p>\n\n<p>Should we use the sensor-names for the metric names (without tags):</p>\n\n<pre><code>sensors.energy.a1234.volt=238.33\nsensors.energy.a1234.ampere=9.3\nsensors.energy.a1234.watts=29.0\n\nsensors.energy.b5678.volt=230.12\nsensors.energy.b5678.ampere=3.9\nsensors.energy.b5678.watts=19.5\n</code></pre>\n\n<p>Or should we use tags on the same metrics for all sensors:</p>\n\n<pre><code>sensors.energy.volt=238.33, tag: sensor=a1234\nsensors.energy.ampere=9.3, tag: sensor=a1234\nsensors.energy.watts=29.0, tag: sensor=a1234\n\nsensors.energy.volt=230.12, tag: sensor=b5678\nsensors.energy.ampere=3.9, tag: sensor=b5678\nsensors.energy.watts=19.5, tag: sensor=b5678\n</code></pre>\n\n<p>or, are we totally on the wrong way?</p>\n\n<p>Is there a difference regarding the query performance?</p>\n"
    }, {
        "tags": ["mysql", "iot", "sensors"],
        "is_answered": false,
        "view_count": 73,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1524633450,
        "creation_date": 1524601325,
        "question_id": 50010194,
        "link": "https://stackoverflow.com/questions/50010194/mysql-query-time-too-long-in-sensor-timestamped-data-table",
        "title": "MySQL query time too long in sensor timestamped data table",
        "body": "<p>I have a very simple table to log reading from sensors. There's a column for sensor id number, one for sensor reading and one for the timestamp. This column is of SQL type Timestamp. There's a big amount of data in the table, a few million rows.</p>\n\n<p>When I query for all rows before a certain timestamp with a certain sensor id number, sometimes it can take a very long time. If the timestamp is far in the past, the query is pretty fast but, if it's a recent timestamp, it can take up to 2 or 3 seconds.</p>\n\n<p>It appears as if the SQL engine is iterating over the table until it finds the first timestamp that's larger than the queried timestamp. Or maybe the larger amount of queried data slows it down, I don't know.</p>\n\n<p>In any case, I'm looking for design suggestions here, specifically to address to points: why is it so slow? and how can I make it faster?</p>\n\n<p>Is there any design technique that could be applied here? I don't know much about SQL, maybe there's a way to let the SQL engine know the data is ordered (right now it's not but I could order it upon insertion I guess) and speed up the query. Maybe I should change the way the query is done or change the data type of the timestamp column.</p>\n"
    }, {
        "tags": ["iot", "aws-iot", "azure-iot-edge"],
        "is_answered": true,
        "view_count": 770,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1523744071,
        "creation_date": 1519153016,
        "question_id": 48892367,
        "link": "https://stackoverflow.com/questions/48892367/why-did-azure-iot-edge-use-container-how-does-this-approach-compare-to-green-gr",
        "title": "why did azure IoT edge use container, how does this approach compare to green grass core?",
        "body": "<p>Azure IoT edge uses container based modules, while AWS greengrass is process centric. It will be interesting to understand the trade-offs, and how might the two evolve in the future? Would like to hear your opinion.</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 787,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1523601853,
        "creation_date": 1523488342,
        "last_edit_date": 1523488728,
        "question_id": 49785684,
        "link": "https://stackoverflow.com/questions/49785684/can-azure-iot-hub-be-used-to-readget-data-from-some-devices",
        "title": "Can Azure IOT hub be used to read(Get) data from some devices?",
        "body": "<p>In my case I have 1000+ of devices that stores activity inside. I need to send a http get request to this device to get those data in csv or json format and save it in a storage hosted on azure. \nCab IOT hub require data using get request and can it be scheduled to read daily/weekly? \nWhat other azure services would you suggest to facilitated this scheduled reads?</p>\n"
    }, {
        "tags": ["streaming", "spark-streaming", "apache-storm", "iot", "heron"],
        "is_answered": true,
        "view_count": 224,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1523307441,
        "creation_date": 1513372107,
        "last_edit_date": 1523307441,
        "question_id": 47839872,
        "link": "https://stackoverflow.com/questions/47839872/maximum-spout-capacity",
        "title": "Maximum spout capacity",
        "body": "<p>I'm using Heron for performing streaming analytics on IoT data. Currently in the architecture there is only one spout with parallelism factor 1.</p>\n\n<p>I'm trying to benchmark the stats on the amount of data Heron can hold in the queue which it internally uses at spout.</p>\n\n<p>I'm playing around with the method setMaxSpoutPending() by passing value to it. I want to know if there is any limit on the number which we pass to this method?</p>\n\n<p>Can we tweak the parameter method by increasing system configuration or providing more resource to the topology?</p>\n"
    }, {
        "tags": ["raspberry-pi", "iot", "cloudant", "node-red"],
        "is_answered": true,
        "view_count": 86,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1523265347,
        "creation_date": 1523256926,
        "question_id": 49727121,
        "link": "https://stackoverflow.com/questions/49727121/format-sensor-data-as-it-comes-into-ibm-watson",
        "title": "Format sensor data as it comes into IBM Watson",
        "body": "<p>I am still new to IBM Watson. Is there any way that i can format the sensor data that comes into IBM Watson? The issue that i am facing right now is that the timestamp bunch the date and the time together and it poses problems when i try to create certain data visualizations in any data analytics and visualization software. I will make things easier for me splitting the date and time from the timestamp. I am aware that the data is in json format.</p>\n\n<p>In addition, I am using node-red, do let me know if the formatting of data should be done at node red.</p>\n\n<p>Here is my sample sensor data :</p>\n\n<pre><code>{\n   \"_id\": \"04691370-387e-11e8-8cd5-8b3f61628d0d\",\n   \"_rev\": \"1-a4328ecd41d03b8e4ac86de06baf03d2\",\n   \"deviceType\": \"RaspberryPi\",\n   \"deviceId\": \"9074bd\",\n   \"eventType\": \"event\",\n   \"format\": \"json\",\n   \"timestamp\": \"2018-04-05T11:04:12.583+08:00\",\n   \"data\": {\n     \"d\": {\n       \"temperature\": 19.5,\n       \"humidity\": 44,\n       \"heatIndex\": 18.65\n     }\n   }\n }\n</code></pre>\n\n<p>Things that I am using:</p>\n\n<ul>\n<li>Raspberry Pi 3 Model B </li>\n<li>Raspbian for Robots (Dexter Industries) </li>\n<li>GrovePi+  </li>\n<li>GrovePi DHT 11, Light sensor , Sound sensor , UV sensor</li>\n<li>Node Red with all the grovepi+ nodes including nodes for IBM Watson </li>\n<li>IBM Watson , IBM Waston Iot </li>\n<li>Cloudant NoSQL DB</li>\n<li>CData ODBC Driver for Cloudant </li>\n<li>Microsoft Power Bi (subject to change , depends on which software is easier to Adopt)</li>\n</ul>\n"
    }, {
        "tags": ["ios", "objective-c", "mongodb", "iot", "kaa"],
        "is_answered": false,
        "view_count": 79,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1522825225,
        "creation_date": 1522825225,
        "question_id": 49644721,
        "link": "https://stackoverflow.com/questions/49644721/using-the-kaa-server-how-to-save-the-data-from-endpoints-and-how-to-fetch-the-d",
        "title": "Using the kaa server, How to save the data from endpoints and how to fetch the data from mongoDB",
        "body": "<p>I am using the kaa server for Data collections,How to save the data from endpoints and how to fetch the data from mongoDB for objective-c.\nMy idea is using kaa server to save the values in mongoDB and retrieve. </p>\n"
    }, {
        "tags": ["javascript", "charts", "iot", "weather"],
        "is_answered": false,
        "view_count": 266,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1521977574,
        "creation_date": 1521976927,
        "question_id": 49475242,
        "link": "https://stackoverflow.com/questions/49475242/thingspeak-data-visualisation-graphs-for-webpage",
        "title": "Thingspeak data visualisation/ graphs for webpage",
        "body": "<p>i want to make graphs/ charts from my data, that is stored on Thingspeak (its a weather station project using arduino). Now, i want to be able to plot these data into graphs. Thingspeak itself does that, but the graph is non customizable.</p>\n\n<p>What i want do do is to have some buttons and comboboxes to select for example certain time interval (days,weeks,months), be able to point on line in graph and it tels the stats for the one spot etc.</p>\n\n<p>Is there any templates or web services that do that? Or tutorials on how to do that? I want to have these charts embedde to my webpages. Thank you for replies.</p>\n"
    }, {
        "tags": ["c#", "server", "iot", "opc"],
        "is_answered": false,
        "view_count": 1362,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1521735754,
        "creation_date": 1521627977,
        "last_edit_date": 1521735754,
        "question_id": 49403815,
        "link": "https://stackoverflow.com/questions/49403815/creating-an-opc-server-c",
        "title": "Creating an OPC Server C#",
        "body": "<p>I'm trying to dig up information on creating an OPC server to connect a piece of software to an OPC network.</p>\n\n<p>Essentially my customer has hardware connected to an aggregation software, we gather aggregate data from that software's SDK and now the challenge is to make that available to OPC.</p>\n\n<p>There seems to be lots of options all of which are poorly documented online unless you pay to get into their eco systems or they are focused on you buying their specific server implementations. I am happy to pay for a licence but I'm trying to work out what I'm getting into.</p>\n\n<p>I've checked out:</p>\n\n<p><a href=\"https://opcfoundation.org/\" rel=\"nofollow noreferrer\">https://opcfoundation.org/</a> (seems my company needs to be a member to use the examples)</p>\n\n<p><a href=\"https://www.matrikonopc.com/\" rel=\"nofollow noreferrer\">https://www.matrikonopc.com/</a> (seems the generic server toolkit is EOL)</p>\n\n<p><a href=\"http://www.opcconnect.com/freesrv.php\" rel=\"nofollow noreferrer\">http://www.opcconnect.com/freesrv.php</a> (looked through the options there which range from pay to even investigate to ancient or poor quality open source)</p>\n\n<p>Are there free options to explore the possibilities and effort involved in coding up a server?</p>\n\n<p>What are the best paid (or free) options to get the job done quickly?</p>\n\n<p>Should we just bite the bullet and buy into an eco system?</p>\n"
    }, {
        "tags": ["ipc", "embedded-linux", "zeromq", "iot", "dbus"],
        "is_answered": false,
        "view_count": 331,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1521359937,
        "creation_date": 1521318234,
        "last_edit_date": 1521324020,
        "question_id": 49341557,
        "link": "https://stackoverflow.com/questions/49341557/ipc-mechanism-for-iot",
        "title": "IPC mechanism for IOT",
        "body": "<p>I'm developing an IOT platform, running embedded Linux OS.\nI'm considering 2 options for implementing the IPC between applications, ZeroMQ and D-Bus.<br>\nAt first ZeroMQ seems to fit as I can build with its building blocks the exact wanted architecture, but when I read about the ready D-Bus mechanism, suddenly it sounds like I will reinvent the wheel with the ZeroMQ. </p>\n\n<p>Please, advise if for my needs there are any drawbacks for choosing D-Bus over ZeroMQ, and I would like to know what you would choose. </p>\n\n<p>I haven't hard about real time constraints, but I do need the system to be scale-able as the number of apps always grows and they all need to interact with each other.  </p>\n\n<p>The applications interact with each other using blocking &amp; non-blocking request-responses.</p>\n\n<p>Thanks.</p>\n"
    }, {
        "tags": ["apache", "apache-storm", "iot", "azure-eventhub"],
        "is_answered": false,
        "view_count": 43,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1520428346,
        "creation_date": 1520428346,
        "question_id": 49152817,
        "link": "https://stackoverflow.com/questions/49152817/there-is-two-hour-time-difference-between-events-recived-at-iot-hub-and-time-at",
        "title": "there is two hour time difference between events recived at IOT hub and time at which event delivered to datapipeline topology",
        "body": "<p>I am working on storm  topology . When I checked the storm log I can see that there is two hour time difference between time at which events are received at IOT hub and the time on which events delivered to data pipeline my topology.</p>\n\n<p>Can anyone have any idea what might be the probable cause of this time lag.</p>\n"
    }, {
        "tags": ["amazon-web-services", "google-app-engine", "raspberry-pi", "iot"],
        "is_answered": true,
        "view_count": 114,
        "favorite_count": 0,
        "score": -2,
        "last_activity_date": 1518090368,
        "creation_date": 1517905004,
        "question_id": 48638344,
        "link": "https://stackoverflow.com/questions/48638344/architecture-for-iot",
        "title": "architecture for IOT",
        "body": "<p>I'm trying to create a product where customers have remote Raspberry Pi computers. These computers should be able to submit captured images and metadata to cloud file storage once a day (e.g., Google Cloud Storage) </p>\n\n<p>How would you organize such system in terms of security, key storage, protocols, etc? </p>\n\n<p>I reviewed Google Cloud IOT, but this solution seems to be an overkill. maybe I'm wrong. Also, seems like passing images via such solution is too expensive.\nbut I like the part of device management and software upgrades.\nIs it better to use Google Cloud IOT or Amazon IOT or try to develop a more simple solution on my own?</p>\n"
    }, {
        "tags": ["node.js", "protocols", "iot", "solution"],
        "is_answered": true,
        "view_count": 33,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1517531782,
        "creation_date": 1517464018,
        "question_id": 48556125,
        "link": "https://stackoverflow.com/questions/48556125/what-is-major-difference-when-we-want-to-build-iot-solution-if-we-use-middleware",
        "title": "What is major difference when we want to build IoT solution if we use middleware or libraries or custom development?",
        "body": "<p>What is major difference when we want to build IoT solution if we use middleware or libraries or custom development?</p>\n\n<p>Let's imagine that there are so many street lights, camera for illegal parking or some sensors and we should build some solution to integrate. What I found is that they are using different protocol(tcp, serial) and data type(binary, xml, text). Colleague recommend some way like middleware or libraries but I doubt if it is efficient for maintenance or not.  </p>\n"
    }, {
        "tags": ["architecture", "iot", "bigdata"],
        "is_answered": false,
        "view_count": 53,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1516859258,
        "creation_date": 1502173745,
        "last_edit_date": 1506102531,
        "question_id": 45560821,
        "link": "https://stackoverflow.com/questions/45560821/are-there-any-good-bigdata-iot-reference-architectures",
        "title": "Are there any good BigData/IoT reference architectures?",
        "body": "<p>I am looking for some reference architectures for an application that has to be: </p>\n\n<ul>\n<li>geo scalable : allows for ingestion of data from nodes that are\nspread across the globe (possibly like IoT).</li>\n<li>semi-structured data : data that is not all sorted out. scope for new devices to send data that is not yet known</li>\n<li>have a single-pane/portal/api that allows for querying aggregated data from different locations.</li>\n</ul>\n\n<p>(Update):\nSince posting this question, I have been reading through a few presentation and youtube videos. So far I have gathered the following:</p>\n\n<p>Option 1: Use a customized solution stack from a cloud vendor like Amazon, Azure. To quickly bootstrap, there are also consulting firms that specialize in one of these cloud solutions. </p>\n\n<p>Option 2: Build using open-source stacks. Couple of interesting architectures are : SMACK , Lambda. </p>\n\n<p>Any help that you could provide in pointing to the right links/books/blogs or your notes/comments from which I could derive an reference architecture will be greatly appreciated.</p>\n"
    }, {
        "tags": ["google-bigquery", "google-cloud-platform", "iot", "dashboard", "particle-photon"],
        "is_answered": false,
        "view_count": 833,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1516719342,
        "creation_date": 1516719342,
        "question_id": 48404642,
        "link": "https://stackoverflow.com/questions/48404642/real-time-iot-dashboard-using-google-cloud-platform",
        "title": "Real time iot dashboard using Google Cloud Platform",
        "body": "<p>I am looking to build a quick and dirty IoT prototype. I am using particle photons to stream sensor information to the Google Cloud Platform. Inside GCP, I have Pub/Sub->Cloud Functions->BigQuery as my architecture.</p>\n\n<p>I am now looking to build a live/real-time dashboard with the data coming in. \n1. What would be the easiest tool to achieve this? (Google Data Studio won't work as it only produces reports, I need an update every few seconds)?\n2. Would it be wise to create a web app and query my bigquery database every few seconds? I am given to understand this can c</p>\n"
    }, {
        "tags": ["json", "mqtt", "iot", "intel-edison"],
        "is_answered": false,
        "view_count": 97,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1516393009,
        "creation_date": 1461272293,
        "last_edit_date": 1516393009,
        "question_id": 36780335,
        "link": "https://stackoverflow.com/questions/36780335/best-way-to-pass-mpu6050-sensor-data-at-100-hz-from-edison-to-intel-compute-stic",
        "title": "Best way to pass MPU6050 sensor data at 100 Hz from Edison to Intel Compute Stick",
        "body": "<p>Our setup is:</p>\n\n<ol>\n<li>Intel Edison reads raw data from MPU6050 at 100Hz and calibrates</li>\n<li>Intel Compute Stick needs to read this data from Edison. Its connected with USB.</li>\n</ol>\n\n<p>We need to ensure there is no lag as the data needs to be plotted in real time.</p>\n\n<p>To pass data from Edison to Intel Compute Stick we are considering using MQTT but there is an opinion that it may lag due to some internal queue system.</p>\n\n<p>Another option we are looking at is JSON, which could be also cause lagging at 100 readings per second.</p>\n\n<p>Any advise on best protocol to use to communicate the info from Edison boars would be much appreciated, thanks.</p>\n"
    }, {
        "tags": ["amazon-web-services", "amazon-dynamodb", "mqtt", "iot", "aws-iot"],
        "is_answered": false,
        "view_count": 601,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1516178031,
        "creation_date": 1516178031,
        "question_id": 48296716,
        "link": "https://stackoverflow.com/questions/48296716/insert-iot-message-array-as-multiple-rows-in-dynamodb",
        "title": "Insert IoT message array as multiple rows in DynamoDB",
        "body": "<p>I'm starting a new project that involves some IoT devices sending every 5 minutes their status and other info to AWS IoT.</p>\n\n<p>The structure of the message is the following:\n<code>\n{\n    \"SNC\":\"C_SN_15263217541\",\n    \"STATUS\":\"enable\",\n    \"PLANT\":{\n        \"PNAME\":\"nomeimpianto\",\n        \"DVS\":{\n            \"SD\":[{\n                    \"SDSN\":\"LD_SN_15263987543\",\n                    \"TT\":\"30/11/17 4:37 PM\",\n                    \"STATUS\":\"Enable\",\n                    \"TON\":\"3sec\",\n                    \"TOFF\":\"6min\",\n                    \"QTAC\":\"125\",\n                    \"ALCODE\":\"201\",\n                    \"ALDESC\":\"assenza scarico\"\n                },\n                {\n                    \"SDSN\":\"LD_SN_15263987543\",\n                    \"TT\":\"30/11/17 4:39 PM\",\n                    \"STATUS\":\"Enable\",\n                    \"TON\":\"3sec\",\n                    \"TOFF\":\"6min\",\n                    \"QTAC\":\"125\",\n                    \"ALCODE\":\"201\",\n                    \"ALDESC\":\"assenza scarico\"\n                },<br>\n                {\n                    \"SDSN\":\"LD_SN_15263997545\",\n                    \"TT\":\"30/11/17 4:37 PM\",\n                    \"STATUS\":\"Enable\",\n                    \"TON\":\"3sec\",\n                    \"TOFF\":\"6min\",\n                    \"QTAC\":\"125\"<br>\n                },\n                {\n                    \"SDSN\":\"LD_SN_15263997545\",\n                    \"TT\":\"30/11/17 4:39 PM\",\n                    \"STATUS\":\"Enable\",\n                    \"TON\":\"3sec\",\n                    \"TOFF\":\"6min\",\n                    \"QTAC\":\"125\"\n                },<br>\n                {\n                    \"SDSN\":\"LD_SN_15123987543\",\n                    \"TT\":\"30/11/17 4:37 PM\",\n                    \"STATUS\":\"Enable\",\n                    \"TON\":\"3sec\",\n                    \"TOFF\":\"6min\",\n                    \"QTAC\":\"125\"<br>\n                },\n                {\n                    \"SDSN\":\"LD_SN_15123987543\",\n                    \"TT\":\"30/11/17 4:39 PM\",\n                    \"STATUS\":\"Enable\",\n                    \"TON\":\"3sec\",\n                    \"TOFF\":\"6min\",\n                    \"QTAC\":\"125\"<br>\n                }<br>\n            ]\n        }\n    }\n}\n</code></p>\n\n<p>I created a rule that inserts the message on DynamoDB, and it's working nicely, but I'd need to create, for each message received, one row for each item in <code>PLANT.DVS.SD</code>.</p>\n\n<p>My DynamoDB table has as hashkey the field <code>PLANT.DVS.SD[x].SDSN</code> and as sort field <code>PLANT.DVS.SD[x].TT</code>.</p>\n\n<p>I tried with a DynamoDBv2 rule and I managed only to create one row per message with the whole array, but it's not what I'm looking for.</p>\n\n<p>So basically the problem is that I don't know how to structure the SQL statement in the rule definition.</p>\n\n<p>I know that <code>PLANT.DVS.SD</code>'s max length is 12, so the only idea that I've got is to create 12 IoT rules that insert on DynamoDB only the element at a specific position. Although if there is a better way to solve this problem dynamically, it'd be appreciated!</p>\n"
    }, {
        "tags": ["arrays", "database", "mongodb", "time-series", "iot"],
        "is_answered": true,
        "view_count": 850,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1515863412,
        "creation_date": 1515820489,
        "last_edit_date": 1515863412,
        "question_id": 48237121,
        "link": "https://stackoverflow.com/questions/48237121/mongodb-time-series-database-design-hour-minute-second-of-minute-vs-hour-second",
        "title": "MongoDB time series database design: hour/minute/second-of-minute vs hour/second-of-hour?",
        "body": "<p>I'm in an <a href=\"https://github.com/lskk/ecnweb\" rel=\"nofollow noreferrer\">academic research project</a>, and using MongoDB to store time series data for accelerometer values (IoT/telemetry data). The granularity is samples where sample rate can be anything between 1 to 100 Hz. Currently I use one hour of data per document, then there's a 3 dimensional array, first level is minutes, second level is seconds, and third level is samples (double data type). This is inspired by MongoDB for Time Series Data presentations (<a href=\"https://www.mongodb.com/presentations/mongodb-time-series-data-part-1-setting-stage-sensor-management\" rel=\"nofollow noreferrer\">Part 1</a>, <a href=\"https://www.mongodb.com/presentations/mongodb-time-series-data-part-2-analyzing-time-series-data-using-aggregation-framework\" rel=\"nofollow noreferrer\">Part 2</a>).</p>\n\n<p>e.g.</p>\n\n<pre><code>{\n  \"_id\": \"2018011200:4\", /* Jan 12, 2018 hour 00 UTC for sensor 4 */\n  \"z\": [\n    00: [ /* 00h00m */\n      00: [ 0.1, 0.0, -0.1, ... ], /* 00h00m00s */\n      01: [ 0.1, 0.0, -0.1, ... ], /* 00h00m01s */\n      02: [ 0.1, 0.0, -0.1, ... ], /* 00h00m02s */\n      ...\n      59: [ 0.1, 0.0, -0.1, ... ]  /* 00h00m59s */\n    ], ...\n  ]\n}\n</code></pre>\n\n<p>In this way, to get subset of data using <code>$slice</code> can be done only at the minute level, for example if I want to get data from 00:00:00 to 00:00:01, I need to get the whole minute of 00:00 (containing 60 seconds) from MongoDB, then get the second(s) I need in application. Also if I want to get data from 00:00:59 to 00:01:01 then I'll need to get two whole minutes, then in application subset each of them then merge them back. There is a bit of IO waste in this, also some complexity in the app. BTW I have no need to retrieve individual samples, the smallest unit of retrieval (and storage) is a second.</p>\n\n<p>I'm considering a slightly different approach where the hour document is divided directly into array of seconds (as there are 3600 seconds in an hours) and then array of samples. This means to get a data of 5 seconds I will retrieve exactly 5 second of arrays (even if in two different documents, if the time range crosses the hour). There will still be application logic of merging two parts of seconds in different documents, but simpler than the hour/minute/second hierarchy.</p>\n\n<pre><code>{\n  \"_id\": \"2018011200:4\", /* Jan 12, 2018 hour 00 UTC for sensor 4 */\n  \"z\": [\n    0: [ 0.1, 0.0, -0.1, ... ],   /* 00h00m00s */\n    1: [ 0.1, 0.0, -0.1, ... ],   /* 00h00m01s */\n    2: [ 0.1, 0.0, -0.1, ... ],   /* 00h00m02s */\n    ...\n    3599: [ 0.1, 0.0, -0.1, ... ] /* 00h59m59s */\n  ]\n}\n</code></pre>\n\n<p>However, I'm also worried that the alternative approach has weaknesses that I'm not aware of. </p>\n\n<p>Which one do you recommend better? What are potential pitfalls that I need to consider? Or perhaps I should consider another design?</p>\n\n<p>Thank you in advance.</p>\n"
    }, {
        "tags": ["iot", "hyperledger", "quorum"],
        "is_answered": true,
        "view_count": 881,
        "favorite_count": 1,
        "score": 6,
        "last_activity_date": 1515016919,
        "creation_date": 1515005733,
        "question_id": 48083489,
        "link": "https://stackoverflow.com/questions/48083489/hyperledger-sawtooth-vs-quorum-in-concurrency-and-speed",
        "title": "Hyperledger Sawtooth vs Quorum in concurrency and speed",
        "body": "<p>Let's suppose I have <strong>50</strong> machines deployed in multiple locations, every machine has <strong>Linux as OS</strong>.</p>\n\n<p>The machines have not a continued internet connection, for every 2h without connection, they have a 45min period of Wi-Fi connection.</p>\n\n<p>During these 2h the machines are getting data through IoT sensors, stored locally in JSON.</p>\n\n<p>When the 45min. internet connection arrives, the machines send the data into a cloud server for a posterior treatment.</p>\n\n<p>The objective of this question is compare, in this concrete situation, the best DLT for assuring the reliability of the data sent to the Cloud server through multiple concurrent machines.</p>\n\n<p>Thank you very much in advance, and happy new year.</p>\n"
    }, {
        "tags": ["iot", "quantum-computing"],
        "is_answered": true,
        "view_count": 1224,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1513171398,
        "creation_date": 1501742976,
        "last_edit_date": 1513171398,
        "question_id": 45476846,
        "link": "https://stackoverflow.com/questions/45476846/how-is-iota-on-tangle-quantum-proof",
        "title": "How is Iota on Tangle Quantum proof?",
        "body": "<p>I do understand Tangle has a graph based data structure i.e. forming a direct acyclic graph. It is not a merkle tree like a typical blockchain. But I could not figure out this relation makes it quantum proof or not. Is no-mining, and peer verification enough to make a distributed ledger quantum proof?</p>\n"
    }, {
        "tags": ["java", "netty", "iot"],
        "is_answered": true,
        "view_count": 233,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1513075771,
        "creation_date": 1513073881,
        "last_edit_date": 1513074498,
        "question_id": 47770122,
        "link": "https://stackoverflow.com/questions/47770122/netty-share-channels-between-several-netty-servers",
        "title": "netty share channels between several netty servers",
        "body": "<p>I using watches and implementing some custom IoT logic. I'm using netty to receive connection from watch and store some data and everyting working fine with that approach. But I got new requirement to send some command to watch. In order to do that I can send command to already opened channel on Netty but with that approach I need to store somewhere map with <strong>imei</strong> and opened <strong>netty channel</strong>. Are there some solutions for that? Or probably this approach is fine...</p>\n"
    }, {
        "tags": ["json", "database-design", "relational-database", "iot", "non-relational-database"],
        "is_answered": true,
        "view_count": 1870,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1512440924,
        "creation_date": 1512381698,
        "question_id": 47630763,
        "link": "https://stackoverflow.com/questions/47630763/best-iot-database",
        "title": "Best IoT Database?",
        "body": "<p>I have many IoT devices sending data currently to MySQL Database.\n<br />\nI want to port it to some other Database, which will be Open Source and provide me with:</p>\n\n<ul>\n<li>JSON support</li>\n<li>Scalability</li>\n<li>Flexibility to add multiple columns automatically as per payload</li>\n<li>Python and PHP Support</li>\n<li>Extremely Fast Read, Write</li>\n<li>Ability to export at least 6 months of data in CSV format</li>\n</ul>\n\n<p><br />\nPlease revert back soon.\nAny help will be appreciated.</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["mongodb", "database-design", "time-series", "iot"],
        "is_answered": false,
        "view_count": 197,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1512286433,
        "creation_date": 1510973667,
        "question_id": 47361992,
        "link": "https://stackoverflow.com/questions/47361992/mongodb-multi-tenant-design-for-time-series-data",
        "title": "Mongodb Multi tenant design for time series data",
        "body": "<p>I am developing an IOT application for time series data. The application will be provided as SaaS for multi tenants. \nI have decided to go fir mongodb as my database. What should be the design for multi tenant time series data? </p>\n\n<p>Should i go for different document per client or different database per client?  Because i read that different collection per client is considered bad design in MongoDb. </p>\n"
    }, {
        "tags": ["database", "postgresql", "iot", "event-sourcing", "nats.io"],
        "is_answered": true,
        "view_count": 1600,
        "favorite_count": 3,
        "score": 1,
        "last_activity_date": 1511890867,
        "creation_date": 1511870042,
        "question_id": 47530962,
        "link": "https://stackoverflow.com/questions/47530962/using-nats-streaming-server-as-the-primary-data-store-for-iot-position-data",
        "title": "Using NATS Streaming Server as the primary data store for IoT position data?",
        "body": "<p>I have a Mosquitto broker which receives positioning information from remote devices.</p>\n\n<p>I need to store this data somewhere to be processed by other micro-services.</p>\n\n<p>At present, there is a Node.js process which subscribes to the broker, and writes to the Postgres database in batches.</p>\n\n<pre><code>Devices -&gt; Mosquitto -&gt; DB writer -&gt; (source-of-truth) Postgres\n\n(source-of-truth) -&gt; Service A\n                  -&gt; Service B\n</code></pre>\n\n<p>But the problem I see is that now any other service which needs to process this position data, needs to query the Postgres database.</p>\n\n<p>Constraint: This is for on-premise deployments so ideally we want to maintain as little as possible. One VM with a database, and perhaps a link to a customer-maintained database.</p>\n\n<p>An alternative to the database as the source of truth for the sensor data is a Kafka-like event log / event-sourcing approach. Then there would be one subscriber to the broker, and all microservices could read from it, and pick up where they left off if they go down.</p>\n\n<p>Because it is on-premise I want something more lightweight than Kafka, and have found NATS Streaming Server.</p>\n\n<p>Now, the NATS event log can be persisted by configuring it with a data store. It currently supports a simple file store and a SQL store.</p>\n\n<p>Now if I used the SQL store, it seems like a waste of time to store raw messages to database, read from database, then store them again, plus bad for performance. The SQL store interface also has its own batching implemented. I'm not sure how much I trust the file store as the source of truth too.</p>\n\n<p>So, is this a viable approach?</p>\n"
    }, {
        "tags": ["time-series", "iot", "influxdb", "storage-engines"],
        "is_answered": true,
        "view_count": 222,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1511401877,
        "creation_date": 1511254416,
        "question_id": 47408902,
        "link": "https://stackoverflow.com/questions/47408902/insert-data-with-timestamps-from-the-past-to-time-series-db",
        "title": "Insert data with timestamps from the past to time series DB",
        "body": "<p>Let consider Influxdb as an example of TSDB. In outline looks like Influxdb stores data in sorted by time append-only files. But also it claims that it's possible to insert data with random timestamps, not just append. And for IoT world it's a quite usual scenario to occasionally find some data from the past (for example some devices were offline for some time and then get online again) and put this data to the time series db to plot some charts. How influxdb can deal with such scenarios? Will it rewrite the append-only files completely? </p>\n"
    }, {
        "tags": ["iot", "clickhouse"],
        "is_answered": true,
        "view_count": 1552,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1511343512,
        "creation_date": 1510006231,
        "last_edit_date": 1510046077,
        "question_id": 47146682,
        "link": "https://stackoverflow.com/questions/47146682/what-is-the-best-way-to-store-sensor-data-in-clickhouse",
        "title": "What is the best way to store sensor data in Clickhouse?",
        "body": "<p>We have a set of devices and all of them have sensors. All devices have some common set of sensors, but some devices have additional sensors. Every sensor has different discretization level and some sensors could change sometimes very fast, and sometimes could not change for some time. \nFor example, we have DeviceA and have a stream of packets in a form(NULL means that value doesn't change):</p>\n\n<pre><code>Timestamp, Temp, Latitude, Longitude, Speed...\n111, 20, 54.111, 23.111, 10\n112, 20, NULL, NULL, 13\n113, 20, NULL, 23.112, 15\n</code></pre>\n\n<p>And DeviceB:</p>\n\n<pre><code>Timestamp, Temp, Latitude, Longitude, Speed..., AdditionalSensor\n111, 24, 54.111, 23.121, 10 ... 1\n112, 23, 55.111, 23.121, 13 ... 2\n113, 23, 55.111, 23.122, 15 ... 1\n</code></pre>\n\n<p>After some time new sensors could be added to some device.\nEvery sensor could be any of numeric types(Int32, UInt8, Float32)</p>\n\n<p>After that data will be used to calculate: dau, mau, retention, GPS coordinates clustering and so on.</p>\n\n<p>We could simply create some table:</p>\n\n<pre><code>CREATE TABLE Sensors\n(\n        Date Date,\n        Id FixedString(16),\n        DeviceNumber FixedString(16),\n        TimeUtc DateTime,\n        DeviceTime DateTime, \n        Version Int32, \n        Mileage Int32, \n        Longitude Float64, \n        Latitude Float64, \n        AccelX Float64, \n        AccelY Float64, \n        AccelZ Float64\n        ...\n) ENGINE = MergeTree(Date, (DeviceNumber, TimeUtc), 8192);\n</code></pre>\n\n<p>But two problems here: no support for a different set of sensors and sometimes we have null values for some sensor values in case of no changes and it would be great to see last non null value before a timestamp.</p>\n\n<p>The first problem we could solve by creating a table with fields: SensorName, Timestamp, Date, Value. But how to choose correct type? Should we use different tables for different types? \nProbably we need to use graphite engine, unfortunately, I have no any experience with that. So any help is really appreciated. It would be great to have possibility to keep only changed values of any sensor.</p>\n\n<p><strong>Update</strong></p>\n\n<p>I found a way how to deal with null values. We could use \"anyLast\" function to request last received value for a column:</p>\n\n<pre><code>SELECT anyLast(Lights) FROM test where TimeUtc &lt;= toDateTime('2017-11-07 11:13:59');\n</code></pre>\n\n<p>Unfortunately we can't fill all missing values using some kind of overlapping window functions(no support for them in clickhouse). So in case of nullable field aggregate function will use only not null values and in case of non-nullable field all values including zero values will be used and both ways are incorrect. A workaround is to fill null values before insert using select with anyLast values for all null values in a row. </p>\n"
    }, {
        "tags": ["database", "web", "server", "mqtt", "iot"],
        "is_answered": true,
        "view_count": 1336,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1510908416,
        "creation_date": 1499254072,
        "last_edit_date": 1510908416,
        "question_id": 44924983,
        "link": "https://stackoverflow.com/questions/44924983/mqtt-broker-meets-webserver-with-database",
        "title": "MQTT-Broker meets WebServer with Database",
        "body": "<p>I have a question about a combination of a MQTT-Broker and a Webserver.\nPlease check out the image bellow.</p>\n\n<p><img src=\"https://www.allaboutcircuits.com/uploads/articles/Introduction-to-the-MQTT-Protocol-on-NodeMCU-(1).png\" alt=\"MQTT-Broker  &lt;-&gt;  WebServer\"></p>\n\n<p>Is this a good way to save data from different sensors in a database? \nIn the picture the WebServer which communicates with the database is an MQTT Client. The WebServer just subscribe too all topics via #. </p>\n\n<p>Is this scalable? I mean if there are 100.000 sensors out there and all send messages to this one WebServer..?</p>\n"
    }, {
        "tags": ["javascript", "google-maps", "google-maps-api-3", "iot", "thingworx"],
        "is_answered": true,
        "view_count": 319,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1510907794,
        "creation_date": 1510700137,
        "question_id": 47296629,
        "link": "https://stackoverflow.com/questions/47296629/how-to-simulate-position-tracking-of-an-iot-thing",
        "title": "How to simulate position tracking of an iot thing?",
        "body": "<p>I need to simulate the position tracking for my 'IoT thing' in my IoT platform Thingworx. For eg: If I have a car as a Thing, and it has two modes moving and stopped. I need to simulate the changing latitude and longitude for the car when it is in 'moving' state and store the generated coordinates so that they can be later displayed on the Google map widget in the mashup. Similarly, if the car is stopped then it should display the location where it was last stopped. I have the following questions:</p>\n\n<ol>\n<li>How do I achieve this? As in, should I create and use a service or a subscription (with speed values as the trigger)?</li>\n<li>What sort of code snippet will be required for such a functionality?</li>\n<li>Should I employ the Google api?</li>\n<li>Also how do you store the changing values of the lat/long in the variable which has a datatype 'Location'?</li>\n</ol>\n\n<p>Thanks!</p>\n"
    }, {
        "tags": ["python", "linux", "database", "iot", "low-memory"],
        "is_answered": true,
        "view_count": 196,
        "favorite_count": 0,
        "score": -3,
        "last_activity_date": 1510179092,
        "creation_date": 1509692505,
        "question_id": 47090170,
        "link": "https://stackoverflow.com/questions/47090170/database-for-small-iot-devices",
        "title": "Database for small IOT devices",
        "body": "<p>I am looking for a database for IOT device. It should have below features.\n   1. Very low memory usage. (The device will have only 2-4 GB memory for the database).\n   2. Data to be stored in the database is not very large.\n   3. It should be very fast.\n   4. It should support Python.\n   5. It can be installed on Linux.\n   6. It can be NoSQL or relational.</p>\n\n<p>Could you please suggest some databases for this requirements?</p>\n"
    }, {
        "tags": ["classification", "sensors", "iot", "dpi", "traffic"],
        "is_answered": false,
        "view_count": 41,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1509074998,
        "creation_date": 1509068629,
        "last_edit_date": 1509074998,
        "question_id": 46966275,
        "link": "https://stackoverflow.com/questions/46966275/can-you-classify-the-data-that-the-sensor-receives-in-real-time",
        "title": "Can you classify the data that the sensor receives in real time?",
        "body": "<p>Can you classify the data that the sensor receives in real time?</p>\n\n<p>For example, I would like to apply technologies such as DPI (Deep Packet Inspection) and Port based IP traffic classification.</p>\n\n<p>Is this possible?</p>\n"
    }, {
        "tags": ["arduino", "iot", "raspberry-pi3"],
        "is_answered": true,
        "view_count": 133,
        "favorite_count": 0,
        "closed_date": 1508555399,
        "score": -5,
        "last_activity_date": 1508772018,
        "creation_date": 1508535331,
        "last_edit_date": 1508772018,
        "question_id": 46857661,
        "link": "https://stackoverflow.com/questions/46857661/what-is-the-best-scenario-for-implement-detection-of-empty-space-to-park-cars-in",
        "title": "what is the best scenario for implement detection of empty space to park cars in the parking lot?",
        "body": "<p>I have a project to detect the empty space for parked cars in the parking lot.</p>\n\n<p>When a person enters the parking lot with a car, he faces a series of green and red lights above each parking lot.\nThere is a sensor above the head of any park location with 2 green and red lights.</p>\n\n<p>If a car parked in a car, the sensor detects the car and lights its red light to the fullness of the place.\nIf the sensor detects the location empty, that is, the car did not park under it, it lights up its green light as an empty parking spot.\nNow that the driver sees the green lights, he will know where there is space for the park.\nNow, I want to know what is the best scenario in your view to implementing this project? Do you recommend using a central system using a robust firewall or using multiple Arduino modules for each sensor?\nThank you for your guidance.</p>\n"
    }, {
        "tags": ["python", "python-2.7", "python-3.x", "mqtt", "iot"],
        "is_answered": false,
        "view_count": 38,
        "favorite_count": 0,
        "closed_date": 1508399556,
        "score": 2,
        "last_activity_date": 1508396159,
        "creation_date": 1508396159,
        "question_id": 46824205,
        "link": "https://stackoverflow.com/questions/46824205/mqtt-client-does-not-publish-immediately",
        "title": "mqtt_client does not publish immediately",
        "body": "<p>I am trying a trivial networking application where I have multiple nodes communicating using the publish subscribe model in <code>paho-mqtt (mosquitto)</code> in python.</p>\n\n<p>The use case I want to model is a node has a certain job it needs to perform and it should acknowledge the receipt of a request to perform the job, and if the requesting node does not receive the acknowledgement in a certain time, it resubmits the request.</p>\n\n<p>I start performing the job in the method designated for <code>on_message</code> event of <code>mqtt_client</code></p>\n\n<p>I have a following sort of code in my <code>on_message</code> method</p>\n\n<pre><code># mqtt_client is the client object\n# Job represents the job I need to start\n# mqtt_topic is the topic the requester listens\ndef on_message(client, userdata, msg):\n  request = msg.payload\n  if \"PERFORM\" in request:\n    mqtt_client.publish(mqtt_topic, \"ACK\", 2, False)\n    job = Job()\n    job.dispatch()\n</code></pre>\n\n<p>The job definitely requires an arbitrarily long time to complete.\nThe problem I am facing is that the \"ACK\" message is not sent immediately by the <code>mqtt_client</code> before starting the job, and it waits to be send until the job has finished. I tried publishing the \"ACK\" and dispatching the job on a different threads but ended up with same results.</p>\n\n<p>Now because the requester thinks the request has timed out every time, I enter a loop of request and job dispatches. \nI wanted to know if there is any way that I can have the \"ACK\" publish complete before I start with the job.</p>\n"
    }, {
        "tags": ["iot"],
        "is_answered": false,
        "view_count": 19,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1507713057,
        "creation_date": 1507713057,
        "question_id": 46684263,
        "link": "https://stackoverflow.com/questions/46684263/difference-between-data-mining-and-data-minimization",
        "title": "Difference between data mining and data minimization",
        "body": "<p>What is the difference between data mining and data minimization.\nEspecially in the context of Internet of things.Most of the information gathered by me was about the data minimizing role in the internet of things,but no one has discussed their co relation.   </p>\n"
    }, {
        "tags": ["json", "curl", "timestamp", "iot", "thingsboard"],
        "is_answered": true,
        "view_count": 639,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1506615670,
        "creation_date": 1506597706,
        "question_id": 46467901,
        "link": "https://stackoverflow.com/questions/46467901/multiple-timestamps-in-json-body",
        "title": "multiple timestamps in json body",
        "body": "<p>I'm trying to send time stamped key value pairs to the ThingsBoard Demo platform (demo.thingsboard.io). The standard way is to send a timestamp and with some key-value-pairs like so:</p>\n\n<pre><code>{\"ts\":1451649600512, \"values\":{\"key1\":\"value1\", \"key2\":\"value2\"}}\n</code></pre>\n\n<p>My problem is, that I need to process up to 100 acceleration measurements per second and I dont want to send a http post for every x-y-z value-package. Is there a way to send one json body with, lets say, 100 timestamps with corresponding measurements?</p>\n\n<p>I tried it:</p>\n\n<pre><code>{\n\"ts\": 1508695100,\n\"values\": {\n    \"key1\": 34,\n    \"key2\": 26\n},\n\"ts\": 1508695200,\n\"values\": {\n    \"key1\": 38,\n    \"key2\": 29\n}\n</code></pre>\n\n<p>}</p>\n\n<p>There is no error message when pushing this json to ThingsBoard with curl, but only the last timestamp-value-block seems to be recognized by ThingsBoard.</p>\n\n<p>Any suggestions on how to to solve my problem?</p>\n"
    }, {
        "tags": ["google-cloud-platform", "iot", "google-cloud-pubsub", "gcp", "google-cloud-iot"],
        "is_answered": true,
        "view_count": 478,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1506540985,
        "creation_date": 1497532705,
        "last_edit_date": 1506540985,
        "question_id": 44568612,
        "link": "https://stackoverflow.com/questions/44568612/real-time-stream-processing-for-iot-through-google-cloud-platform",
        "title": "Real time stream processing for IOT through Google Cloud Platform",
        "body": "<p>I was concerned about real time stream processing for IOT through GCD pub/sub, Cloud Dataflow and perform analytics through BigQuery.I am seeking help for how to implement this.\n<a href=\"https://i.stack.imgur.com/7RhEw.png\" rel=\"nofollow noreferrer\">Here is the architecture for IOT real-time stream processing</a></p>\n"
    }, {
        "tags": ["graph", "iot", "grafana"],
        "is_answered": false,
        "view_count": 906,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1506084880,
        "creation_date": 1505993591,
        "question_id": 46342873,
        "link": "https://stackoverflow.com/questions/46342873/render-grafana-panel-in-php-without-iframe",
        "title": "Render Grafana panel in PHP without IFRAME?",
        "body": "<p>I tried to embed graph panel of Grafana using PHP's file_get_contents(). But, it doesn't print the graph. How to render Grafana graph in PHP without IFRAME?</p>\n"
    }, {
        "tags": ["hadoop", "hive", "sqoop", "iot", "bigdata"],
        "is_answered": false,
        "view_count": 53,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1505971425,
        "creation_date": 1458813066,
        "last_edit_date": 1506102531,
        "question_id": 36197259,
        "link": "https://stackoverflow.com/questions/36197259/which-is-the-best-approach-for-the-below-use-case-using-hadoop-ecosystem",
        "title": "Which is the best approach for the below use case using Hadoop ecosystem?",
        "body": "<ol>\n<li>I am getting data from more than 1000 sensor units through restservice to my web server, this sensor data I am writing it to single file. </li>\n<li>Through Flume I am storing these sensors data to HDFS. </li>\n<li>Through PIG, Hive and MR I am analysing the data and storing back to HDFS 4. After analytics I am writing back to RDBMS through Sqoop.\nKindly guide me Am I following right approach?</li>\n</ol>\n"
    }, {
        "tags": ["gps", "iot", "bigdata", "nosql"],
        "is_answered": true,
        "view_count": 1435,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1505946692,
        "creation_date": 1414497773,
        "last_edit_date": 1506103282,
        "question_id": 26607875,
        "link": "https://stackoverflow.com/questions/26607875/what-kind-of-nosql-storage-should-we-use",
        "title": "What kind of NoSQL storage should we use?",
        "body": "<p>We are a IoT company that provide services for transportation and logistics companies. As a infrastructure service provider we offer GPS tracking devices to our client.</p>\n\n<p>Although the format of GPS tracking data is very neat (gpsId, longitude, latitude, speed, direction, reportTime, etc), but amount of it is very big. Every device report GPS tracking information per 10 seconds, and we have 100k devices, thus 60*60*24*100000/10 = 864M rows of new data generated every day.</p>\n\n<p>Using the data collected by GPS tracking device of a particular vehicle, client can review the traces of this vehicle within a given time period (for example, last 10 days, will need 60*60*24*10/10 = 86.4K rows of data).</p>\n\n<p>Currently we use MySQL as storage medium, and take advantage of sharding and table partitioning(based on gpsId) of it. But since the data is so big and query on it is so frequent, so I wonder if we can use a NoSQL storage to fit this scenario better?</p>\n\n<p>Historical data is also useful for data analysis.</p>\n\n<p>Appreciated.</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-stream-analytics", "stream-analytics", "bigdata"],
        "is_answered": true,
        "view_count": 358,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1505738912,
        "creation_date": 1485225248,
        "last_edit_date": 1506102531,
        "question_id": 41819084,
        "link": "https://stackoverflow.com/questions/41819084/syntax-issue-in-stream-analytics-query-running-in-azure-invalid-column-name-p",
        "title": "Syntax issue in Stream Analytics Query running in Azure: Invalid column name: &#39;payload&#39;",
        "body": "<p>I am having a syntax issue with my stream analytics query. Following is my Stream Analytics query, where i am trying to get following fields from the events:</p>\n\n<ul>\n<li>Vehicle Id </li>\n<li>Diff of previous and current fuel level (for each\nvehicle), </li>\n<li>Diff of current and previous odometer value (for each\nvehicle).</li>\n</ul>\n\n<p><strong>NON-WORKING QUERY</strong></p>\n\n<pre><code>SELECT input.vehicleId,\nFUEL_DIFF = LAG(input.Payload.FuelLevel) OVER (PARTITION BY vehicleId LIMIT DURATION(minute, 1)) - input.Payload.FuelLevel,\nODO_DIFF = input.Payload.OdometerValue - LAG(input.Payload.OdometerValue) OVER (PARTITION BY input.vehicleId LIMIT DURATION(minute, 1)) \nfrom input\n</code></pre>\n\n<p>Following is one sample input event on which the above query/job is ran on the series of events:</p>\n\n<pre><code>   {\n      \"IoTDeviceId\":\"DeviceId_1\",\n      \"MessageId\":\"03494607-3aaa-4a82-8e2e-149f1261ebbb\",\n      \"Payload\":{\n         \"TimeStamp\":\"2017-01-23T11:16:02.2019077-08:00\",\n         \"FuelLevel\":19.9,\n         \"OdometerValue\":10002\n      },\n      \"Priority\":1,\n      \"Time\":\"2017-01-23T11:16:02.2019077-08:00\",\n      \"VehicleId\":\"MyCar_1\"\n   }\n</code></pre>\n\n<p>Following syntax error is thrown when the Stream Analytics job is ran:</p>\n\n<blockquote>\n  <p><strong><em>Invalid column name: 'payload'. Column with such name does not exist.</em></strong></p>\n</blockquote>\n\n<p>Ironically, the following query works just fine:</p>\n\n<p><strong>WORKING QUERY</strong></p>\n\n<pre><code>SELECT input.vehicleId,\nFUEL_DIFF = LAG(input.Payload.FuelLevel) OVER (PARTITION BY vehicleId LIMIT DURATION(second, 1)) - input.Payload.FuelLevel\nfrom input\n</code></pre>\n\n<p>The only diffrence between WORKING QUERY and NON-WORKING QUERY is number of LAG constructs used. The NON-WORKING QUERY has two LAG constructs, while WORKING QUERY has just one LAG construct.</p>\n\n<p><em>I have referred  Stream Analytics Query Language, they only have basic examples. Also tried looking into multiple blogs. In addition, I have tried using <strong>GetRecordPropertyValue</strong>() function, but no luck. Kindly suggest.</em></p>\n\n<p><em>Thank you in advance!</em></p>\n"
    }, {
        "tags": ["mongodb", "time-series", "sensors", "iot", "bigdata"],
        "is_answered": true,
        "view_count": 842,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1505731433,
        "creation_date": 1483635692,
        "last_edit_date": 1506102531,
        "question_id": 41490635,
        "link": "https://stackoverflow.com/questions/41490635/timestamp-as-key-in-mongodb-as-timeseries-database",
        "title": "Timestamp as Key in mongodb as timeseries database",
        "body": "<p>I want to use mongodb as timeseries database and query via timestamp + id.</p>\n\n<p>Mongodb as shown a way to store data <a href=\"https://www.mongodb.com/blog/post/schema-design-for-time-series-data-in-mongodb\" rel=\"nofollow noreferrer\">here</a>.</p>\n\n<pre><code>{\n  timestamp_hour: ISODate(\"2013-10-10T23:00:00.000Z\"),\n  ID: \u201cSystem1\u201d,\n  values: {\n    0: { 0: 999999, 1: 999999, \u2026, 59: 1000000 },\n    1: { 0: 2000000, 1: 2000000, \u2026, 59: 1000000 },\n    \u2026,\n    58: { 0: 1600000, 1: 1200000, \u2026, 59: 1100000 },\n    59: { 0: 1300000, 1: 1400000, \u2026, 59: 1500000 }\n  }\n}\n</code></pre>\n\n<p>but i have multiple values and for every value a timestamp, its not periodic.\nData has some time a delay or is not coming for days.\nSo i dont want to use 0-24 for my hours and 0-59 for my minutes.\nCan i use instead my measured timestamp? every value in my document has the same timestamp, so if value1 has 50 entries , value2 has also 50 entries and equal timestamp. </p>\n\n<pre><code>     {\n          timestamp_hour: ISODate(\"2013-10-10T23:00:00.000Z\"),\n          ID: \u201cSystem1\u201d,\n          values1: {\n            \"2013-10-10T22:00: {\n         \"2013-10-10T22:01:00.000Z\": 999999, \n         \"2013-10-10T22:02:00.000Z\": 999999,\n         \"2013-10-10T22:03:00.000Z\": 1000000 \n                                },\n            \"2013-10-10T23:00:\": { \n         \"2013-10-10T23:01:00.000Z\": 2000000,\n         \"2013-10-10T23:02:00.000Z\": 2000000, \n                                 },\n          }\n         values2: {\n            \"2013-10-10T22:00: {\n         \"2013-10-10T22:01:00.000Z\": 999999, \n         \"2013-10-10T22:02:00.000Z\": 999999,\n         \"2013-10-10T22:03:00.000Z\": 1000000 \n                                },\n            \"2013-10-10T23:00:\": { \n         \"2013-10-10T23:01:00.000Z\": 2000000,\n         \"2013-10-10T23:02:00.000Z\": 2000000, \n                                 },\n         }\n    }\n</code></pre>\n"
    }, {
        "tags": ["mongodb", "iot", "sharding", "distributed-system", "bigdata"],
        "is_answered": true,
        "view_count": 79,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1505731396,
        "creation_date": 1485127334,
        "last_edit_date": 1506102531,
        "question_id": 41797297,
        "link": "https://stackoverflow.com/questions/41797297/picking-a-shardkey-for-mongodb",
        "title": "picking a shardkey for mongodb",
        "body": "<p>I want to shard my MongoDB database. I have a high insert rate and want to distribute my documents on two shards evenly.</p>\n\n<p>I have considered rangebase sharding, because I have range queries; but I can not find a solution for picking a good shard key.</p>\n\n<pre><code>{\n    Timestamp : ISODate(\"2016-10-02T00:01:00.000Z\"),\n    Machine_ID: \"100\",\n    Temperature:\"50\"\n}\n</code></pre>\n\n<p>If this is my document and I have 100,000 different machines, would the Machine_ID be a suitable shardkey? And if so, how will MongoDB distribute it on the shards, i.e. do i have to specify the shard range myself? like put Machine_ID 0-49,999 on shard A, and 50,000-100,000 on shard B?</p>\n"
    }, {
        "tags": ["java", "iot", "bigdata"],
        "is_answered": false,
        "view_count": 25,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1505537832,
        "creation_date": 1493277028,
        "last_edit_date": 1506102531,
        "question_id": 43650923,
        "link": "https://stackoverflow.com/questions/43650923/open-souce-to-handle-lagre-data-to-create-curve-graph",
        "title": "Open souce to handle lagre data to create curve graph",
        "body": "<p>I here from some one there seems to be a software which can handle large data so that customer can filter data used to create curve graph.</p>\n\n<p>e.g. There has data of the temperature of every minutes from a sensor, and then we want to create a curve graph of one year based on this large data. </p>\n\n<p>Seems this software can filter data based on highest and lowest temperature(will not use all the temperature of every minutes).</p>\n\n<p>Like the way to create curve graph of stock(data of stack is also big, but still can draw a curve graph).</p>\n\n<p>Does anyone know such software? Both business software or open source are ok.</p>\n"
    }, {
        "tags": ["cassandra", "redis", "couchbase", "iot", "nosql"],
        "is_answered": true,
        "view_count": 110,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1505523240,
        "creation_date": 1430939061,
        "last_edit_date": 1506103282,
        "question_id": 30085458,
        "link": "https://stackoverflow.com/questions/30085458/i-am-not-sure-which-nosql-is-suitable-for-my-scenario",
        "title": "I am not sure which NoSQL is suitable for my scenario",
        "body": "<p>I am trying to design create a cloud based system (IaaS) that will gather data from sensors (water pollution related activity) and upon certain events will decide to process the data for a specific sensor. \nData characteristics are:\n1. For each sensor data is being sent once every couple of days (up to 6 times a month)\n2. each sensor reading contains about 5000 events that are encapsulated in 50-100 messages that are sent to the server (such \"session\" takes about 20 minutes where messages are sent every 5 seconds)\n3. I am building the system to handle rate of 30,000 messages per second.\n4. processing of the data shouldn't be real time , I have about 10 minutes once the \"session\" is finished to do the processing.\n5. 90% of the sessions are not interesting and can be thrown away once they are finished. the other 10% have event or event encapsulated in the messages that according to them I need to decide if I need to process the entire session data and send an alert to the sensor that there is a pollution.</p>\n\n<p>I created a tool that generates 5000 messages per second and I am trying to figure out which database would be the most optimal for my scenario.\nThese are the databases I am thinking to try:</p>\n\n<ol>\n<li><p>Cassandra - I will save for each session an in memory collection of keys. the keys are for the messages that are stored in cassandra. Once I detect a message that contains bad readings I will need to pull all of the other messages in the \"session\" and process them (that means 50-100 requests to cassandra). My concern here is about write performance (since I have many read and write operations) + I don't have a good strategy for deleting the 90% not needed sessions.</p></li>\n<li><p>Couchbase - I will save a document for each \"session\" according to sensorID and will append each message to the document. Once I detect a message that contains bad readings I will only need to send one request for the document. My concern here is about the read performance.</p></li>\n<li><p>Redis - use it like cassandra. I assume performance will be the best but I will need to handle the sharding and replication of data myself in order not to reach the memory limit</p></li>\n</ol>\n\n<p>I would love to hear which option would be the most appropriate</p>\n\n<p>thanks</p>\n"
    }, {
        "tags": ["database", "cassandra", "redis", "iot", "nosql"],
        "is_answered": true,
        "view_count": 980,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1505261439,
        "creation_date": 1504961467,
        "last_edit_date": 1506103282,
        "question_id": 46130900,
        "link": "https://stackoverflow.com/questions/46130900/frequently-updated-table-in-cassandra",
        "title": "Frequently Updated Table in Cassandra",
        "body": "<p>I am doing an IoT sensor based project. In this each sensor is sending data to the server in every minute. I am expecting a maximum of 100k sensors in the future.</p>\n\n<p>I am logging the data sent by each sensor in history table. But I have a Live Information table in which latest status of each sensor is being updated. </p>\n\n<p>So I want to update the row corresponding to each sensor in Live Table, every minute. </p>\n\n<p>Is there any problem with this? I read that frequent update operation is bad in cassandra.</p>\n\n<p>Is there a better way? </p>\n\n<p>I am already using Redis in my project for storing session etc. Should I move this LIVE table to Redis?</p>\n"
    }, {
        "tags": ["amazon-web-services", "iot"],
        "is_answered": false,
        "view_count": 163,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1505198746,
        "creation_date": 1505198746,
        "question_id": 46169672,
        "link": "https://stackoverflow.com/questions/46169672/aws-iot-to-dynamodb-returns-encrypted-mqtt-messages",
        "title": "Aws IOT to dynamoDB - returns encrypted MQTT messages",
        "body": "<p>My IOT device populates the following data:</p>\n\n<pre><code>12-09-2017 12:05:26PeopleIn: 2\n</code></pre>\n\n<p>and this gets streamed in AWS IOT server when I subscribe to the topic.  When I export a message as csv I get the following getting exported: </p>\n\n<pre><code>format,payload,qos,timestamp,topic\nstring,12-09-2017 12:05:26PeopleIn: 2,0,1505190320098,TestSite\n</code></pre>\n\n<p>I am trying to configure AWS DynamoDB to store the above data as a table.  In AWS IOT rules setting I have provided the following:</p>\n\n<pre><code>Attribute = *\ntopicFilter = # (i have also tried with TestSite)\nSelected Action = Insert a message into DynamoDB table\n</code></pre>\n\n<p>I created a dynamoDB table with the following settings:</p>\n\n<pre><code>Primary Partition Key = timestamp (Number)\n</code></pre>\n\n<p>I have set any sort key. Upon setting up the rule, the data gets populated into the dynamoDB.  However, the timestamp is populated as Unix Timestamp and the data is populated in an encrypted format.  The following is the output that I am getting in AWS DynamoDB.</p>\n\n<pre><code>timestamp     data_raw\n1505198126899 MTItMDktMjAxNyAxMjowNToyNlBlb3BsZUluOiAy\n</code></pre>\n\n<p>Not sure what I am missing.  What I ideally want is the timestamp + payload.  The desired output would be:</p>\n\n<pre><code>timestamp       data\n1505198126899   12-09-2017 12:05:26PeopleIn: 2\n</code></pre>\n\n<p>Can someone help?  </p>\n"
    }, {
        "tags": ["amazon-web-services", "neo4j", "amazon-dynamodb", "aws-lambda", "iot"],
        "is_answered": false,
        "view_count": 154,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1504295782,
        "creation_date": 1504295782,
        "question_id": 46007205,
        "link": "https://stackoverflow.com/questions/46007205/how-do-i-perform-spatial-data-analysis-to-kinesis-streams",
        "title": "How do I perform spatial data analysis to kinesis streams?",
        "body": "<p>I'm quite new to AWS products and have been learning from their tutorials a lot. Recently I'm building an end-to-end IoT application to do near real time spatial analytics. So the scenario is that we will deploy several sensors in each room/space and examine the spatial correlation across all the sensor measurements every once a while (maybe every 10s). The iot sensor data payload would look like this:</p>\n\n<p>{\"roomid\": 1,\"sensorid\":\"s001\", \"value\": 0.012}</p>\n\n<p>Kinesis seems powerful for real time analytics, so Im sending data from IoT to kinesis stream, and then using kinesis analytics to perform data aggregation using tumbling window and enrich measurements with sensor locations. The processed data to be sent out would look like this now[pic]:</p>\n\n<p><a href=\"https://i.stack.imgur.com/6nl2g.png\" rel=\"nofollow noreferrer\">kinesis analytics data</a></p>\n\n<p>My plan was to send data from kinesis analytics to kinesis stream and then invoke lambda function to do spatial analysis. It seems that the records with same rowtime(process time) will be sent within a single event in kinesis stream. But I want to apply lambda function to each room separately, how can I separate these records from kinesis stream? (Am I using the wrong strategy?)</p>\n\n<p>I'm also looking for a data storage solution for the sensor data. Any advice? There are many mature time-series DB but I want to find the best suitable DB for spatial analysis. Shall I look into graph DB, like Neo4j? </p>\n\n<p>Thanks, \nSasa</p>\n"
    }, {
        "tags": ["database", "azure", "amazon-web-services", "cloud", "iot"],
        "is_answered": false,
        "view_count": 54,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1504276020,
        "creation_date": 1504273033,
        "question_id": 46001469,
        "link": "https://stackoverflow.com/questions/46001469/iot-cloud-how-to-send-data-from-mcu-based-hardware-onto-iaas-saas-cloud-service",
        "title": "IOT Cloud: How to send data from MCU based hardware onto IAAS/SAAS cloud services?",
        "body": "<p>I have a large variable data set between 50MB to 1GB to be sent to cloud services AWS/ AZURE etc from MCU based systems in a single request on a daily basis. As far as I know the cost associated to send such large amount of data to AWS IOT/ AZURE would be extremely high for a small company as they consider a single packet equivalent to 512kb of data.</p>\n\n<p>I'm looking for suggestions as to what could be my alternatives - whether using simple cloud database (like digitalocean etc) be cost efficient for me. But then if I use non IOT specific cloud services then how could I send my data from MCU upto database.</p>\n"
    }, {
        "tags": ["python", "database", "web", "iot", "robotics"],
        "is_answered": false,
        "view_count": 312,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1504016662,
        "creation_date": 1504015348,
        "question_id": 45941162,
        "link": "https://stackoverflow.com/questions/45941162/how-would-one-connect-an-sql-db-securely-to-an-external-client",
        "title": "How would one connect an SQL db securely to an external client?",
        "body": "<p>I'm attempting to connect a database, located on a web server, to a robot but I do not know how to connect the database to the robot. I would like the robot to run SELECT and UPDATE queries from the robot. The other issue is that I do not intend on using C-languages or Java; I plan on using python in the main control system.</p>\n\n<p>I do know:\nPHP\nVBScript\nBatch\nPython</p>\n\n<p>If anyone knows how to connect the DB to a bot it would be a great help.</p>\n"
    }, {
        "tags": ["arduino", "iot"],
        "is_answered": true,
        "view_count": 78,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1503680921,
        "creation_date": 1503311027,
        "question_id": 45794723,
        "link": "https://stackoverflow.com/questions/45794723/custom-circuit-fabrication-for-modified-arduino-based-iot",
        "title": "Custom circuit fabrication for modified arduino based IoT",
        "body": "<p>i am learning arduino and want to build a circuit specific to my need with bluetooth, small factor etc. I have created a design in eagal. Is it possible to get is printed with components. what is the beast place to do that.</p>\n"
    }, {
        "tags": ["database", "cassandra", "etl", "iot"],
        "is_answered": true,
        "view_count": 678,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1503052000,
        "creation_date": 1502892948,
        "question_id": 45716011,
        "link": "https://stackoverflow.com/questions/45716011/what-is-the-best-free-etl-to-extract-data-from-cassandra-3-10",
        "title": "What is the best free ETL to extract Data from Cassandra 3.10 ?",
        "body": "<p>I want to extract Data from my database Cassandra, and do some transformations.</p>\n"
    }, {
        "tags": ["apache-spark", "iot"],
        "is_answered": false,
        "view_count": 32,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1502397052,
        "creation_date": 1502397052,
        "question_id": 45622964,
        "link": "https://stackoverflow.com/questions/45622964/are-there-patterns-for-lookbehind-filters-in-apache-spark",
        "title": "Are there patterns for &quot;lookbehind filters&quot; in Apache Spark?",
        "body": "<p>I stumbled at a couple of workloads which seem to require filtering data with \"lookback\" capability - mainly in IoT scenarios, where sensors can produce garbage data, and to detect that it's necessary to look at the previous record of that sensor.</p>\n\n<p>Spark's <code>filter()</code> operation is obviously \"element-only\" - in fact, the RDD as a whole can't know the order of the elements you want it to look behind on. So another approach is needed.</p>\n\n<p>My naive approach would involve keying the RDD on the sensor, repartitioning it so that keys and partitions become one and the same, and sorting all elements for the keys/partition so that they are in temporal order. Then we can filter with a user function and maybe emit the data back so that the rest of the pipeline can deal with it as it wishes.</p>\n\n<p>However, this looks heavyweight and likely inefficient. Is there a more idiomatic way?</p>\n\n<p><strong>Summary: Is there a Spark-related design pattern to deal with filtering tasks which need to \"lookback\" at the previous element of a sequence?</strong></p>\n"
    }, {
        "tags": ["iot", "kaa"],
        "is_answered": false,
        "view_count": 36,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1502105679,
        "creation_date": 1502039960,
        "question_id": 45534619,
        "link": "https://stackoverflow.com/questions/45534619/postgres-on-kaa-cluster-nodes-dont-sync-with-each-other",
        "title": "Postgres on kaa cluster nodes dont sync with each other",
        "body": "<p>I have setup kaa cluster with two nodes.\nThe postgres on second node does not sync with the first one, as i add any schema or sdk. Do I need to manually setup replication between postgres.\nOr kaa handles this by itself, if it is so then why my second node is not in sync with the first.</p>\n\n<blockquote>\n  <p>admin-dao.properties</p>\n\n<pre><code>jdbc_url=jdbc:postgresql://192.168.1.21:5432,192.168.1.22:5432/kaa\n</code></pre>\n  \n  <p>sql-dao.properties</p>\n\n<pre><code>jdbc_host_port=192.168.1.21:5432,192.168.1.22:5432\n</code></pre>\n</blockquote>\n\n<p>Thanks<br/>\nRizwan</p>\n"
    }, {
        "tags": ["wso2", "metadata", "iot", "wso2iot"],
        "is_answered": false,
        "view_count": 33,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1499950953,
        "creation_date": 1499883790,
        "question_id": 45065027,
        "link": "https://stackoverflow.com/questions/45065027/which-database-dbms-is-approved-and-recommended-for-storing-wso2-iot-server-3",
        "title": "Which database (DBMS) is approved and recommended for storing WSO2 IOT Server 3.0 metadata?",
        "body": "<p>Which database (DBMS) is approved and recommended for storing WSO2 IOT Server 3.0 metadata?</p>\n\n<p>Is it recommended to use a database to store WSO2 IOT Server metadata?</p>\n\n<p>Has anyone done this setup? You have already set up a database to store the metadata of the\nWSO2 IOT</p>\n"
    }, {
        "tags": ["iot"],
        "is_answered": false,
        "view_count": 648,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1499875499,
        "creation_date": 1499850664,
        "question_id": 45053340,
        "link": "https://stackoverflow.com/questions/45053340/design-productivity-gap-in-ics-and-internet-of-things",
        "title": "Design Productivity Gap in ICs and Internet of things",
        "body": "<p>The complexity of Integrated Circuits is increasing exponentially with time which in turn make the number of available transistors to grow fast within a chip. This increase in number of transistors in an IC is way more than the productive design need. The situation raised due this problem is a well-known design productivity gap.Will  IoT and Mobile devices significantly decrease this design productivity gap?</p>\n"
    }, {
        "tags": ["apache-kafka", "iot", "flume"],
        "is_answered": true,
        "view_count": 251,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1498763665,
        "creation_date": 1498677415,
        "last_edit_date": 1498678561,
        "question_id": 44810821,
        "link": "https://stackoverflow.com/questions/44810821/real-time-event-processing",
        "title": "Real Time event processing",
        "body": "<p>I really want to get an architectural solution for my below scenario.</p>\n\n<p>I have a source of events (Say sensors in oil wells , around 50000 ), that produces events to a server. At the server side I want to process all these events in such a way that , the information from the sensors about latest humidity, temperature,pressure ...etc will be stored/updated to a database.</p>\n\n<p>I am confused with flume or kafka. </p>\n\n<p>Can somebody please address my simple scenario in architectural terms. </p>\n\n<p>I don't want to store the event somewhere, since I am already updating the database with latest values. </p>\n\n<p>Should I really need spark , (flume/kafka) + spark , to meet the processing side?.</p>\n\n<p>Can we do any kind of processing using flume without a sink?</p>\n"
    }, {
        "tags": ["mongodb", "cassandra", "iot", "nosql"],
        "is_answered": true,
        "view_count": 588,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1498388207,
        "creation_date": 1478709467,
        "last_edit_date": 1506103282,
        "question_id": 40511870,
        "link": "https://stackoverflow.com/questions/40511870/mongodb-cassandra-or-maybe-any-other-database-which-is-the-best-for-iot",
        "title": "MongoDB, Cassandra or maybe any other database which is the best for IoT?",
        "body": "<p>My project is to build a product that has some PM 2.5 sensor, temperature sensor, humidity and I must decide what database I should choose. That product connects to the server and sent data to it.\nI'm thinking about MongoDB and Cassandra. Don't know what is suitable for my project. Maybe my project is not big enough to build with Cassandra.\nI want to have some advice. And if it has something better, you can let me know.</p>\n"
    }, {
        "tags": ["apache-spark", "amazon-dynamodb", "mqtt", "iot"],
        "is_answered": false,
        "view_count": 321,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1498283234,
        "creation_date": 1498206918,
        "last_edit_date": 1498212376,
        "question_id": 44716773,
        "link": "https://stackoverflow.com/questions/44716773/mqtt-spark-streaming-and-dynamodb",
        "title": "Mqtt + Spark streaming and dynamodb",
        "body": "<p>I am trying to design an <code>IoT</code> platform using the above mentioned technologies. I would be happy if someone can comment on the architecture, if its good and scalable !</p>\n\n<p>I get <code>IoT</code> sensor data through <code>mqtt</code> which I will receive through spark streaming( There is a mqtt connector for spark streaming which does it). I only have to subscribe to the topics and there is a third party server which publishes <code>IoT</code> data to the topic. </p>\n\n<p>Then I parse the data , and insert in AWS DynamoDB . Yes whole setup will run on AWS. </p>\n\n<p>I may have to process/transform the data in future depending on the <code>IoT</code> use cases so I thought spark might be useful . Also I have heard spark streaming is blazing fast. </p>\n\n<p>It's a simple overview and I am not sure if its a good architecture. Will it be a overkill to use spark streaming ? Are there other ways to directly store data on DynamoDB received from <code>mqtt</code> ?</p>\n"
    }, {
        "tags": ["azure", "iot", "azure-stream-analytics", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 949,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1498055624,
        "creation_date": 1498027818,
        "question_id": 44668718,
        "link": "https://stackoverflow.com/questions/44668718/what-are-the-different-ways-to-fetch-data-from-iot-hubmicrosoft-azure",
        "title": "what are the different ways to fetch data from IoT Hub(microsoft azure)?",
        "body": "<p>What are the ways to store refined/filtered data from Microsoft azure IoT Hub at another servers/cloud storages(like AWS)? </p>\n"
    }, {
        "tags": ["json", "iot"],
        "is_answered": false,
        "view_count": 105,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1497505430,
        "creation_date": 1497473622,
        "question_id": 44554558,
        "link": "https://stackoverflow.com/questions/44554558/how-to-interpret-the-data-logger-format-of-yoctopuce-sensors",
        "title": "How to interpret the data logger format of Yoctopuce sensors?",
        "body": "<p>I'd like to use Yoctopuce's Virtualhub rest api to get data from the sensors data loggers. The sensor manuals mention how to get the data, but not how to interpret it. I've been trying to make sense from it, but I can't get something 100% conclusive.</p>\n\n<p>Underneath an example of how far I got. Maybe somebody in here sees where I'm wrong. Or maybe somebody already went through the effort.</p>\n\n<hr>\n\n<p>My example is the light sensor. I've cleared the datalogger's data and started logging once a minute. The value is a number measured in Lux.</p>\n\n<p>First we need the summary data from the data logger. This is done by calling <strong><a href=\"http://hub:4444/bySerial/LIGHTMK1-2ABABA/dataLogger.json\" rel=\"nofollow noreferrer\">http://hub:4444/bySerial/LIGHTMK1-2ABABA/dataLogger.json</a></strong>. Which yields a json document as below:</p>\n\n<pre><code>[{\"id\":\"lightSensor\",\"unit\":\"lx\",\"calib\":\"0,\",\"cal\":\"*\",\"streams\":\n[{\"run\":0,\"utc\":1497384000,\"dur\":1380,\"freq\":\"1/m\",\"val\":[0,11,15]}\n,{\"run\":1,\"utc\":1497384360,\"dur\":1800,\"freq\":\"1/m\",\"val\":[13,15,16]}\n,{\"run\":2,\"utc\":1497387000,\"dur\":600,\"freq\":\"1/m\",\"val\":[14,16,17]}\n,{\"run\":2,\"utc\":1497387600,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,1,18]}\n,{\"run\":2,\"utc\":1497391200,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,0,0]}\n,{\"run\":2,\"utc\":1497394800,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,0,0]}\n,{\"run\":2,\"utc\":1497398400,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,0,0]}\n,{\"run\":2,\"utc\":1497402000,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,0,0]}\n,{\"run\":2,\"utc\":1497405600,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,0,0]}\n,{\"run\":2,\"utc\":1497409200,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,1,12]}\n,{\"run\":2,\"utc\":1497412800,\"dur\":3600,\"freq\":\"1/m\",\"val\":[0,1,2]}\n,{\"run\":2,\"utc\":1497416400,\"dur\":3600,\"freq\":\"1/m\",\"val\":[2,10,18]}\n,{\"run\":2,\"utc\":1497420000,\"dur\":780,\"freq\":\"1/m\",\"val\":[17,17,19]}\n...etc...\n]}]\n</code></pre>\n\n<p>The first line is pretty obvious - the name of the sensor (lightSensor) and the symbol for its unit of measure (lx). The <strong>streams</strong> seem to be holding the data. Unfortunately I haven't been able to find out what streams represent. Neither what the <strong>run</strong> number represents. Elements are being added while time passes. For the remainder I presume <strong>utc</strong> stands for the start utc time stamp of the data. Duration in seconds for <strong>dur</strong>; which is confirmed in a later stage. And <strong>freq</strong> is indeed once a minute. The property <strong>val</strong> seem to represent a summarized min/avg/max triplet.</p>\n\n<p>Next, the detail can be requested by providing the name of the sensor and one of the exact utc values. For example calling <strong><a href=\"http://hub:4444/bySerial/LIGHTMK1-2ABABA/dataLogger.json?id=lightSensor&amp;utc=1497387000\" rel=\"nofollow noreferrer\">http://hub:4444/bySerial/LIGHTMK1-2ABABA/dataLogger.json?id=lightSensor&amp;utc=1497387000</a></strong> yields the json document</p>\n\n<pre><code>[[16,16,16]\n,[16,16,16]\n,[15,16,16]\n,[14,15,15]\n,[14,15,15]\n,[14,15,15]\n,[15,16,17]\n,[14,17,17]\n,[17,17,17]\n,[16,17,17]\n]\n</code></pre>\n\n<p>At first sight this is an array of min/avg/max triplets. Which confirms the assumption <strong>dur</strong> represents a duration in seconds. Because there are 10 elements, one for each minute. Which represents 600 seconds.</p>\n\n<p>I assume therefor each triplet time stamp is the utc value + (index * 60). This all seems to make sense. To the exception of time overlaps.</p>\n\n<p>The first summary element starts at utc 1497384000 and lasts 1380 seconds. Which would mean the next element should start at utc 1497385380. However, it starts at utc 1497384360. Which is an <em>earlier</em> time stamp ! There is an overlap of 1020 seconds. In fact, when the run number is incremented, there always seem to be an overlap.   </p>\n\n<p>I was hoping for the overlapping values to be identical. Which they aren't. They are in fact pretty different. So I can not ignore them.</p>\n\n<p>This all puts a doubt on how to interpret the data. Should all the triplets be considered ? Should the overlapping be overwritten by the values of the next elements ? Or the inverse ? Maybe they should be summed ?</p>\n\n<p>The problem is that I can't find a way to interpret the data that makes sense beyond reasonable doubt. </p>\n"
    }, {
        "tags": ["cloud", "kubernetes", "scalability", "iot"],
        "is_answered": false,
        "view_count": 66,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1497377324,
        "creation_date": 1497377324,
        "question_id": 44528629,
        "link": "https://stackoverflow.com/questions/44528629/autoscaling-with-kubernetes",
        "title": "Autoscaling with kubernetes",
        "body": "<p><h2>Objective</h2><br>\nA scalable/resilient IoT platform with potentially thousands of devices.\nWe are using kubernetes to deploy this chain:<br></p>\n\n<pre><code>kafka --&gt; logstash --&gt; elasticsearch\n</code></pre>\n\n<h3>Choice 1</h3>\n\n<p>We start with a static configuration and manually scale up as demand grows.<br>\nPros: simple and works.<br>\nCons: requires human intervention.<br></p>\n\n<h3>Choice 2</h3>\n\n<p>We build a software that monitors the different components, and automatically scale up when demand grows.<br>\nPros: Does not require human intervention.<br>\nCons: Complex.<br></p>\n\n<p>It is complex because too many case arise, for example:<br></p>\n\n<ul>\n<li>Kafka: Simply adding a new broker does distribute load, we have to create a new partition for a set of topics, or we initially create enough partitions then we reassign partitions. </li>\n<li>Elasticsearch: If we want to distribute read load, we must add a data node, increase the number of replicas. If we want to distribute the write load we must use sharding. Data is split into shards, the number of shards cannot be altered after index creation, we can bypass this limitation by the use of aliases.<br></li>\n</ul>\n\n<p><br>\nAutoscaling also involve interaction with the cloud provider to automatically add resources.<br></p>\n"
    }, {
        "tags": ["azure", "iot", "azure-stream-analytics", "azure-iot-hub"],
        "is_answered": false,
        "view_count": 91,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1496449262,
        "creation_date": 1496060402,
        "last_edit_date": 1496063200,
        "question_id": 44242353,
        "link": "https://stackoverflow.com/questions/44242353/azure-iot-data-warehouse-updates",
        "title": "Azure IoT data warehouse updates",
        "body": "<p>I am building Azure IoT solution for my BI project. For now I have an application that once per set time window sends a .csv blob to Azure Blob Storage with incremental number in name. So after some time I will have in my storage files such as 'data1.csv', 'data2.csv', 'data3.csv', etc.</p>\n\n<p>Now I will need to load these data into a database which will be my warehouse with the use of Azure Stream Analytics job. The issue might be that .CSV files will have overlapping data. They will be send every 4h and contain data for past 24h. I need to always read only last file (with highest number) and prepare lookup so it properly updates data in the warehouse. What will be the best approach to make Stream Analytics read only latest file and for updating records in DB?</p>\n\n<p>EDIT:\nTO clarify - I am fully aware that ASA is not capable of being an ETL job. My question is what would be best approach for my case with using IoT tools</p>\n"
    }, {
        "tags": ["java", "amazon-web-services", "asynchronous", "amazon-dynamodb", "iot"],
        "is_answered": false,
        "view_count": 96,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1493569838,
        "creation_date": 1484002443,
        "last_edit_date": 1493569838,
        "question_id": 41558348,
        "link": "https://stackoverflow.com/questions/41558348/how-to-update-servlet-asynchronously-upon-reading-data-from-aws-dynamodb",
        "title": "How to update servlet asynchronously upon reading data from AWS DynamoDB?",
        "body": "<p>I am working on a IoT project where AWS IoT service is used to store a record in a DynamoDB table whenever data is retrieved from sensor (connected to RPI) . </p>\n\n<p>I am able to connect to DynamoDB from my Java code and fetch the data, as expected, but I'm trying to go further and accomplish the following:</p>\n\n<ul>\n<li>How can I get <em>data in real time</em> ? </li>\n<li>How can I show sensor data in a web app, as a chart that gets <em>updated once-per-second whenever the sensor has available data</em>. In other words, when a new record is added in DynamoDB, I want it to be shown in my Servlet.</li>\n</ul>\n\n<p>Is there anyway to make this communication asynchronous (sockets for instance)?</p>\n\n<p>I have searched on DynamoDB streams but I don't see how to open communication with AWS from my program. </p>\n"
    }, {
        "tags": ["bluetooth", "bluetooth-lowenergy", "sensors", "iot", "spp"],
        "is_answered": false,
        "view_count": 487,
        "favorite_count": 1,
        "score": 1,
        "last_activity_date": 1493153277,
        "creation_date": 1493153277,
        "question_id": 43620652,
        "link": "https://stackoverflow.com/questions/43620652/how-to-connect-to-multiple-bluetooth-sensors-to-a-device",
        "title": "How to connect to multiple bluetooth sensors to a device",
        "body": "<p>I have multiple bluetooth sensors (20+) that I need to connect to whatever central device(PC, Phone, ...) via Bluetooth to collect the transmitted data.</p>\n\n<p>I need to keep the connections active with the sensors because it is broadcasting data 20 times/second.</p>\n\n<p>I already thought about using RaspberryPi to setup a Master-Slave pattern and enable multi Bluetooth connections, but I was wondering if there is a more elegant and simple way to do this.</p>\n\n<p>The only objective is to collect the data simultaneously from the sensors. I have a no limitation for the \"collector device\" for the technology.</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["apache-spark", "pyspark", "spark-streaming", "iot", "amazon-kinesis"],
        "is_answered": false,
        "view_count": 189,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1492888151,
        "creation_date": 1492873847,
        "last_edit_date": 1492888151,
        "question_id": 43560807,
        "link": "https://stackoverflow.com/questions/43560807/pyspark-streaming-from-kinesis-kills-heap",
        "title": "pyspark streaming from kinesis kills heap",
        "body": "<p>Running a sample application streaming data from kinesis. I did not get why this application uses so much heap and crashes.</p>\n\n<p>Here is the code :</p>\n\n<pre><code>from __future__ import print_function\nimport sys\n\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\nfrom pyspark.sql.session import SparkSession\nfrom datetime import datetime\n\n# function declaration\ndef isDfEmpty(df):\n\n    try:\n        if not df.take(1) :\n            return True\n\n    except Exception as e:\n        return True\n\n    return False\n\n# function declaration\ndef mergeTable(df):\n    print(\"b:mergeTable\")\n    print(str(datetime.now()))\n\n    try:\n        global refDf\n\n        if isDfEmpty(df) :\n            print(\"no record, waiting !\")\n        else :\n            if(isDfEmpty(refDf)) :\n                refDf = df\n            else :\n                print(\"        before count %s\" % refDf.count())\n                refDf = df.unionAll(refDf)\n                print(\"        after count %s\" % refDf.count())\n\n    except Exception as e:\n        print(e)\n    print(str(datetime.now()))\n    print(\"e:mergeTable\")   \n\n# function declaration\ndef doWork(df):\n    print(\"b:doWork\")\n    print(str(datetime.now()))\n\n    try:\n        mergeTable(df)\n    except Exception as e:\n        print(e)\n    print(str(datetime.now()))\n    print(\"e:doWork\")\n\n# function declaration\ndef sensorFilter(sensorType, rdd):\n    df = spark.read.json(rdd.filter(lambda x : sensorType in x))\n    doWork(df)\n\n\ndef printRecord(rdd):\n    print(\"========================================================\")\n    print(\"Starting new RDD\")\n    print(\"========================================================\")\n    sensorFilter(\"SensorData\", rdd)\n\nrefDf = None    \n\nif __name__ == \"__main__\":\n    reload(sys)  \n    # sys.setdefaultencoding('utf-8')\n\n\n\n    if len(sys.argv) != 5:\n        print( \"Usage: dump.py &lt;app-name&gt; &lt;stream-name&gt; &lt;endpoint-url&gt; &lt;region-name&gt;\", file=sys.stderr)\n        sys.exit(-1)\n\n    spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n    sc = spark.sparkContext\n\n    # sc = SparkContext(appName=\"PythonStreamingKinesisWordCountAsl\")\n    ssc = StreamingContext(sc, 10)\n    appName, streamName, endpointUrl, regionName = sys.argv[1:]\n    dstream = KinesisUtils.createStream(ssc, appName, streamName, endpointUrl, regionName, InitialPositionInStream.LATEST, 10)\n    dstream.foreachRDD(printRecord)\n    ssc.start()\n    ssc.awaitTermination()\n</code></pre>\n\n<p>After a time the spark application slowed down due to heap usage. But when i comment out the lines, heap usage decrease to normal levels.(According to SparkUI)</p>\n\n<pre><code>print(\"        before count %s\" % refDf.count())\nprint(\"        after count %s\" % refDf.count())\n</code></pre>\n\n<p>I am really new with pyspark and trying to get what is going on. \nMerging on data frame continuously may explode the memory of course but the problem of heap occurs very beginning. </p>\n\n<p>EDIT\nEnvironment : Tried on single ubuntu and on cents VM hosted by macOS nothing changed.</p>\n"
    }, {
        "tags": ["cassandra", "database-schema", "iot", "spark-cassandra-connector", "cassandra-cli"],
        "is_answered": true,
        "view_count": 94,
        "favorite_count": 0,
        "score": 3,
        "last_activity_date": 1492491663,
        "creation_date": 1492434019,
        "last_edit_date": 1492491663,
        "question_id": 43452010,
        "link": "https://stackoverflow.com/questions/43452010/no-sql-shema-design-backup-strategy",
        "title": "No SQL Shema Design/Backup Strategy",
        "body": "<p>We are using cassandra for IOT based application. Currently we are receiving 10 GB data in every day. We are storing all data into single table in Cassandra in the way of time series model. What is the best approach keeping the data in single table or multiple table(year,monthly). </p>\n\n<p>schema:</p>\n\n<pre><code>CREATE TABLE SensorData (\n    cid text,\n    event_date date,\n    event_time timestamp,\n    data text,\n    device_id text,\n    device_type text,\n    rawdata text,\n    PRIMARY KEY ((cid, event_date), event_time)\n) WITH CLUSTERING ORDER BY (event_time DESC)\n</code></pre>\n\n<p><strong>List of Spark Job:</strong></p>\n\n<ol>\n<li>Need to run client jobs for single date </li>\n<li>Need to run client jobs for single date. (applying allow filtering)</li>\n<li>Need to specific job for all client single data</li>\n<li>Need to specific job for all client single data</li>\n</ol>\n\n<p>If the data size increase job get slow. Do we need to focus on performance metrics(cassandra/spark) or shall we keep the data in different small talble.</p>\n\n<p><strong>BackUp Strategy</strong></p>\n\n<p>What is the best way to do a backup strategy?</p>\n\n<ol>\n<li><p>Cassandra way \n<a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_backup_restore_c.html\" rel=\"nofollow noreferrer\">https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_backup_restore_c.html</a></p></li>\n<li><p>External data source type to disk like csv/flat file etc..</p></li>\n</ol>\n"
    }, {
        "tags": ["java", "performance-testing", "cassandra-2.0", "iot", "manual-testing"],
        "is_answered": false,
        "view_count": 101,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1491391564,
        "creation_date": 1472505315,
        "question_id": 39215338,
        "link": "https://stackoverflow.com/questions/39215338/performance-testing-for-java-backend-and-cassandra-for-iot-project",
        "title": "Performance testing for JAVA Backend and Cassandra for IOT Project",
        "body": "<p>I am working on an IOT project for which I need to do performance testing.\nHere are the details,</p>\n\n<ul>\n<li>Performance testing on <strong>JAVA Backend application.</strong></li>\n<li>Performance testing on <strong>CASSANDRA DATABASE</strong>.</li>\n</ul>\n\n<p>Data is pushed from JAVA Backend application to CASSANDRA DATABASE.</p>\n\n<p>Please suggest me the best performance testing tool. </p>\n"
    }, {
        "tags": ["iot", "hazelcast", "hazelcast-imap"],
        "is_answered": true,
        "view_count": 193,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1491122363,
        "creation_date": 1491121510,
        "question_id": 43166218,
        "link": "https://stackoverflow.com/questions/43166218/get-hazelcast-entryexpiredlistener-only-of-one-node-not-all-replicas",
        "title": "Get Hazelcast EntryExpiredListener only of one node (not all replicas)",
        "body": "<p>I have requirement where i have a continuous flow of signals and i want to be informed if there has'nt been a signal for a certain amount of time.</p>\n\n<p>right now i am adding every signal to a Hazelcast map and if it has not been updated for a certain amount of time the Hazelcast EntryExpiredListener fires.</p>\n\n<p>So far, so good, but the EntryExpiredListener is called for every replica, so i am notified N times.</p>\n\n<p>Is there a way to be only informed in the master node or only one replica?</p>\n\n<p>I am aware that i am abusing HZ a bit and i am open for other ideas too.</p>\n"
    }, {
        "tags": ["python", "matlab", "csv", "iot"],
        "is_answered": true,
        "view_count": 1201,
        "favorite_count": 0,
        "score": 3,
        "last_activity_date": 1487278813,
        "creation_date": 1487246142,
        "question_id": 42273254,
        "link": "https://stackoverflow.com/questions/42273254/how-to-access-valuesan-array-from-specific-field-of-thingspeak-channel",
        "title": "How to access values(an array) from specific Field of ThingSpeak channel?",
        "body": "<p>I am unable to access data from field which contains the array of values with error as Not a Number(NaN). These values were stored in a buffer before sending as the website can be accessed only once in 15 mins.  Do I have to follow any specific method to send data from my RaspberryPi ( using Python) while using buffer values (stored values)?</p>\n\n<p>Attached the screenshot of the values received by ThingSpeak ( CSV file). Is there any way to send multiple values to a field by accessing it once in 15 seconds.</p>\n\n<p><a href=\"https://i.stack.imgur.com/hwqIr.png\" rel=\"nofollow noreferrer\">Screenshot of CSV file generated with ThingSpeak </a></p>\n"
    }, {
        "tags": ["azure", "streaming", "tableau-api", "iot", "dashboard"],
        "is_answered": true,
        "view_count": 431,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1483062796,
        "creation_date": 1482426972,
        "question_id": 41288543,
        "link": "https://stackoverflow.com/questions/41288543/how-to-create-customized-dashboards-of-streaming-data-in-microsoft-azure-iot",
        "title": "How to create customized Dashboards of streaming data in Microsoft Azure IoT?",
        "body": "<p>I have created an Event Hub in the Azure portal. I am streaming Temperature data into the hub (1 value per minute) and would like to create 2 plots (a Temperature time series and a Temperature histogram of the respective last 480 minutes) in a dashboard. I have created a Stream Analytics job to achieve this. It takes the Event Hub data as input. </p>\n\n<p>Ideally I'd like to have the visualizations created directly from the data stream without any data storage, i.e. \"stream -> dashboard\" rather than \"stream -> storage -> dashboard\". I'm used to creating dashboards in Tableau and I'd love to use a similarly intuitive and maybe even interactive tool. </p>\n\n<p>One option may be to store the streaming data in Azure and to import from Azure to Tableau. Another option might be to output the Stream Analytics job to Microsoft Power BI.</p>\n\n<p><strong>Question is: What other/more convenient options are there to create my dashboard from my streaming data? In particular: Are there any native ways within Azure for creating customizable visualizations/dashboards of streaming data? Does Azure IoT Suite enable creating customizable dashboards of streaming data?</strong></p>\n\n<p>Thanks.</p>\n"
    }, {
        "tags": ["iot", "kaa", "apache-edgent"],
        "is_answered": true,
        "view_count": 493,
        "favorite_count": 0,
        "closed_date": 1482226321,
        "score": 0,
        "last_activity_date": 1482216636,
        "creation_date": 1474968965,
        "last_edit_date": 1482216636,
        "question_id": 39721177,
        "link": "https://stackoverflow.com/questions/39721177/which-one-to-use-for-implementing-edge-analytics-apache-edgent-or-kaa",
        "title": "Which one to use for implementing Edge Analytics? Apache Edgent or Kaa?",
        "body": "<p>I have came across the term <strong>edge analytics</strong> in recent times and understood the advantages of pushing intelligence to edge devices. Now coming to implementation, I have gone through <strong>Apache Edgent</strong> and <strong>Kaa</strong>. Both seems to perform more or less similar job for me with respect to analytics on edge devices, but Kaa is entirely a platform for IoT which has edge analytics as a feature. Which is more efficient and used in the industry?</p>\n"
    }, {
        "tags": ["iot", "dataflow", "apache-nifi"],
        "is_answered": true,
        "view_count": 373,
        "favorite_count": 1,
        "score": 6,
        "last_activity_date": 1479750305,
        "creation_date": 1479676921,
        "last_edit_date": 1479743661,
        "question_id": 40709528,
        "link": "https://stackoverflow.com/questions/40709528/apache-nifi-with-iot-sensors",
        "title": "Apache Nifi with IOT sensors",
        "body": "<p>Im new to Apache Nifi , and I'm having a use case which i need to parse and decode different kind of messages from Sensors, transform and load the data in Hbase all my sensors send data every 10 minutes through an API via a post request, what I have done for now is a service with JAVA that listen on a specific port and do all the ETL dataflow, any idea how can I use Apache Nifi for this use case ?</p>\n"
    }, {
        "tags": ["hadoop", "apache-spark", "analytics", "iot", "hadoop-streaming"],
        "is_answered": false,
        "view_count": 115,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1479576095,
        "creation_date": 1479450873,
        "last_edit_date": 1479459883,
        "question_id": 40670755,
        "link": "https://stackoverflow.com/questions/40670755/analytics-engine-for-iot-devices",
        "title": "Analytics Engine for IoT Devices",
        "body": "<p>I am working for a M2M IoT provider. There are millions of deployed IoT devices in the field. The data is aggregated by edge devices and sent to our central server. Server processes the device data and sends out the processed info to various other sub-systems.</p>\n\n<p>We are conceptualizing the idea of an analytics engine (AE) in parallel to existing system. The data coming from the IoT devices shall be fed into this engine to come with different types of analysis. Example: Monitor the events coming from a device over the last 24 hours and figure out the health or predict other stuff.\nWe are trying to figure out the answers to the below questions in this regard:</p>\n\n<p>1) Where shall we tap the incoming data from IoT devices to feed into the new system(AE)? If we tap at the existing server then we shall be introducing a strong coupling between the two systems. Any downtime at the exiting server shall dry the pipeline to AE as well. What is the general recommended strategy for such a case?</p>\n\n<p>2) We plan to front end AE with Kafka to ensure data availability. There shall be various micro services acting as Kafka consumers and doing their stuff. What shall be the architecture of a micro service which shall manage device state as per the events reported during the last 24 hours (i.e. need to consider both historical and live data)? There are numerous technologies out there like Spark, Hadoop, Apache storm and I am not sure which one to start with. How should I persist the incoming device data and how to process historical and incoming data together to figure out current device state? What is the standard practice in such scenarios: Is the summarized data calculated and stored beforehand or is it calculated in real time as per the incoming query?</p>\n"
    }, {
        "tags": ["iot", "pubnub"],
        "is_answered": true,
        "view_count": 205,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1478705784,
        "creation_date": 1478352447,
        "last_edit_date": 1478625914,
        "question_id": 40438773,
        "link": "https://stackoverflow.com/questions/40438773/pubnub-eon-graph",
        "title": "PubNub EON Graph",
        "body": "<p>My sensor data consists of temperature and humidity. I can already console those datas temp and humid, to a single graph using simple example provided by pubnub. Now I decide to separate the temp and humid into 2 different graph, the temperature graph on top and humidity graph on bottom so that it can be viewed better and clearer due to resolution. How can I implement this using the eon sdk?\nThe formatted json data sent is,</p>\n\n<pre><code>\"eon\":{\"tpr\":%.1f,\"hum\":%.1f}\n</code></pre>\n\n<p>Here's the code i'm following,</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;body&gt;\n    &lt;h1&gt;Getting Started with EON&lt;/h1&gt;\n    &lt;p&gt;Create real-time charts and maps.&lt;/p&gt;\n    &lt;div id=\"chart12\"&gt;&lt;/div&gt;\n    &lt;div id=\"chart13\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n          var pubnub = PUBNUB.init({\n            publish_key:   'pub-c-3d6b4414-redacted', // replace with your own pub-key\n            subscribe_key: 'sub-c-0d045036-redacted' // replace with your own sub-key\n          });\n\n          eon.chart({\n            pubnub: pubnub,         \n            channel: \"birdpeek\", // the pubnub channel for real time data\n            limit:20,\n            flow:true,\n            generate: {           // c3 chart object\n              bindto: '#chart12',\n              data: {\n                type: 'spline',\n                labels: true\n              }\n            }\n          });\n        &lt;/script&gt;\n&lt;/body&gt;\n</code></pre>\n"
    }, {
        "tags": ["php", "ibm-cloud", "mqtt", "iot", "phpmqtt"],
        "is_answered": true,
        "view_count": 880,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1478543970,
        "creation_date": 1455719335,
        "last_edit_date": 1478543970,
        "question_id": 35459449,
        "link": "https://stackoverflow.com/questions/35459449/mqtt-subscribe-with-php-to-ibm-bluemix",
        "title": "MQTT Subscribe with PHP to IBM Bluemix",
        "body": "<p>I want to connect to IBM Bluemix through the MQTT protocol using PHP to subscribe to messages come from IoT Foundation.\nI use this code:</p>\n\n<pre><code>&lt;?php\n\nrequire(\"../phpMQTT.php\");\n\n\n$config = array(\n  'org_id' =&gt; 't9m318',\n  'port' =&gt; '1883',\n  'app_id' =&gt; 'phpmqtt',\n  'iotf_api_key' =&gt; 'my api key',\n  'iotf_api_secret' =&gt; 'my api secret',\n  'device_id' =&gt; 'phpmqtt'\n);\n\n$config['server'] = $config['org_id'] .'.messaging.internetofthings.ibmcloud.com';\n$config['client_id'] = 'a:' . $config['org_id'] . ':' .$config['app_id'];\n$location = array();\n\n// initialize client\n$mqtt = new phpMQTT($config['server'], $config['port'], $config['client_id']); \n$mqtt-&gt;debug = false;\n\n// connect to broker\nif(!$mqtt-&gt;connect(true, null, $config['iotf_api_key'], $config['iotf_api_secret'])){\n  echo 'ERROR: Could not connect to IoT cloud';\n    exit();\n} \n\n$topics['iot-2/type/+/id/phpmqtt/evt/+/fmt/json'] = \n  array(\"qos\"=&gt;0, \"function\"=&gt;\"procmsg\");\n$mqtt-&gt;subscribe($topics, 0);\n\n// process messages\nwhile ($mqtt-&gt;proc(true)) { \n\n}\n// disconnect\n$mqtt-&gt;close();\nfunction procmsg($topic, $msg) {\n echo \"Msg Recieved: $msg\";\n}\n\n?&gt;\n</code></pre>\n\n<p>But the browser show this message:</p>\n\n<blockquote>\n  <p>Fatal error: Maximum execution time of 30 seconds exceeded in /Library/WebServer/Documents/phpMQTT/phpMQTT.php on line 167</p>\n</blockquote>\n"
    }, {
        "tags": ["iot", "complex-event-processing", "esper", "cumulocity"],
        "is_answered": true,
        "view_count": 121,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1477299350,
        "creation_date": 1477299350,
        "question_id": 40214317,
        "link": "https://stackoverflow.com/questions/40214317/cep-generate-measurement-event-not-in-utc-time-but-in-local",
        "title": "CEP generate Measurement/Event not in UTC time but in Local",
        "body": "<p>All Measurements came with a time-stamp (event time) of when the measurement was created. Some of these measurements are artificial ones, meaning that they are not created by the device itself, but by a CEP rule running inside the CoT.</p>\n\n<pre><code>The \"normal\" measurements have the time format coded as UTC \n[{ \n\"id\": \"12704547\", \n\"data\": { \n\"data\": { \n\"time\": \"2016-07-25T15:24:11.000Z\", \n\"id\": \"1152930\", \n\"self\": \"http://testTenant.c8y.com/measurement/measurements/1152930\", \n\"source\": { \n\"id\": \"222812\", \n\"self\": \"http://testTenant.c8y.com/inventory/managedObjects/222812\" \n}, \n\"type\": \"tsystems_cumulocity_energymeter_digital_ping\", \n\"Energieverbrauch\": { \n\"Ping\": { \n\"unit\": \"Wh\", \n\"value\": 1 \n} \n} \n}, \n\"realtimeAction\": \"CREATE\" \n}, \n\"channel\": \"/measurements/222812\" \n}, { \n\"successful\": true, \n\"channel\": \"/meta/connect\" \n}] \n</code></pre>\n\n<p>But the \"artificial\" measurements (created by the CEP rule) use a timestamp with local time </p>\n\n<pre><code>[{ \n\"id\": \"12704578\", \n\"data\": { \n\"data\": { \n\"time\": \"2016-07-25T17:24:00.952+02:00\", \n\"id\": \"1152931\", \n\"self\": \"http://testTenant.c8y.com/measurement/measurements/1152931\", \n\"source\": { \n\"id\": \"222812\", \n\"self\": \"http://testTenant.c8y.com/inventory/managedObjects/222812\" \n}, \n\"type\": \"tsystems_cumulocity_energymeter_power_consumption\", \n\"Leistung\": { \n\"Aggregation_1min\": { \n\"unit\": \"W\", \n\"value\": 900 \n} \n} \n}, \n\"realtimeAction\": \"CREATE\" \n}, \n\"channel\": \"/measurements/222812\" \n}] \n</code></pre>\n\n<p>The measurements from one device should always be encoded with the same timezone (UTC preferred) as different timezone can create problems in clients using that data.</p>\n\n<p>I create the 'time' in the CEP with </p>\n\n<pre><code>current_timestamp().toDate() as time \n</code></pre>\n"
    }, {
        "tags": ["node.js", "azure", "iot"],
        "is_answered": true,
        "view_count": 61,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1476885993,
        "creation_date": 1476553649,
        "last_edit_date": 1476885993,
        "question_id": 40062302,
        "link": "https://stackoverflow.com/questions/40062302/in-azure-iot-device-portal-how-to-change-data-field-names",
        "title": "In Azure IOT Device Portal, how to change data field names?",
        "body": "<p>The sample comes with temperature and humidity as data fields. Although I replaced the data with my own different fields, the names remain the same. </p>\n\n<p>I cannot find a simple rename column option. How do I change it?</p>\n"
    }, {
        "tags": ["scala", "apache-spark", "spark-streaming", "iot"],
        "is_answered": true,
        "view_count": 1309,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1476814444,
        "creation_date": 1475661859,
        "last_edit_date": 1475668470,
        "question_id": 39871052,
        "link": "https://stackoverflow.com/questions/39871052/raise-alert-through-apache-spark",
        "title": "Raise Alert through apache spark",
        "body": "<p>I am using Apache Spark to take real time data from Apache Kafka which are from any sensors in Json format.</p>\n\n<p>example of data format : </p>\n\n<pre><code>{\n    \"meterId\" : \"M1\",\n    \"meterReading\" : \"100\"\n }\n</code></pre>\n\n<p>I want to apply rule to raise alert in real time. i.e. if I did not get data of \"meter M 1\" from last 2 hours or meter Reading exceed some limit the alert should be created.</p>\n\n<p>so how can I achieve this in Scala?</p>\n"
    }, {
        "tags": ["amazon-web-services", "amazon-rds", "iot", "amazon-aurora"],
        "is_answered": true,
        "view_count": 937,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1476744928,
        "creation_date": 1476655851,
        "question_id": 40076019,
        "link": "https://stackoverflow.com/questions/40076019/amazon-rds-billing",
        "title": "Amazon RDS billing",
        "body": "<p>My doubt is how the Amazon RDS instance is billed. I read somewhere that it is a component based billing based on the CPU/hour, number of input/output requests, etc.,</p>\n\n<p>How are the I/O requests interpreted? I have a model in which I am trying to reduce the number of input queries that go into the cloud. Will it reduce my yearly cost to a good extent?</p>\n"
    }, {
        "tags": ["node.js", "database", "web-services", "web", "iot"],
        "is_answered": true,
        "view_count": 41,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1475556492,
        "creation_date": 1475035229,
        "last_edit_date": 1592644375,
        "question_id": 39737815,
        "link": "https://stackoverflow.com/questions/39737815/database-which-doesnt-need-a-private-key-to-push-data",
        "title": "Database which doesn&#39;t need a Private Key to Push Data",
        "body": "<p>I have a project I am working on which will be open source. The project records data from all over. People can look at the instructions on a website, build the prototype, and the prototype will contribute its data to a giant database.</p>\n<p><a href=\"https://data.sparkfun.com/\" rel=\"nofollow noreferrer\">data.sparkfun.com/</a> is perfect for this, except for a major problem. In order to push data, you need to put the Private Key in the code (<a href=\"http://phant.io/docs/input/http/\" rel=\"nofollow noreferrer\">docs</a>). This will also allow anybody with the code (which will be everybody looking at my project because it is open source) to edit, modify, and delete data from the database because they have the Private Key.</p>\n<p>Is there any alternative to <a href=\"http://data.sparkfun.com/\" rel=\"nofollow noreferrer\">data.sparkfun</a> for free so that I can achieve this? I am using NodeJS as the main language for my project.</p>\n<p><strong>EDIT:</strong> I also do not have a server to host my own database on. I also would need a hosting service (which is why data.sparkfun is so close to what I need).</p>\n"
    }, {
        "tags": ["frameworks", "iot"],
        "is_answered": true,
        "view_count": 63,
        "favorite_count": 1,
        "score": 2,
        "last_activity_date": 1474978574,
        "creation_date": 1437343666,
        "question_id": 31506437,
        "link": "https://stackoverflow.com/questions/31506437/similarities-and-differences-between-fiteagle-and-openiot-frameworks",
        "title": "similarities and differences between FIteagle and OpenIot frameworks",
        "body": "<p>I m trying to understand better the two frameworks therfore i m trying to figure out the similarities and differences between FIteagle framework and OpenIot because both of the frameworks includes the same aims, the first one provides a testbeds environments which provide different resources to manage and communicate with and the second one provides the possiblity to connect to different sensors within a database cloud and it provide the ability to communicate with the sensors and to aply some IoT services on it. Does anyone has an idea about the two frameworks ?</p>\n"
    }, {
        "tags": ["database", "iot"],
        "is_answered": true,
        "view_count": 958,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1474918679,
        "creation_date": 1474823129,
        "question_id": 39689530,
        "link": "https://stackoverflow.com/questions/39689530/data-storage-for-iot-devices",
        "title": "Data storage for IoT devices",
        "body": "<p>How data is stored for Internet of Things devices? Are they stored in traditional relational database format (tables, rows , columns ) or some other format? Is there some software or algorithm applied to raw sensor data to organize them ?\nAny reference to research paper is appreciated.</p>\n"
    }, {
        "tags": ["azure", "iot", "unpivot", "azure-stream-analytics"],
        "is_answered": true,
        "view_count": 498,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1473290324,
        "creation_date": 1473159365,
        "question_id": 39347310,
        "link": "https://stackoverflow.com/questions/39347310/stream-analytics-job-has-validation-errors-job-will-exceed-the-maximum-amount-o",
        "title": "Stream Analytics job has validation errors: Job will exceed the maximum amount of Event Hub Receivers.",
        "body": "<p>I am trying to write a query in ASA (Azure Stream Analytics) where the input is a json line message that looks like this</p>\n\n<pre><code>{\n    \"DeviceId\": \"Device3\",\n    \"DateTime\": \"2016-09-05T13:23:04.5444423\",\n    \"Value\": [ 1, 0, 1, 0, 0, 1, 0 ]\n}\n</code></pre>\n\n<p>And what I want to do is to perform an Unpivot so the data comes on the form </p>\n\n<pre><code>create table LightBeacon (\n     DeviceId int primary key not null,\n     EventDateTime datetime not null,\n     LightBeaconId varchar(25) not null,\n     LightBeaconState SmallInt not null\n)\n</code></pre>\n\n<p>But seems that ASA do not support the SQL Unpivot function and is there by left with multi select statements like</p>\n\n<pre><code>with \n    DataUnArray as (    \n        SELECT DeviceId, DateTime as EventDateTime\n            , GetArrayElement(Value, 0) as LigthBeacon01\n            , GetArrayElement(Value, 1) as LigthBeacon02\n            , GetArrayElement(Value, 2) as LigthBeacon03\n            , GetArrayElement(Value, 3) as LigthBeacon04\n            , GetArrayElement(Value, 4) as LigthBeacon05\n            , GetArrayElement(Value, 5) as LigthBeacon06\n            , GetArrayElement(Value, 6) as LigthBeacon07\n        FROM DataIoT\n        where DeviceId = 'Device3'),\n    DataUnpivot as (\n            select DeviceId, EventDateTime, 'LigthBeacon01' as LigthBeaconId, LigthBeacon01 as LigthBeaconState from DataUnArray\n        Union All select DeviceId, EventDateTime, 'LigthBeacon02' as LigthBeaconId, LigthBeacon02 as LigthBeaconState from DataUnArray\n        Union All select DeviceId, EventDateTime, 'LigthBeacon03' as LigthBeaconId, LigthBeacon03 as LigthBeaconState from DataUnArray\n        Union All select DeviceId, EventDateTime, 'LigthBeacon04' as LigthBeaconId, LigthBeacon04 as LigthBeaconState from DataUnArray\n        Union All select DeviceId, EventDateTime, 'LigthBeacon05' as LigthBeaconId, LigthBeacon05 as LigthBeaconState from DataUnArray\n        Union All select DeviceId, EventDateTime, 'LigthBeacon06' as LigthBeaconId, LigthBeacon06 as LigthBeaconState from DataUnArray\n        Union All select DeviceId, EventDateTime, 'LigthBeacon07' as LigthBeaconId, LigthBeacon07 as LigthBeaconState from DataUnArray\n    )   \n    select DeviceId, EventDateTime, LigthBeaconId, LigthBeaconState\n    into DataLakeCSV\n    from DataUnpivot\n</code></pre>\n\n<p>The ASA Query is failing to start with the following error:</p>\n\n<blockquote>\n  <p>Stream Analytics job has validation errors: Job will exceed the\n  maximum amount of Event Hub Receivers</p>\n</blockquote>\n\n<p>If I reduce it to 5 beacon \u2013 it works!!! So how can I write a ASA Query that can handle more than 5 columns in an unpivot?</p>\n\n<p>\\Bj\u00f8rn</p>\n"
    }, {
        "tags": ["ibm-cloud", "real-time", "scalability", "iot", "rule-engine"],
        "is_answered": true,
        "view_count": 74,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1472590389,
        "creation_date": 1472566412,
        "last_edit_date": 1472587493,
        "question_id": 39229919,
        "link": "https://stackoverflow.com/questions/39229919/ibm-bluemix-iot-realtime-insights",
        "title": "IBM Bluemix IoT Realtime Insights",
        "body": "<p>I have recently started working on IBM Bluemix IoT Platform. I learned that IoT Real-Time Insights delivers a highly scalable, parallel processing rules engine which is designed from the ground up for IoT data. I was wondering how this scaling happens for the large volume of event data. Can somebody explain me the architecture/scalability of IoT platform? Can we configure rule engine (parallel processing) on how may parallel process can be run?</p>\n\n<p>Thanks,\nSuhas</p>\n"
    }, {
        "tags": ["node.js", "apache-spark", "spark-streaming", "apache-storm", "iot"],
        "is_answered": false,
        "view_count": 192,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1472035684,
        "creation_date": 1472026928,
        "last_edit_date": 1472035684,
        "question_id": 39118057,
        "link": "https://stackoverflow.com/questions/39118057/is-streaming-analytics-framework-like-apache-spark-or-apache-storm-suitable-for",
        "title": "Is streaming analytics framework like Apache spark or Apache storm suitable for IoT gateways?",
        "body": "<p>I want the streaming analytics framework to be executed in IoT gateway itself instead of cloud server. The <code>Node.js</code> application, installed in gateway, listens for messages through a message broker ( <code>ZeroMQ</code> ) and does streaming analytics with a minimal latency.</p>\n\n<p>Does <code>Node.js</code> support <code>Apache spark</code> or <code>Apache storm</code>?</p>\n\n<p>The specification of IoT gateway is as follows:<br>\n1. <strong><code>CPU:</code></strong> Onboard single core 400MHz<br>\n2. <strong><code>MEM:</code></strong> DDR2 1GB<br>\n3. <strong><code>STORAGE:</code></strong> eMMC 4GB (expandable) <br>\n4. <strong><code>OS:</code></strong> Ostro <br></p>\n\n<p>If the mentioned frameworks are not supported, kindly suggest the alternatives.</p>\n"
    }, {
        "tags": ["database", "oracle", "indexing", "iot"],
        "is_answered": true,
        "view_count": 185,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1471113206,
        "creation_date": 1471087520,
        "last_edit_date": 1471113206,
        "question_id": 38932199,
        "link": "https://stackoverflow.com/questions/38932199/how-to-create-index-organized-table-with-desc-order",
        "title": "How to create Index-organized table with desc order",
        "body": "<p>Could you please help me with creating IOT with three columns and one of them has to have desc order.</p>\n\n<p>I've created the table, but it's not ordered by created_date desc.</p>\n\n<p>My table:</p>\n\n<pre><code>create table audit_log (\n     id integer,\n     created_date timestamp,\n     module_type_id integer,\n     ...&lt;other columns&gt;...\n     constraint pk_audit_log primary key (created_date, module_type_id, id)) \norganization index overflow;\n</code></pre>\n\n<p>I need IOT like <code>constraint pk_audit_log primary key (created_date DESC, module_type_id, id)</code></p>\n\n<p>Unfortunately, I couldn't find any resources on the web how to do it.\nHas anybody faced with any similar task or know how to create a script?</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["database", "architecture", "iot"],
        "is_answered": true,
        "view_count": 79,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1469949681,
        "creation_date": 1469235004,
        "question_id": 38537076,
        "link": "https://stackoverflow.com/questions/38537076/storage-and-monitoring-for-working-with-streaming-data-such-as-from-sensors",
        "title": "Storage and monitoring for working with streaming data, such as from sensors?",
        "body": "<p>I need to work with data coming from sensors for a project. The scale of the project is rather small, as the purpose will be mainly for demonstration. However I need to be able to visualize and process the data in real time.</p>\n\n<p>The workflow is simple: receive data from sensors (configurable) -> process data real time -> display and update results ( in a chart/graphical form )</p>\n\n<p>Are there any solutions that are fast to deploy for my needs? (Python and open-sourced solutions are preferred)</p>\n"
    }, {
        "tags": ["arduino", "wifi", "iot", "arduino-uno"],
        "is_answered": false,
        "view_count": 133,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1469819422,
        "creation_date": 1469749194,
        "last_edit_date": 1469819422,
        "question_id": 38648388,
        "link": "https://stackoverflow.com/questions/38648388/connect-devices-to-arduino-via-wifi",
        "title": "Connect devices to Arduino via WiFi",
        "body": "<p>I am trying to figure out the best way to control devices in my room through an iPhone app.  For now it would just be things like music and lights but I would like to tackle much more complex systems once I get the hang of it.  </p>\n\n<p>I have found a bunch of different resources online that are helping me figure out what parts I need/ where I should start but they all have a different way of doing things.  My biggest question is what board should I use and what other components do I need for the system?  I currently have a few <code>Arduino Uno's</code> and am thinking about ordering a <code>Ethernet Shield 2</code> to connect to WiFi but then I also found out about the <code>Arduino Zero</code> and the <code>WiFi Shield 10</code>1 as well as the <code>MKR1000</code> which both seem like good options.  </p>\n\n<p>If anyone has some advice of where to start of has done something similar I would really appreciate hearing from you.</p>\n"
    }, {
        "tags": ["data-structures", "iot"],
        "is_answered": false,
        "view_count": 80,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1469592004,
        "creation_date": 1469509621,
        "last_edit_date": 1469592004,
        "question_id": 38581392,
        "link": "https://stackoverflow.com/questions/38581392/in-iot-vehicle-parking-system-designing-which-data-model-best-suited-for-milli",
        "title": "In IOT , vehicle parking system designing which data model best suited for millions numbers of concurrent users",
        "body": "<p>I am trying to designing a IOT based  parking slot booking management system.\nHere is requirement:</p>\n\n<pre><code> 1. Get all available slots for a given location and time period\n 2. Dynamic pricing for parking slots as per traffic like more traffic \n       more charges.\n 3. Suggestion for available booking slots at near location , in case no    slots available at requested location.\n 4. During post analytics  used data model able to give results like \n          a. Most used parking slots\n          b. Peak traffic hours\n          c. optimize unused traffic slots\n 5. In every booking slot there is difference of 30 min. no idea how to optimize this.\n 6. How to detect booking slot overlapping and then suggest available booking slot option.\n</code></pre>\n\n<p>I am planning to  design this system with Tree based data model but thing that blocking me is that in case number of concurrent user going to millions then we need to traverse lot's of tree node to get available time slots.  Any better approach to build such system.              </p>\n"
    }, {
        "tags": ["amazon-web-services", "amazon-dynamodb", "aws-lambda", "iot"],
        "is_answered": true,
        "view_count": 5170,
        "favorite_count": 3,
        "score": 4,
        "last_activity_date": 1468730172,
        "creation_date": 1468060925,
        "question_id": 38280958,
        "link": "https://stackoverflow.com/questions/38280958/dynamodb-triggers-streams-lambda-details-on-trim-horizon",
        "title": "DynamoDB Triggers (Streams + Lambda) : details on TRIM_HORIZON?",
        "body": "<p>I want to process recent updates on a DynamoDB table and save them in another one. Let's say I get updates from an IoT device irregularly put in Table1, and I need to use the N last updates to compute an update in Table2 for the same device in sync with the original updates (kind of a sliding window).</p>\n\n<p>DynamoDB Triggers (Streams + Lambda) seem quite appropriate for my needs, but I did not find a clear definition of <code>TRIM_HORIZON</code>. In some docs I understand that it is the oldest data in Table1 (can get huge), but in other docs it would seems that it is 24h. Or maybe the oldest in the stream, which is 24h?</p>\n\n<p>So anyone knows the truth about <code>TRIM_HORIZON</code>? Would it even be possible to configure it? </p>\n\n<p>The alternative I see is not to use <code>TRIM_HORIZON</code>, but rather tu use <code>LATEST</code> and perform a query on Table1. But it sort of defeats the purpose of streams.</p>\n"
    }, {
        "tags": ["iot"],
        "is_answered": false,
        "view_count": 166,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1468459212,
        "creation_date": 1428648012,
        "question_id": 29554951,
        "link": "https://stackoverflow.com/questions/29554951/trying-to-explore-connected-shop-floor-use-case-and-need-advice-on-an-iot-archit",
        "title": "Trying to explore connected shop floor use case and need advice on an IOT architecture",
        "body": "<p>Apologies if this question ends up being very general and broad. IoT is new and upcoming with people still trying to figure it out so I decided to take a chance and ask my question. </p>\n\n<p>I looked at the legacy and current applications being used on the shop floor and reached an understanding that at the very low level (device layer) we have PLC/SCADA that needs to communicate with OPC(OLE for process control) Source Agent(Server) to store data as PLCs are real time with no storage capability. Then we use a Plant Connectivity tool like SAP to query the OPC and pass on that information to an MII(Management Intelligence Integration) tool. </p>\n\n<ol>\n<li>Connected shop floor in the context of IoT would basically be a\ndirect communication between PLCs and IIM with some sort of\nintelligent layer in between right?</li>\n<li><p>Coming from an IT standpoint where hardware is a blackbox what\nshould be kept in mind while building a dashboard as far as sensors\nand the device layer is concerned?</p></li>\n<li><p>Should I be looking at all the gazillion sensors being used on the\nshop floor and design a dashboard that incorporates a majority of\nthem? </p></li>\n<li>What standards should I follow?</li>\n<li>Where can I get opensource data from shop floor sensor<br>\n(temp,humidity,vibration,CNC laithe data, PLC data etc)?</li>\n</ol>\n\n<p>Thanks in advance for any help/advice/encouragement.</p>\n"
    }, {
        "tags": ["rest", "http", "arduino", "embedded", "iot"],
        "is_answered": true,
        "view_count": 1095,
        "favorite_count": 2,
        "score": 5,
        "last_activity_date": 1467886336,
        "creation_date": 1467841390,
        "last_edit_date": 1467878302,
        "question_id": 38234272,
        "link": "https://stackoverflow.com/questions/38234272/why-when-to-use-iot-publish-subscribe-protocols-rather-then-restful-http",
        "title": "Why/when to use IoT publish/subscribe protocols rather then RESTful HTTP?",
        "body": "<p>I am sending data (GPS coordinates) from Arduino once a minute with HTTP POST request to REST API (in OpenShift PaaS). Data is then stored to MySQL db.</p>\n\n<p>Would so called \"IoT\" publish/subscribe protocols (XMPP, MQTT) be better? Why?</p>\n\n<p>When exactly do you use those two protocols rather than Restful HTTP? Would I really save a significant baterry energy using them?</p>\n\n<p>AFAIK in those protocols machine would \"publish\" a data to broker and my app would subscribe to it. If I would like to gather data every minute in my app I guess that I would got to have some CRON job that would subscribe to data every minute? Or how would data gathering be achieved?</p>\n"
    }, {
        "tags": ["c++", "arduino", "iot"],
        "is_answered": false,
        "view_count": 632,
        "favorite_count": 0,
        "score": -3,
        "last_activity_date": 1467226165,
        "creation_date": 1467196694,
        "question_id": 38097160,
        "link": "https://stackoverflow.com/questions/38097160/how-to-send-float-value-in-url-im-sending-float-value-but-getting-null-values",
        "title": "how to send float value in url , im sending float value but getting Null values",
        "body": "<p>i want to send the values and insert the values into the database, but im getting null values.</p>\n\n<pre><code> float PHValue = Value/10;\n  float DOValue= 12.22;\n  gprsSerial.println(\"AT+HTTPPARA=\\\"URL\\\",\\\"http://itempurl.com/smartpond/AddTemprature?WaterTemperature=\"\"+celsius+\"\"&amp;PHValue=\"\"+PHValue+\"\"&amp;DOValue=\"\"+DOValue+\"\"&amp;currentTime=06-30-2016\\\"\");\n</code></pre>\n"
    }, {
        "tags": ["azure", "iot", "powerbi", "azure-stream-analytics", "azure-iot-hub"],
        "is_answered": false,
        "view_count": 152,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1467160055,
        "creation_date": 1467103526,
        "question_id": 38071281,
        "link": "https://stackoverflow.com/questions/38071281/how-to-reset-in-powerbi-the-data-in-table",
        "title": "How to reset in PowerBI the data in table",
        "body": "<p>I send data from Azuer IoT Hub with Stream Analytics to Power BI and show it with the Stack Column Chart. I Send the Productname and amount=1.</p>\n\n<p><a href=\"https://i.stack.imgur.com/726MK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/726MK.png\" alt=\"enter image description here\"></a> </p>\n\n<p>The amount will be show as Sum for a product. I send new amount it will be added to last value. </p>\n\n<p>How can i reset the value amount of one or all product. I will send a command to reset the value of product amount. </p>\n\n<p>I use the PowerBI for free online.</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["java", "azure", "raspberry-pi", "iot", "azure-iot-hub"],
        "is_answered": true,
        "view_count": 288,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1467043060,
        "creation_date": 1466855021,
        "last_edit_date": 1467007709,
        "question_id": 38028130,
        "link": "https://stackoverflow.com/questions/38028130/how-can-i-work-with-the-received-data-from-azure-iot-hub",
        "title": "How can I work with the received DATA from Azure IoT Hub",
        "body": "<p>I receive the Data:</p>\n\n<pre><code>public void accept(PartitionReceiver receiver)\n{\n    System.out.println(\"** Created receiver on partition \" + partitionId);\n    try {\n        while (true) {\n            Iterable&lt;EventData&gt; receivedEvents = receiver.receive(10).get();\n            int batchSize = 0;\n            if (receivedEvents != null)\n            {\n                for(EventData receivedEvent: receivedEvents)\n                {                                    \n                    System.out.println(String.format(\"| Time: %s\", receivedEvent.getSystemProperties().getEnqueuedTime()));\n                    System.out.println(String.format(\"| Device ID: %s\", receivedEvent.getProperties().get(\"iothub-connection-device-id\")));\n                    System.out.println(String.format(\"| Message Payload: %s\", new String(receivedEvent.getBody(), Charset.defaultCharset())));\n                    batchSize++;\n                }\n            }\n        }\n    } catch (Exception e)\n    {\n        System.out.println(\"Failed to receive messages: \" + e.getMessage());\n    }\n}\n</code></pre>\n\n<p>Here i become the product name and price:</p>\n\n<pre><code>System.out.println(String.format(\"| Message Payload: %s\", new String(receivedEvent.getBody(), Charset.defaultCharset())));\n</code></pre>\n\n<p>How can i take the Payload, product into a String product; and the price into double price;?</p>\n"
    }, {
        "tags": ["cassandra", "time-series", "device", "iot"],
        "is_answered": true,
        "view_count": 2494,
        "favorite_count": 1,
        "score": 3,
        "last_activity_date": 1465816365,
        "creation_date": 1465052578,
        "question_id": 37631684,
        "link": "https://stackoverflow.com/questions/37631684/data-modelling-in-cassandra-for-iot",
        "title": "Data Modelling in cassandra for IOT",
        "body": "<p>We are trying to use Apache Cassandra in an IoT based application. We are planning to create a device  abstraction.  Any user shall be able to define a device with a series of  attributes. For each attribute, the user shall be able to define a series of properties  like  <strong>name , data type , minimum value , maximum value etc.</strong></p>\n\n<p>Some examples of devices are given below</p>\n\n<p><strong>Vehicle</strong></p>\n\n<p>The vehicle can have the following attributes</p>\n\n<ol>\n<li>Speed [name :- speed , data type:- double , minimum value :- 0 , maximum value :-300]</li>\n<li>Latitude [name :- speed , data :- double , minimum :- -90 , maximum :-90] </li>\n<li>Longitude[name :- Longitude, data :- double , minimum :- -180 , maximum :- 180]</li>\n</ol>\n\n<p><strong>Temperature Sensor</strong></p>\n\n<p>The temperature sensor can have the following attributes</p>\n\n<ol>\n<li>Current Temperature[name :- Current Temperation, data type:- double , minimum value :- 0 , maximum value :-300]</li>\n<li>Unit  [name :- Unit , datatype:-string]</li>\n</ol>\n\n<p>In real time , each device will be sending data as key value pairs  . </p>\n\n<p>For ex:- A <strong>Vehicle</strong> can send the following data </p>\n\n<p>Time :- 6/4/2016 11:15:15.150 , Latitude : -1.256 , Longitude :- -180.75, Speed :- 50</p>\n\n<p>Time :- 6/4/2016 11:15:16.150 , Latitude : -1.257 , Longitude :- -181.75, Speed :- 51</p>\n\n<p>For ex:- A <strong>Temperature sensor</strong> can send the following data </p>\n\n<p>Time :- 6/4/2016 11:15:15.150 , Current Temperature: 100, Unit : farenheit</p>\n\n<p>Time :- 6/4/2016 11:15:16.150 , Latitude : 101 , Unit : farenheit</p>\n\n<p>Since the attributes of different devices can be different , we are confused on how the model the tables in cassandra... Some of the options that came to mind are <strong>creating a table for a device, or create a single table and store the values in Map data types... We are little confused on which approach should be taken...\nAny suggestions is appreciated</strong></p>\n"
    }, {
        "tags": ["wso2", "iot", "wso2cep"],
        "is_answered": true,
        "view_count": 81,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1465369683,
        "creation_date": 1465267432,
        "last_edit_date": 1465270715,
        "question_id": 37669677,
        "link": "https://stackoverflow.com/questions/37669677/wso2-iots-how-to-use-event-store-to-store-device-history",
        "title": "WSO2 IOTS: How to use Event Store to store device history",
        "body": "<p>I'm trying to find out how <code>WSO2 CEP</code> work, <code>Event Stream</code> is simple, but what is <code>Event Store</code>? It can store device history but in where? I'm thinking about <code>Spark</code> and it only store data in memory, right? If I want to store device history in <code>RDMS</code>, what I can do? Thanks</p>\n"
    }, {
        "tags": ["raspberry-pi", "communication", "raspberry-pi2", "iot"],
        "is_answered": false,
        "view_count": 50,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1465337042,
        "creation_date": 1465128271,
        "question_id": 37641666,
        "link": "https://stackoverflow.com/questions/37641666/concept-for-communcation-of-a-raspberrypi-2-on-the-internet-of-things",
        "title": "Concept for communcation of a RaspberryPi 2 on the Internet of Things",
        "body": "<p>I'm currently writing a thesis about the Internet of Things. It includes connecting a RaspberryPi 2 to the Internet and evaluating the possibilities of communication, that means type of connection (i guess LAN or WLAN) and how to set up the connection. Also I have to measure different criteria, like the amount of data that is transferred, the speed and so on. \nI have never worked with a RasPi before, so does anybody have a suggestion on how I can measure these parameters like amount of data and speed properly? </p>\n"
    }, {
        "tags": ["node.js", "optimization", "simulation", "iot", "scada"],
        "is_answered": true,
        "view_count": 2742,
        "favorite_count": 3,
        "score": 3,
        "last_activity_date": 1464982094,
        "creation_date": 1462972810,
        "question_id": 37163953,
        "link": "https://stackoverflow.com/questions/37163953/node-js-for-iot-monitoring-optimization-and-system-simulation",
        "title": "Node.js for IoT monitoring, optimization and system simulation",
        "body": "<p>I am currently working on a project of real time monitoring, control and optimisation of a system (SCADA), in an Internet of Things context. \nThe acquisition of the data and the control of the assets will be done through REST api and notifications from an IoT platform (Cumulocity or Predix, not sure yet), itself collecting measures from the sensors and sending operations to the equipment.</p>\n\n<p>The server application will then control the state of the system, set alarms, and make optimised decisions (plannings  for the equipment for instance). </p>\n\n<p>In order to test the behaviour of the system, the response to a decision and finding a good optimisation solution, I plan to also make a simulation of the system (maybe even a simulation of the real-time measures). </p>\n\n<p>Finally, an HMI in Angular JS will be provided to the user for him to enter the user input data, follow the current state of the system/alarms and its history, and be notified of the decisions made.</p>\n\n<p>I am not sure of the computation needed, but the <strong>simulation</strong> will probably be quite heavy, and the <strong>optimisation</strong> solution may be a bit heavy too.</p>\n\n<p>A company would usually implement such an application in C, and it could also be done in Python (<strong>fast development</strong> is also needed), but I was thinking of doing it with Node.js. \nI found a JS library for discrete event simulation, <a href=\"https://en.wikipedia.org/wiki/SIM.JS\" rel=\"nofollow\">Sim.js</a>, but I don't know if this could handle a complex system simulation.</p>\n\n<p>In short, do you see any reason why not use Node.js for that ? \nDoes something alike has already been done, in IoT monitoring for instance ?</p>\n\n<p>Thanks a lot for your help,</p>\n\n<p>Layvier</p>\n"
    }, {
        "tags": ["azure", "message-queue", "iot", "azureservicebus"],
        "is_answered": true,
        "view_count": 101,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1463388543,
        "creation_date": 1463352151,
        "question_id": 37244469,
        "link": "https://stackoverflow.com/questions/37244469/message-queues-multiple-topics-vs-generic-topics-with-filters",
        "title": "Message Queues: Multiple Topics vs Generic Topics with Filters",
        "body": "<p>I am new to this paradigm so please bear with my stupidity. I started reading about this topic since I am building an IoT System that will use message queues to allow devices to communicate with each other. </p>\n\n<p>I am currently using Azure's Service Bus, However I believe that my question applies to this paradigm in general. </p>\n\n<p>So on to my question... My devices have inputs and outputs, with my current level of knowledge, the obvious thing to do is to create a topic per input or output, it feels much cleaner this way and I avoid unnecessary filtering. However, I am not sure whether this would create performance issues or any other issues which I cannot foresee. </p>\n\n<ul>\n<li>Should I create a topic per input/output or should I create a topic per device and then use filters to get the required information?</li>\n<li>Is it bad practice to have a lot of topics? </li>\n</ul>\n"
    }, {
        "tags": ["javascript", "html", "azure", "storage", "iot"],
        "is_answered": true,
        "view_count": 458,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1463125492,
        "creation_date": 1463072965,
        "question_id": 37193341,
        "link": "https://stackoverflow.com/questions/37193341/getting-json-file-out-in-html-through-azure-storage",
        "title": "Getting JSON file out in HTML through Azure Storage",
        "body": "<p>I have hooked my Particle photon to a eventhub on Azure where i stream the data through Stream Analytics and then to my Azure Storage. Right now i recieve the temperature as a JSON file in Azure Storage. </p>\n\n<p>Is there any way to do HTTP request or something to the storage account?\nThis is the JSON file: <a href=\"https://pptlbhstorage.blob.core.windows.net/temperature/0_d1e8a2b709b14461b5ac12265f33020b_1.json\" rel=\"nofollow\">https://pptlbhstorage.blob.core.windows.net/temperature/0_d1e8a2b709b14461b5ac12265f33020b_1.json</a></p>\n\n<p>Also, should i make a WepAPI that recieves theese data? so i also can make mobile app and not only webpage? </p>\n"
    }, {
        "tags": ["xml", "mule", "mule-studio", "iot"],
        "is_answered": true,
        "view_count": 48,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1462933681,
        "creation_date": 1462879439,
        "last_edit_date": 1462880398,
        "question_id": 37137112,
        "link": "https://stackoverflow.com/questions/37137112/declaring-variables-to-store-data-from-previous-iot-feed-in-mule",
        "title": "Declaring variables to store data from previous IoT feed in MULE",
        "body": "<p>Im starting working on Mule and im trying to make an app that will \"predict\" if it's going to rain and post it on twitter using IoT.</p>\n\n<p>The app has an http connector that gets humidity feeds from a sensor, and here comes my question.</p>\n\n<p>Is there anyway to store the humidity in some kind of variable so when a new feeds comes in I can compare both, the old and the new? Thanks.</p>\n"
    }, {
        "tags": ["javascript", "angularjs", "rest", "iot"],
        "is_answered": false,
        "view_count": 1121,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1460416001,
        "creation_date": 1460415481,
        "question_id": 36560741,
        "link": "https://stackoverflow.com/questions/36560741/handling-realtime-data-with-angularjs",
        "title": "Handling realtime data with angularJs",
        "body": "<p>So, I am playing with these temperature sensors and I have a REST API structure to retrieve current temperature. In the front end, I have my angularJs code and I want to be able to see my data real time.</p>\n\n<p>So, say this is my service call</p>\n\n<pre><code>getTemperature: function(){\n   return $http({\n     method:'GET',\n     url:'https://someurltoget/device10/temperature',\n     header:{\n            'Accept':'application-json'},\n     }).then(\n      function(response){\n         return response;\n    },function(error){\n         return error;})}\n</code></pre>\n\n<p>in my controller, I call this service, and update my view. But, temperature values update themselves in every 60 seconds, and I want this call to update my view accordingly.</p>\n\n<p>However, I am not sure which way would be the best practice to get real-time data simultaneously. </p>\n\n<p>I can only think of using $interval and update the view every 60 sec. but I feel like this is not the best way to resolve this problem.</p>\n\n<p>So, any idea how can this be accomplished in its best way in AngularJS?</p>\n"
    }, {
        "tags": ["web-applications", "iot", "intel-galileo"],
        "is_answered": true,
        "view_count": 100,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1459678674,
        "creation_date": 1459447464,
        "question_id": 36340551,
        "link": "https://stackoverflow.com/questions/36340551/what-is-the-seamless-way-to-build-an-iot-application",
        "title": "What is the seamless way to build an IoT application?",
        "body": "<p>I need to build a lighting control system. I'll use an Arduino and a light sensor to acquire data and a remote app to do the processing and control. I could do this locally within Arduino or with a client/server app using my notebook. But I want to build my app on the cloud, receiving data from Arduino and sending back the action (i.e. increase or decrease light according to the amount of natural light).</p>\n\n<p>What would be the best approach to building this? </p>\n"
    }, {
        "tags": ["real-time", "ibm-cloud", "iot"],
        "is_answered": true,
        "view_count": 172,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1458053766,
        "creation_date": 1455652954,
        "last_edit_date": 1455679582,
        "question_id": 35441970,
        "link": "https://stackoverflow.com/questions/35441970/ibm-iot-real-time-insights-chart-visualizations",
        "title": "IBM IoT Real-Time insights chart visualizations",
        "body": "<p>Within the IBM IoT Real-Time insights we can display charts of data contained with incoming events.  Here is an example of such a chart:\n<a href=\"https://i.stack.imgur.com/NGEfE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NGEfE.png\" alt=\"enter image description here\"></a></p>\n\n<p>What configuration do we have over the visualization?  For example:</p>\n\n<ul>\n<li>Can I control the boldness or visibility of division lines?</li>\n<li>Can I place labels on the X-Axis?</li>\n<li>Can I move the labels on the Y-Axis to the left of the chart?</li>\n<li>Can we plot marker points on the graph showing data points?</li>\n<li>Can we control the nature of the curve (it appears like some kind of best fit curve by default)?</li>\n</ul>\n"
    }, {
        "tags": ["message-queue", "iot"],
        "is_answered": false,
        "view_count": 32,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1457019661,
        "creation_date": 1457019661,
        "question_id": 35776550,
        "link": "https://stackoverflow.com/questions/35776550/messages-in-a-queue-based-on-order",
        "title": "Messages in a Queue based on order",
        "body": "<p>What is the best way to make sure you have all the messages in the proper order in your queue such as SQS or Kafka?</p>\n\n<p>Basically, I need to make sure all the messages that I have in the queue are ordered for example based on date+time and process them online.  I need to do this because I need to process only the first 1000 messages in the queue online. What is the best approach here?</p>\n"
    }, {
        "tags": ["rabbitmq", "iot"],
        "is_answered": true,
        "view_count": 1021,
        "favorite_count": 0,
        "score": 4,
        "last_activity_date": 1456086530,
        "creation_date": 1456081695,
        "question_id": 35540645,
        "link": "https://stackoverflow.com/questions/35540645/is-it-safe-to-expose-rabbitmq-amqp-port-over-the-internet",
        "title": "Is it safe to expose rabbitmq amqp port over the internet?",
        "body": "<p>I have a lot of different machines in multiple geographical locations. I need to command them from my backend and get data from them. I was thinking about connecting them all to a rabbitmq amqps connection to enable the bi-directionnel communication of my machines.</p>\n\n<p>Is it a good approach? Is rabbitmq secure enough to do that?</p>\n"
    }, {
        "tags": ["database", "iot"],
        "is_answered": true,
        "view_count": 321,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1453910523,
        "creation_date": 1453737650,
        "last_edit_date": 1453800512,
        "question_id": 34996896,
        "link": "https://stackoverflow.com/questions/34996896/which-database-for-a-internet-of-things-case-would-work",
        "title": "Which database for a internet of things case would work?",
        "body": "<p>Ok. Just a scenario. </p>\n\n<p>I got about 100 smart homes. Every smart home has a different amount of door sensors, temperature sensors, humidity sensors. Every house could add or remove some of them. \nFor this example every temp &amp; hum sensor uploads every 10 minutes (asynchron) their current value (via MQTT) to a server (144 Values/per day/per sensor). And all door sensors upload their state (open, closed) everytime the state changes.</p>\n\n<p>In my study I just got a little insight in relational database (MySQL) and I think that is the wrong model for my purpose (but I have nearly forget everything). So could you just give me tip like:</p>\n\n<p>\"Search for \"relational Database\". \"MySQL\". This works fine for your case because...\" </p>\n\n<p>I want a database where i could add a smart home: with informations like <em>city, street, inhabitant,...</em>. And to this smart home I add sensors like (temp1, temp2, temp3, temp4, door1, door2, hum1). Important: I want to be able to add and remove sensors.\nand for every Sensor I want the data stored like this:</p>\n\n<pre><code>temp1\nDate                   Temperature    \n01.01.2016 09:42       22.2\u00b0C\n01.01.2016 09:52       21.2\u00b0C\n01.01.2016 10:02       21.5\u00b0C\n01.01.2016 10:42       21.7\u00b0C\n01.01.2016 10:42       21.9\u00b0C\n01.01.2016 10:42       21.8\u00b0C\n01.01.2016 10:42       22.1\u00b0C\n</code></pre>\n\n<p>is this a object-orient-database? </p>\n\n<p>Would it be possible to get the History of the last seven days from smarthome45.door7 </p>\n\n<p>greetings from germany</p>\n\n<p>Edit:</p>\n\n<blockquote>\n  <p>\"Unclear. What is your question ? Please edit.\" \u2013 kebs \n  I don't have much experience in databases. I read some summarys of the different database models but I still don't know, which of them would fit for my project. So I just hoped, that someone who has some experience in databases could give me some advices. Because for me it looks like I have a database in a database.</p>\n</blockquote>\n"
    }, {
        "tags": ["iot", "getresource", "coap", "custom-receiver"],
        "is_answered": true,
        "view_count": 671,
        "favorite_count": 1,
        "score": 0,
        "last_activity_date": 1453269169,
        "creation_date": 1444977371,
        "last_edit_date": 1445154227,
        "question_id": 33163975,
        "link": "https://stackoverflow.com/questions/33163975/coap-as-a-streaming-source",
        "title": "COAP as a Streaming Source",
        "body": "<p>I am currently working on IOT Coap protocol.I accessed server on local host through copper firefox plugin. Then i Added resouce having \"GET\" functionality in server.\nAfter that i made its client as a streaming source. \nHere is the code of client streaming</p>\n\n<pre><code> class customReceiver(test:String) extends  Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging with Serializable { \n   @volatile private var stopped = false\n   override def onStart() {\n\n      val client = new CoapClient(\"ip/resource\")\n      var text = client.get().getResponseText();  \n      store(text)\n   }\n   override def onStop(): Unit = synchronized { \n      try\n      {\n         stopped = true\n      }\n      catch\n      {\n         case e: Exception =&gt; println(\"exception caught: \" + e);\n      }\n   }\n }\n</code></pre>\n\n<p>but i am facing a problem. During streaming it just read a resource once. after that it fetches all empty rdd and completes its batches. Meanwhile if resource changes its value it doesn't read that. \nare i doing something wrong? or is there exists any other functionality to read whenever resource get changed that i can handle in my Custom receiver.? or any idea about how to GET value continuously during streaming?</p>\n\n<p>Any help is much awaited and appreciated.\nThanks</p>\n"
    }, {
        "tags": ["c#", "async-await", "iot", "azure-eventhub"],
        "is_answered": false,
        "view_count": 429,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1449600271,
        "creation_date": 1441791223,
        "last_edit_date": 1495542192,
        "question_id": 32475983,
        "link": "https://stackoverflow.com/questions/32475983/burst-mode-of-request-with-eventhub-and-async",
        "title": "Burst Mode of Request with EventHub and Async",
        "body": "<p>i am trying to send a audio file(<code>wav</code>) from a <code>IoT</code> Device to <code>Eventhub</code>. As <code>Eventhub</code> has size restriction of 64Kb per message ,each message is chunked into a <code>7kb</code> byte array and sent to <code>Eventhub</code>. i am trying to achieve maximum send rate <em>[not crossing the threshold]</em> from client.</p>\n\n<p>Recording a live audio , save it in file stream and chunk it for sending. i avoided this part with custom stream implementation</p>\n\n<pre><code>public class CustomAudioStream : IRandomAccessStream{\n    public IAsyncOperationWithProgress&lt;uint, uint&gt; WriteAsync(IBuffer buffer)\n        {\n            return AsyncInfo.Run&lt;uint, uint&gt;((token, progress) =&gt;\n             {\n                 return Task.Run(() =&gt;\n                 {\n                     using (var memoryStream = new MemoryStream())\n                     {\n                         using (var outputStream = memoryStream.AsOutputStream())\n                         {\n                             outputStream.WriteAsync(buffer).AsTask().Wait();\n\n                             var byteArray = memoryStream.ToArray();\n\n                             //bytes are ready to send from here \n                             ChunkedEventSender(this, byteArray);\n#if DEBUG\n                             Debug.WriteLine(\"Array Length:  \" + byteArray.Length + \"    MemoryStream length:\" + memoryStream.Length);\n#endif \n                             return (uint)memoryStream.Length;\n                         }\n                     }\n                 });\n             });\n        }\n   }\n</code></pre>\n\n<p>But i am not able to send the byte at same speed here with REST implementation. for Sending i am using a third party wrapper so i can't do at that side. </p>\n\n<p>But i can span thread to make application responsive while interacting  so i use</p>\n\n<pre><code> Task.Factory.StartNew(()=&gt;{ \n          BackgroundSender(byte[data]);\n        });\n</code></pre>\n\n<p>i don't want to wait for Task to complete neither the outcome of Task.but when i do this most of the my request are getting \"<code>Request Timed out</code>\" and Application is getting stuck because of those request.</p>\n\n<p>Is there anyway i can make application responsive without blocking the thread. </p>\n\n<hr>\n\n<p>Edit: Currently Applied <code>Parallel.Invoke</code> and not loosing a single message and application is also responsive but sending drop drastically to 2-3 message/Second</p>\n\n<p>Should i switch to Multi Thread model rather than using Async. Found Simlar Question like this <a href=\"https://stackoverflow.com/questions/16194054/is-async-httpclient-from-net-4-5-a-bad-choice-for-intensive-load-applications?rq=1\">Is async HttpClient from .Net 4.5 a bad choice for intensive load applications?</a></p>\n"
    }, {
        "tags": ["ibm-cloud", "iot", "node-red"],
        "is_answered": false,
        "view_count": 262,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1449498905,
        "creation_date": 1449331651,
        "question_id": 34107600,
        "link": "https://stackoverflow.com/questions/34107600/how-to-put-timestamp-on-bluemix-iot-payload-of-connected-device",
        "title": "How to put timestamp on bluemix iot payload of connected device",
        "body": "<p>How do i put timestamp on the msg payload from each streaming devices.\nand also those timestamp should be from device timezone.</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["azure", "iot"],
        "is_answered": true,
        "view_count": 9099,
        "favorite_count": 1,
        "score": 8,
        "last_activity_date": 1449139786,
        "creation_date": 1444384585,
        "last_edit_date": 1445827099,
        "question_id": 33035243,
        "link": "https://stackoverflow.com/questions/33035243/does-microsoft-azure-iot-hub-stores-data",
        "title": "Does Microsoft Azure IoT Hub stores data?",
        "body": "<p>I have just started learning Azure IoT and it's quite interesting. I am confuse about does IoT Hub stores data somewhere? <br/><br/>\ni.e. Suppose i am passing room Temperature to IoT hub and want to store it in database for further use. How it's possible?<br/><br/>\nI am clear on how device-to-cloud and cloud-to-device works with IoT hub.</p>\n"
    }, {
        "tags": ["api", "logging", "iot"],
        "is_answered": true,
        "view_count": 677,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1448368002,
        "creation_date": 1448265346,
        "question_id": 33866033,
        "link": "https://stackoverflow.com/questions/33866033/get-data-range-by-date-from-thingspeak-channels-api",
        "title": "Get data range by date from Thingspeak Channels api",
        "body": "<p>I have logged tens of thousands of values in Thingspeak for a couple of months. When retrieving the data with the Channels api I can set the start parameter. If I set it to yesterday I will get everything after that (as long as the value count is below 8000, which is the maximum amount that can be retrieved in one request). If I set it to two months ago (after which about 20000 values have been logged) I will get the most recent 8000 values.</p>\n\n<pre><code>STARTTIMExxxxxxXXXXXXXXXXXXENDTIME\nSTARTTIMEXXXXXXXXXXXXxxxxxxENDTIME\n</code></pre>\n\n<p>So now when I set start date (end date is implicitly set to now) I get the big X from the first row as in the example above when I want the values on the second row.</p>\n\n<p>How would I get 8000 values from the moment I specify?</p>\n\n<p>Is there any other way or do I actually need to set both start and end date and put an interval that is smaller than the time I can log 8000 values and then make more queries to the api to fill up my requested values?</p>\n"
    }, {
        "tags": ["fiware", "iot", "fiware-orion"],
        "is_answered": false,
        "view_count": 275,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1447938038,
        "creation_date": 1447935918,
        "question_id": 33803927,
        "link": "https://stackoverflow.com/questions/33803927/fiware-for-data-collection-from-iot-devices",
        "title": "Fiware for data collection from IOT devices",
        "body": "<p>I am to design an IOT platform which should collect sensor data from tens of thousands of sources or more. Process this data in different ways and present the user with interactive dashboards which allow the user to drill down through the aggregate information down to the specific device level. Also support the case where the user might ask for live information from that specific device.</p>\n\n<p>I have ideas which tools I could use for data stream distributed processing but not much about how to interface with IOT devices. And that's why I started looking into FIWARE.</p>\n\n<p>Reading through FIWARE site, different FIWARE architecture documents, GE specific documentation etc. and trying to answer my own design architecture questions. The abundance of available information is confusing to say the least.\n<strong>I think that my main difficulty at the moment is to map from the architecture as it is described <a href=\"https://forge.fiware.org/plugins/mediawiki/wiki/fiware/index.php/Internet_of_Things_(IoT)_Services_Enablement_Architecture\" rel=\"nofollow\">here</a> to the actual available implementation components.</strong></p>\n\n<p><strong>Question:</strong>\nIf I am to use FIWARE GEis as a device proxy which receives data from devices and forwards to the realtime processing pipeline, which GEis should I use? And which specific implementations? </p>\n"
    }, {
        "tags": ["iot", "cumulocity"],
        "is_answered": true,
        "view_count": 243,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1447246234,
        "creation_date": 1447076638,
        "question_id": 33610510,
        "link": "https://stackoverflow.com/questions/33610510/cumulocity-cloud-fieldbus-modbus-device-sometimes-does-not-recognize-modbus-me",
        "title": "Cumulocity Cloud Fieldbus modbus device (sometimes) does not recognize modbus measurements",
        "body": "<p>I am using a NETCOMM NTC-6200 modem to connect devices via the modbus TCP protocol.  Using the Cloud Fieldbus application I can register the devices on the modbus registers I want and add the device to cumulocity.\nHowever, once I have done this the measurements only show up sometimes.</p>\n\n<p>To give a concrete example, I am connecting a meter measuring voltage, current and power.  When adding the device the current measurements are recognized and shown.  Upon deleting the device and redoing the entire process, all three measurements are shown.  Repeating a third time results in no measurements being recognized. This can be repeated seemingly at random. </p>\n\n<p>In these cases of \"non-recognition\" the \"measurements\" icon in the device menu does not appear and in custom dashboards the device is shown but no data-points are available for the device.</p>\n\n<p>Is there a possibility that the measurement recognition needs some sort of trigger to add the measurement to the device?</p>\n"
    }, {
        "tags": ["iot", "cumulocity"],
        "is_answered": true,
        "view_count": 372,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1446718347,
        "creation_date": 1446711321,
        "question_id": 33539520,
        "link": "https://stackoverflow.com/questions/33539520/custom-date-queries-using-cumulocity-api",
        "title": "Custom Date queries using Cumulocity API",
        "body": "<p>Is it possible to aggregate measurements or create custom queries beyond the standard <code>dateFrom</code> <code>dateTo</code> queries?</p>\n\n<p>As an example, I have measurements which have a time delta of 1 minute (2015-01-01T05:05:00, 2015-01-01T05:05:00, 2015-01-01T05:05:00, ...) and I would like to query the measurements at 15 minute intervals (2015-01-01T05:15:00, 2015-01-01T05:30:00, 2015-01-01T05:45:00, ...)</p>\n\n<p>So far I have only come up with these solutions:</p>\n\n<ol>\n<li>Using the standard api request as in\n<code>https://tenant.cumulocity.com/measurement/measurements?dateFrom=2015-10-01&amp;dateTo=2015-11-05</code>\nand then throwing away most of the data will use a massive amount of time loading the data.</li>\n<li>Using cep (cumulocity event language) to generate a new measurement every 15 minutes using the nearest 1 minute measurement seems like a bit of overkill and not very elegant.</li>\n<li>Batch requesting the exact minute \n<code>https://tenant.cumulocity.com/measurement/measurements?dateFrom=2015-11-05T05:15:00%2B01:00&amp;dateTo=2015-11-05T05:16:00%2B01:00</code>\nwhich will in a massive amount of API requests and also does not seem very efficient.</li>\n<li>Use the <code>/measurements/series</code> endpoint which will only give me all series, even those I do not want, as well as only having the aggregation options hourly and daily (as far as I can tell).  </li>\n</ol>\n\n<p>Is there a better way of doing this?</p>\n"
    }, {
        "tags": ["cassandra", "apache-kafka", "spark-streaming", "iot"],
        "is_answered": false,
        "view_count": 161,
        "favorite_count": 0,
        "score": 2,
        "last_activity_date": 1446631957,
        "creation_date": 1446628853,
        "last_edit_date": 1446629169,
        "question_id": 33517932,
        "link": "https://stackoverflow.com/questions/33517932/perform-queries-over-the-time-series-stream-of-data",
        "title": "Perform queries over the time-series stream of data",
        "body": "<p>I'm trying to design an architecture of my streaming application and choose the right tools for the job.</p>\n\n<p>This is how it works currently:\n<img src=\"https://photos-3.dropbox.com/t/2/AABY7uSjmrbcplaj4b8M387lHyaxEaXUDp1P5WpZWKRYYg/12/27579084/png/32x32/1/_/1/2/spark_streaming.png/EKK94RQY95YDIAIoAg/s2lUoNfSkN9g4rcL4S8x9MG-iNT-HHz59YcfHP0wbwE?size=1024x768&amp;size_mode=2\" alt=\"app-arch\"></p>\n\n<p>Messages from \"application-producer\" part have a form of <code>(address_of_sensor, timestamp, content)</code> tuples. </p>\n\n<p>I've already implemented all functionality before Kafka, and now I've encountered major flaw in the design. In \"Spark Streaming\" part, consolidated stream of messages is translated into stream of events. The problem is that events for the most part are composite - consist of multiple messages, which have occurred at the same time at different sensors. </p>\n\n<p>I can't rely on \"time of arrival to Kafka\" as a mean to detect \"simultaneity\". So I has to somehow sort messages in Kafka before extracting them with Spark. Or, more precisely, make queries over Kafka messages. </p>\n\n<p>Maybe Cassandra is the right replacement for Kafka here? I have really simple data model, and only two possible types of queries to perform: query by address, and range query by timestamp. Maybe this is the right choice? </p>\n\n<p>Do somebody have any numbers of Cassandra's throughput?  </p>\n"
    }, {
        "tags": ["sparql", "semantics", "jena", "semantic-web", "iot"],
        "is_answered": true,
        "view_count": 497,
        "favorite_count": 2,
        "score": 4,
        "last_activity_date": 1445006017,
        "creation_date": 1421217499,
        "question_id": 27937100,
        "link": "https://stackoverflow.com/questions/27937100/semantic-techniques-in-iot",
        "title": "Semantic techniques in IOT",
        "body": "<p>I am trying to use semantic technologies in IOT. From the last two months I am doing literature survey and during this time I came to know some of the tools required like (protege, Apache Jena). Now along with reading papers I want to play with semantic techniques like annotation, linking data etc so that I can get the better understanding of the concepts involved. For the same I have put the roadmap as:</p>\n\n<ol>\n<li>Collect data manually (using sensors) or use some data set already on the web.</li>\n<li>Annotate the dataset and possibly use ontology (not sure)</li>\n<li>Apply open linking data principles\nI am not sure whether this road map is correct or not. I am asking for suggestions in following points</li>\n</ol>\n\n<p>Is this roadmap correct?\nHow should I approach for steps 2 and 3. In other words which tools should I use for these steps?</p>\n\n<p>Hope you guys can help me in finding a proper way for handling this issue. Thanks</p>\n"
    }, {
        "tags": ["networking", "gprs", "iot"],
        "is_answered": true,
        "view_count": 153,
        "favorite_count": 1,
        "score": 1,
        "last_activity_date": 1443768101,
        "creation_date": 1442932415,
        "question_id": 32719725,
        "link": "https://stackoverflow.com/questions/32719725/is-it-possible-to-send-data-from-a-server-to-a-gprs-module-without-it-opens-a-co",
        "title": "Is it possible to send data from a server to a gprs module without it opens a connection first?",
        "body": "<p>If not, what is the best way to deal with the fact that the module needs to open a connection first?</p>\n\n<p>Thanks</p>\n"
    }, {
        "tags": ["powershell", "raspberry-pi", "iot", "windows-10-iot-core"],
        "is_answered": true,
        "view_count": 3429,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1443461169,
        "creation_date": 1443460154,
        "last_edit_date": 1443461160,
        "question_id": 32828332,
        "link": "https://stackoverflow.com/questions/32828332/how-to-use-powershell-to-set-the-time-on-a-remote-device",
        "title": "How to use PowerShell to set the time on a remote device?",
        "body": "<p>I want to set the Date and Time of a remote device (<em>Raspberry Pi 2 running Windows IoT</em>) to the value of the Date Time of a local device.</p>\n\n<p>I create a variable $dateTime to hold the local DateTime.\nI assign a password to connect to  a remote device to a variable $password.\nI create a credential object.\nI connect to the remote device using Enter-PSSession.\nNow that I'm connected I try assigning the remote devices DateTime using Set-Date = $dateTime | Out-String.</p>\n\n<p>I get <em>cannot convertvalue \"=\" to type \"System.TimeSpan\"</em> error.</p>\n\n<pre><code>$dateTime = Get-Date\n$password = ConvertTo-SecureString \"mypassword\" -AsPlainText -Force\n$cred = New-Object System.Management.Automation.PSCredential (\"myremotedevice     \\Administrator\",$password)\nEnter-PSSession -ComputerName myremotedevice -Credential $cred\nSet-Date = $dateTime | Out-String\n</code></pre>\n\n<p>It seems as if the $dateTime variable is out of scope once I am connected via the PSSession.  Is there a way around this ?</p>\n"
    }, {
        "tags": ["cloud", "bigdata", "iot"],
        "is_answered": false,
        "view_count": 50,
        "favorite_count": 0,
        "score": -2,
        "last_activity_date": 1443189448,
        "creation_date": 1443139629,
        "last_edit_date": 1443165753,
        "question_id": 32772835,
        "link": "https://stackoverflow.com/questions/32772835/cloud-vs-bigdata-what-is-real-technical-difference",
        "title": "Cloud vs Bigdata - What is real technical difference",
        "body": "<p>As I know both cloud and big data are used to store huge amount of data. So in case of data storing, I do not see any difference. However, I think there is architecture difference as well as the way data is getting stored,processed and retrieved is also difference.</p>\n\n<p>In this question, I wanted to know, what is the real difference (something very technical and internal operation) between these two.</p>\n\n<p>Asking because, one of my trainer said, \"cloud is for data storage and Big data is for data analysis\", I'm not completely agreed and convinced by this statement.\nAs per my knowledge both can handle real time data analysis (in case of IoT data). And, there are tools available for this(in Apache).</p>\n\n<p>Cloud was present previously which can handle large amount of data...Why Big data came into picture? What extra benefit its giving?</p>\n\n<p>My question may be funny but your answer will give me real things to understand during client facing and requirement gathering and estimation and design.</p>\n\n<p>PLease help. Your honest suggestion may help me to take a better decision, when should I go for cloud and when Big data? Thanks.  </p>\n"
    }, {
        "tags": ["amazon-web-services", "rabbitmq", "load-balancing", "amazon-elb", "iot"],
        "is_answered": false,
        "view_count": 1054,
        "favorite_count": 3,
        "score": 3,
        "last_activity_date": 1442662391,
        "creation_date": 1441055943,
        "last_edit_date": 1442662391,
        "question_id": 32320150,
        "link": "https://stackoverflow.com/questions/32320150/how-to-distribute-long-lived-tcp-connections-to-individual-nodes-e-g-to-rabbit",
        "title": "How to distribute long-lived TCP connections to individual nodes, e.g. to RabbitMQ nodes in a cluster?",
        "body": "<p>I'm about to deploy a cluster of RabbitMQ nodes in AWS. I expect there would be 100k+ (possibly 1m+) clients - IoT devices - which will connect to the RabbitMQ cluster over MQTT and will stay connected 24/7/365.</p>\n\n<p>Now the question is: how to distribute the TCP connections from clients to individual nodes comprising the RabbitMQ cluster? I see a couple of options:</p>\n\n<ol>\n<li>Use AWS Elastic Load Balancer</li>\n<li>Use HAProxy on AWS EC2</li>\n<li>Use DNS round-robin</li>\n<li>Use DNS SRV (e.g. like in SIP service deployments)</li>\n<li>Use a custom hand-written client-side load balancing algorithm, based on a list of addresses obtained from a server or hard-coded into the device</li>\n</ol>\n\n<p>Which solution from the ones listed above would you recommend given the amount of load and the expected duration each connection? Or maybe a combination of these? Or is there any other solution worth knowing? </p>\n\n<p>EDIT: answers to the questions stated in the first comment:</p>\n\n<ul>\n<li>SSL: yes, of course</li>\n<li>keepalive interval: I was thinking about 1 minute</li>\n<li>Are the connection mostly idle? -> no, the devices will report various sensor data, I assume at least once in a minute</li>\n<li>How many connections can your MQ cluster nodes service, each, max, and how heavily do you plan to load them? -> as mentioned earlier, 100k+ or even 1m+ connections total, no clue how many per node, I was about to conduct some tests to check that</li>\n<li>How many nodes do you plan to need for this load? -> honestly, I don't know yet, I assume 10 should be enough for 100k+ connections, but it also depends on the type of EC2 machines, right?</li>\n<li>Are the clients globally distributed, and is your system, or concentrated in one region? -> the system is targeted globally, but initially I only plan to cover one region; eventually I expect to cover other regions with separate RabbitMQ clusters and have the clients connect with geo- or latency- based load balancing</li>\n</ul>\n\n<p>EDIT 2: I've just found out about a potential <a href=\"http://oomagnitude.com/2014/05/14/aws-elastic-load-balancer-black-hole/\" rel=\"nofollow\">Black Hole problem</a> of AWS ELB with long-lived TCP connections. I haven't checked if AWS ELB still behaves this way, but it could be a show-stopper for my use case. Does anyone have any experience with this or similar issues?</p>\n"
    }, {
        "tags": ["db2", "ibm-cloud", "iot", "node-red"],
        "is_answered": true,
        "view_count": 384,
        "favorite_count": 0,
        "score": -1,
        "last_activity_date": 1440658388,
        "creation_date": 1439469740,
        "last_edit_date": 1439555579,
        "question_id": 31988590,
        "link": "https://stackoverflow.com/questions/31988590/bluemix-nodered-sql",
        "title": "Bluemix NodeRed SQL",
        "body": "<p>I am using NodeRed to collect data from a CC2650 SensorTag and store it in a SQL database. \nThe SensorTag is connected to my phone through the app and the data is pushed to the cloud. My NodeRed application collects this data then stores it to my SQL. However I have found the system will disconnect and is unable to monitor the data for long periods of time. I have two sensortags connected and the data stream seems to randomly stop. I believe it is due to a loss in connection between NodeRed and SQL but am unsure. Is anyone else running a similar programme facing these problems? </p>\n\n<p>The data will appear in the NodeRED debug screen and get uploaded to the SQL database in the table. As the data is being sent I can see each point being uploaded as I refresh the table however the upload is not consistent and will sometimes stop uploading to the SQL table despite still being displayed on the debug. So far I have not seen any pattern the data will stop randomly for no clear reason. I have been able to refresh the system and deploy it again to try and get it running again however this is not always successful and only a temporary solution. </p>\n"
    }, {
        "tags": ["python", "django", "iot"],
        "is_answered": true,
        "view_count": 3397,
        "favorite_count": 0,
        "closed_date": 1439155914,
        "score": -2,
        "last_activity_date": 1439157218,
        "creation_date": 1439155596,
        "last_edit_date": 1439157218,
        "question_id": 31909000,
        "link": "https://stackoverflow.com/questions/31909000/is-it-a-good-idea-to-use-django-for-an-iot-project",
        "title": "Is it a good idea to use Django for an IoT project?",
        "body": "<p>I'm thinking to use Django in IoT(Internet of Things) project. Do you thing this is a good idea ? I'm expecting some arguments please.</p>\n\n<p>PS : The project is an industrial project that contain thousands (or more) of devices. And I want to use Django in the server side.</p>\n"
    }, {
        "tags": ["iot", "cumulocity"],
        "is_answered": false,
        "view_count": 293,
        "favorite_count": 0,
        "score": 0,
        "last_activity_date": 1437133616,
        "creation_date": 1437125696,
        "last_edit_date": 1437133383,
        "question_id": 31472707,
        "link": "https://stackoverflow.com/questions/31472707/cumulocity-data-export",
        "title": "Cumulocity data export",
        "body": "<p>I noticed a limit of 2000 records per API call for getting collections out of Cumulocity. Will we be constrained to these limits or is there any other batch API available?</p>\n"
    }, {
        "tags": ["architecture", "message-queue", "iot", "proxy-server"],
        "is_answered": false,
        "view_count": 601,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1436942745,
        "creation_date": 1436860039,
        "question_id": 31400707,
        "link": "https://stackoverflow.com/questions/31400707/app-communicate-with-message-queue-directly-vs-communicating-with-a-proxy-fron",
        "title": "App communicate with message queue directly vs. communicating with a proxy (front end service)",
        "body": "<p>We are working a new architecture for our product. Out product is not exactly IoT - devices communicate with a single box at the client site and that box communicates with our servers.</p>\n\n<p>We have 2 options:</p>\n\n<ol>\n<li>The box will send a message directly to a queue which will be picked up by the worker server and handled at it's turn.</li>\n<li>The box will send a message to a front end server. All that server does is put the message in the queue for the worker to handle.</li>\n</ol>\n\n<p>There are pros and cons to each method. Number one pro for communicating directly with the queue is that we don't need to spend money on machines to hold the front end services.</p>\n\n<p>The biggest pro for using a front end server is that it acts as an abstraction layer against the queue technology we are working with - so if we change the queue we don't need to update all clients with a new version for them to keep working. Another advantage that we think about is the it allows us to simulate synchronios  calls.</p>\n\n<p>Of course, there are many pros and cons to each. What is the suggested way to work? best practices? security?</p>\n"
    }, {
        "tags": ["database", "rest", "time-series", "publish-subscribe", "iot"],
        "is_answered": true,
        "view_count": 221,
        "favorite_count": 3,
        "score": 1,
        "last_activity_date": 1436261360,
        "creation_date": 1430213061,
        "question_id": 29915419,
        "link": "https://stackoverflow.com/questions/29915419/time-series-oriented-iot-platform",
        "title": "Time Series Oriented IoT Platform",
        "body": "<p>I have an embedded \"thing\" which generates data samples from several sensors at 1kHz. It has a fairly bandwidth constrained 3G connection to the outside world.</p>\n\n<p>Does anyone know of a platform which can provide the following (or at least a subset of the following):</p>\n\n<ol>\n<li>A Publish/Subscribe interface to send/receive real time data streams</li>\n<li>A (very) compact/compressed time series oriented transmission protocol for bandwidth-constrained devices</li>\n<li>A database to store the data samples in a compact format</li>\n<li>A REST-ful query-based API to retrieve historic data samples</li>\n<li>A GUI javascript? dashboard capable of real time and historic graphing capabilities</li>\n</ol>\n\n<p>The majority of platforms I have found are designed to work in the domain of  seconds rather than milliseconds and/or use string-heavy data formats such as JSON.</p>\n"
    }, {
        "tags": ["oracle", "iot"],
        "is_answered": true,
        "view_count": 125,
        "favorite_count": 0,
        "score": 1,
        "last_activity_date": 1412155891,
        "creation_date": 1412132213,
        "question_id": 26133425,
        "link": "https://stackoverflow.com/questions/26133425/update-index-organized-tables-using-multiple-update-queries-temporary-duplicate",
        "title": "Update Index Organized Tables using multiple UPDATE queries (temporary duplicates)",
        "body": "<p>I need to update the primary key of a large Index Organized Table (20 million rows) on Oracle 11g.</p>\n\n<p>Is it possible to do this using multiple UPDATE queries? i.e. Many smaller UPDATEs of say 100,000 rows at a time. The problem is that one of these UPDATE batches could temporarily produce a duplicate primary key value (there would be no duplicates after all the UPDATEs have completed.)</p>\n\n<p>So, I guess I'm asking is it somehow possible to temporarily disable the primary key constraint (but which is required for an IOT!) or alter the table temporarily some other way. I can have exclusive and offline access to this table.</p>\n\n<p>The only solution I can see is to create a new table and when complete, drop the original table and rename the new table to the original table name.</p>\n\n<p>Am I missing another possibility?</p>\n"
    }]
}